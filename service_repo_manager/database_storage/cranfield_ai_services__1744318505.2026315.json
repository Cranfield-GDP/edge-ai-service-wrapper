[
    {
        "model_name": "microsoft/resnet-50",
        "model_url": "https://huggingface.co/microsoft/resnet-50",
        "task": "image-classification",
        "task_detail": "The ResNet-50 v1.5 model is an advanced convolutional neural network specifically designed for image classification tasks. This pre-trained model is based on the ResNet (Residual Network) architecture, which incorporates residual learning and skip connections, allowing for the effective training of deeper neural networks. ResNet-50 v1.5 is an improvement over the original ResNet v1 due to modifications in the bottleneck blocks, specifically implementing a stride of 2 in the 3x3 convolution layer, enhancing its classification accuracy by approximately 0.5% in top-1 accuracy metrics.\n\nThe model has been trained on the ImageNet-1k dataset, which consists of 1,000 distinct classes, making it highly versatile for classifying a wide array of images into these categories. Typical images suitable for processing by this model include those containing various animals, everyday objects, scenes, and more, as represented in the ImageNet-1k dataset.\n\nTo utilize this model effectively, the input image should be formatted to a resolution of 224x224 pixels. The model expects the input data to be processed into tensors, which can be achieved using the AutoImageProcessor from the Transformers library. Once the image data is prepared and passed through the model, it produces logits as output. These logits represent the classification scores for each of the 1,000 ImageNet classes. The ultimate prediction is obtained by selecting the class with the highest score, which indicates the model's prediction for the most likely category of the given input image.\n\nThis model is primarily intended for raw image classification tasks and can be a powerful component in applications requiring image recognition capabilities, such as automated tagging systems, visual search engines, and enhancement of image-based data analytics. Due to its pre-trained nature, it can immediately be used or fine-tuned for specific image classification tasks, making it an accessible tool for developers and machine learning practitioners interested in leveraging state-of-the-art image recognition technology.",
        "accuracy_info": "The README mentions that ResNet-50 v1.5 is slightly more accurate (approximately 0.5% top-1 accuracy improvement) compared to ResNet v1. This minor improvement in accuracy is attributed to changes in the bottleneck blocks, with stride adjustments made in the 3x3 convolution.",
        "image_repository_url": "",
        "service_disk_size_bytes": 3742758510.0,
        "profiles": [
            {
                "node_id": "ugurcan.celik",
                "device_type": "DeviceType.CPU",
                "device_name": "cuda",
                "initialization_time_ms": 4850.1739501953125,
                "eviction_time_ms": 0.0,
                "initialization_cost": 0.0,
                "keep_alive_cost": 0.0,
                "energy_consumption_idle": 0.0,
                "idle_container_cpu_memory_usage": "3.6GB",
                "idle_container_device_memory_usage": "3.4GB",
                "inference": {
                    "cpu_time_ms": 171.51089199999998,
                    "device_time_ms": 5.471868999999551,
                    "cpu_memory_usage_MB": 0.0,
                    "self_cpu_memory_usage_MB": 0.0,
                    "device_memory_usage_MB": 8.19140625,
                    "self_device_memory_usage_MB": -129.001953125,
                    "energy_consumption_execution": 0.0,
                    "disk_IO_MB": 0.0,
                    "input_data_MB": 0.0,
                    "output_data_MB": 0.0,
                    "execution_time_ms": 323.20960362752277,
                    "execution_cost": 0.0
                },
                "xai": [
                    {
                        "xai_method": "GradCAM",
                        "cpu_time_ms": 128.102668,
                        "device_time_ms": 1.680099999999999,
                        "cpu_memory_usage_MB": 0.765625,
                        "self_cpu_memory_usage_MB": 0.378997802734375,
                        "device_memory_usage_MB": 190.6484375,
                        "self_device_memory_usage_MB": 65.1611328125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 565.9523804982504,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "HiResCAM",
                        "cpu_time_ms": 25.622917,
                        "device_time_ms": 1.6638439999999928,
                        "cpu_memory_usage_MB": 0.765625,
                        "self_cpu_memory_usage_MB": 0.378997802734375,
                        "device_memory_usage_MB": 85.43359375,
                        "self_device_memory_usage_MB": -45.0849609375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 451.2217839558919,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "AblationCAM",
                        "cpu_time_ms": 2546.7321690000003,
                        "device_time_ms": 2287.483079999991,
                        "cpu_memory_usage_MB": 0.765625,
                        "self_cpu_memory_usage_MB": -0.01163482666015625,
                        "device_memory_usage_MB": 96.49609375,
                        "self_device_memory_usage_MB": -248715.4375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 21090.437332789104,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "XGradCAM",
                        "cpu_time_ms": 22.41546,
                        "device_time_ms": 1.670695000000003,
                        "cpu_memory_usage_MB": 0.765625,
                        "self_cpu_memory_usage_MB": 0.378997802734375,
                        "device_memory_usage_MB": 84.62109375,
                        "self_device_memory_usage_MB": -44.3349609375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 422.3488966623942,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "GradCAMPlusPlus",
                        "cpu_time_ms": 23.371682,
                        "device_time_ms": 1.6659899999999916,
                        "cpu_memory_usage_MB": 0.765625,
                        "self_cpu_memory_usage_MB": 0.378997802734375,
                        "device_memory_usage_MB": 82.90234375,
                        "self_device_memory_usage_MB": -42.3974609375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 415.4957135518392,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "ScoreCAM",
                        "cpu_time_ms": 2464.1027630000003,
                        "device_time_ms": 2336.9675850000076,
                        "cpu_memory_usage_MB": 784.3828125,
                        "self_cpu_memory_usage_MB": -0.027256011962890625,
                        "device_memory_usage_MB": 82.90234375,
                        "self_device_memory_usage_MB": -253668.060546875,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 29110.875368118286,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "LayerCAM",
                        "cpu_time_ms": 23.468355,
                        "device_time_ms": 1.6646129999999992,
                        "cpu_memory_usage_MB": 0.765625,
                        "self_cpu_memory_usage_MB": 0.378997802734375,
                        "device_memory_usage_MB": 82.90234375,
                        "self_device_memory_usage_MB": -42.3974609375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 434.1920216878255,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenCAM",
                        "cpu_time_ms": 328.232484,
                        "device_time_ms": 1.664332999999984,
                        "cpu_memory_usage_MB": 0.3828125,
                        "self_cpu_memory_usage_MB": -0.003814697265625,
                        "device_memory_usage_MB": 82.90234375,
                        "self_device_memory_usage_MB": -42.396484375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 413.7431780497233,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenGradCAM",
                        "cpu_time_ms": 203.765798,
                        "device_time_ms": 1.6654799999999987,
                        "cpu_memory_usage_MB": 0.765625,
                        "self_cpu_memory_usage_MB": 0.378997802734375,
                        "device_memory_usage_MB": 82.90234375,
                        "self_device_memory_usage_MB": -42.3974609375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 605.5757999420166,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "KPCA_CAM",
                        "cpu_time_ms": 16.058455,
                        "device_time_ms": 1.6651079999999887,
                        "cpu_memory_usage_MB": 0.3828125,
                        "self_cpu_memory_usage_MB": -0.003814697265625,
                        "device_memory_usage_MB": 82.90234375,
                        "self_device_memory_usage_MB": -42.396484375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 200.6988525390625,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "RandomCAM",
                        "cpu_time_ms": 26.230151,
                        "device_time_ms": 1.6675619999999975,
                        "cpu_memory_usage_MB": 0.765625,
                        "self_cpu_memory_usage_MB": 0.378997802734375,
                        "device_memory_usage_MB": 82.90234375,
                        "self_device_memory_usage_MB": -42.3974609375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1235.6553077697754,
                        "execution_cost": 0.0
                    }
                ]
            },
            {
                "node_id": "LAP004262",
                "device_type": "DeviceType.CPU",
                "device_name": "None",
                "initialization_time_ms": 15958.317041397095,
                "eviction_time_ms": 0.0,
                "initialization_cost": 0.0,
                "keep_alive_cost": 0.0,
                "energy_consumption_idle": 0.0,
                "idle_container_cpu_memory_usage": "1.4GB",
                "idle_container_device_memory_usage": "0GB",
                "inference": {
                    "cpu_time_ms": 60.512445,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 0.003814697265625,
                    "self_cpu_memory_usage_MB": -122.220703125,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0.0,
                    "disk_IO_MB": 0.0,
                    "input_data_MB": 0.0,
                    "output_data_MB": 0.0,
                    "execution_time_ms": 142.1976089477539,
                    "execution_cost": 0.0
                },
                "xai": [
                    {
                        "xai_method": "GradCAM",
                        "cpu_time_ms": 207.431249,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 179.80865478515625,
                        "self_cpu_memory_usage_MB": -139.88883209228516,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 456.406831741333,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "HiResCAM",
                        "cpu_time_ms": 212.796587,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 82.31631469726562,
                        "self_cpu_memory_usage_MB": -139.88883209228516,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 554.9743970235189,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "XGradCAM",
                        "cpu_time_ms": 215.191887,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 82.31631469726562,
                        "self_cpu_memory_usage_MB": -139.88883209228516,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 561.3647301991781,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "GradCAMPlusPlus",
                        "cpu_time_ms": 237.48627399999998,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 82.31631469726562,
                        "self_cpu_memory_usage_MB": -139.88883209228516,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 496.6437021891276,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "LayerCAM",
                        "cpu_time_ms": 233.310481,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 82.31631469726562,
                        "self_cpu_memory_usage_MB": -139.88883209228516,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 570.0839360555013,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenCAM",
                        "cpu_time_ms": 390.83834099999996,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 81.93350219726562,
                        "self_cpu_memory_usage_MB": -42.396484375,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 424.9148368835449,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenGradCAM",
                        "cpu_time_ms": 471.613328,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 82.31631469726562,
                        "self_cpu_memory_usage_MB": -139.88883209228516,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 766.4829095204672,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "KPCA_CAM",
                        "cpu_time_ms": 82.38864600000001,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 81.93350219726562,
                        "self_cpu_memory_usage_MB": -42.396484375,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 188.76123428344727,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "RandomCAM",
                        "cpu_time_ms": 208.612707,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 82.31631469726562,
                        "self_cpu_memory_usage_MB": -139.88883209228516,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 573.2090473175049,
                        "execution_cost": 0.0
                    }
                ]
            }
        ],
        "feedback": {
            "likes": [],
            "dislikes": [],
            "comments": []
        },
        "code": {
            "readme_content": "---\nlicense: apache-2.0\ntags:\n- vision\n- image-classification\ndatasets:\n- imagenet-1k\n---\n\n# ResNet-50 v1.5\n\nResNet model pre-trained on ImageNet-1k at resolution 224x224. It was introduced in the paper [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) by He et al. \n\nDisclaimer: The team releasing ResNet did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nResNet (Residual Network) is a convolutional neural network that democratized the concepts of residual learning and skip connections. This enables to train much deeper models.\n\nThis is ResNet v1.5, which differs from the original model: in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate (\\~0.5% top1) than v1, but comes with a small performance drawback (~5% imgs/sec) according to [Nvidia](https://catalog.ngc.nvidia.com/orgs/nvidia/resources/resnet_50_v1_5_for_pytorch).\n\n![model image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/resnet_architecture.png)\n\n## Intended uses & limitations\n\nYou can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=resnet) to look for\nfine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\n\n```python\nfrom transformers import AutoImageProcessor, ResNetForImageClassification\nimport torch\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingface/cats-image\")\nimage = dataset[\"test\"][\"image\"][0]\n\nprocessor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\nmodel = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\")\n\ninputs = processor(image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label])\n```\n\nFor more code examples, we refer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/resnet).\n\n### BibTeX entry and citation info\n\n```bibtex\n@inproceedings{he2016deep,\n  title={Deep residual learning for image recognition},\n  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},\n  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},\n  pages={770--778},\n  year={2016}\n}\n```\n",
            "dockerfile_content": "# Base image for AI service powered by HuggingFace pre-trained AI models.\n# the image has the following packages/libraries installed already:\n# - python3.12, pip, git, fastapi, uvicorn, torch, torchvision, opencv-python, transformers, python-multipart, Pillow, requests\nFROM python3.12_ai_service_base:latest\n\n# Set working directory\nWORKDIR /app\n\n# Copy application code\nCOPY . .\n\n# Install additional dependencies\nRUN pip install datasets\n\n# Expose port 8000\nEXPOSE 8000\n\n# Start the FastAPI server\nCMD [\"uvicorn\", \"ai_server:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--timeout-keep-alive\", \"600\"]",
            "ai_server_script_content": "import json\nimport os\nimport time\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import JSONResponse\nfrom contextlib import asynccontextmanager\nfrom ai_server_utils import (\n    PROFILE_OUTPUT_JSON_SPEC,\n    NODE_ID,\n    K8S_POD_NAME,\n)\n\n\n# -------------------------------------------\n# App Lifespan setup\n# -------------------------------------------\n# Record the script start time (when uvicorn starts the process)\nSCRIPT_START_TIME = time.time()\nINITIALIZATION_DURATION = 0.0\nservice_endpoint_specs = {\n    \"model_input_form_spec\": None,\n    \"model_output_json_spec\": None,\n    \"profile_output_json_spec\": None,\n    \"xai_model_input_form_spec\": None,\n    \"xai_model_output_json_spec\": None,\n    \"xai_profile_output_json_spec\": None,\n}\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n\n    global INITIALIZATION_DURATION\n    global SCRIPT_START_TIME\n    global service_endpoint_specs\n\n    # Load the AI model\n    print(\"Loading AI model...\")\n    from model import (\n        MODEL_INPUT_FORM_SPEC,\n        MODEL_OUTPUT_JSON_SPEC,\n        router as model_router,\n    )\n\n    service_endpoint_specs[\"model_input_form_spec\"] = MODEL_INPUT_FORM_SPEC\n    service_endpoint_specs[\"model_output_json_spec\"] = MODEL_OUTPUT_JSON_SPEC\n    service_endpoint_specs[\"profile_output_json_spec\"] = PROFILE_OUTPUT_JSON_SPEC\n\n    app.include_router(model_router, prefix=\"/model\", tags=[\"AI Model\"])\n\n    # Load the XAI model\n    if os.path.exists(os.path.dirname(__file__) + \"/xai_model.py\"):\n        print(\"Loading XAI model...\")\n        from xai_model import (\n            XAI_OUTPUT_JSON_SPEC,\n            router as xai_model_router,\n        )\n\n        # by default, the xai_model input form spec is the same as the model input form spec\n        service_endpoint_specs[\"xai_model_input_form_spec\"] = MODEL_INPUT_FORM_SPEC\n        service_endpoint_specs[\"xai_model_output_json_spec\"] = MODEL_OUTPUT_JSON_SPEC\n        service_endpoint_specs[\"xai_model_output_json_spec\"].update(\n            XAI_OUTPUT_JSON_SPEC\n        )\n        service_endpoint_specs[\"xai_profile_output_json_spec\"] = (\n            PROFILE_OUTPUT_JSON_SPEC\n        )\n        service_endpoint_specs[\"xai_profile_output_json_spec\"].update(\n            XAI_OUTPUT_JSON_SPEC\n        )\n\n        app.include_router(xai_model_router, prefix=\"/xai_model\", tags=[\"XAI Model\"])\n\n    # Record the initialization duration\n    INITIALIZATION_DURATION = time.time() - SCRIPT_START_TIME\n\n    print(f\"AI service loaded in {INITIALIZATION_DURATION:.2f} seconds.\")\n\n    yield\n\n    # Clean up the models and release the resources\n    service_endpoint_specs.clear()\n\n\n# -------------------------------------------\n# FastAPI application setup\n# -------------------------------------------\napp = FastAPI(lifespan=lifespan)\n\n\n# -------------------------------------------\n# Middlewares\n# -------------------------------------------\n@app.middleware(\"http\")\nasync def prepare_header_middleware(request: Request, call_next):\n    start_time = time.perf_counter()\n    response = await call_next(request)\n    process_time = time.perf_counter() - start_time\n    response.headers[\"X-Process-Time\"] = str(process_time)\n    response.headers[\"X-NODE-ID\"] = NODE_ID\n    response.headers[\"X-K8S-POD-NAME\"] = K8S_POD_NAME\n    return response\n\n\n# -------------------------------------------\n# General Endpoints\n# -------------------------------------------\n@app.get(\"/initialization_duration\")\ndef get_initialization_duration():\n    \"\"\"\n    Endpoint to retrieve the initialization duration of the AI model.\n    \"\"\"\n    global INITIALIZATION_DURATION\n\n    if INITIALIZATION_DURATION == 0.0:\n        return JSONResponse(\n            content={\"error\": \"Model not initialized.\"},\n            status_code=500,\n        )\n    return JSONResponse(\n        content={\n            \"initialization_duration\": INITIALIZATION_DURATION,\n            \"script_start_time\": SCRIPT_START_TIME,\n        }\n    )\n\n\n@app.get(\"/help\")\ndef get_help():\n    global service_endpoint_specs\n    help_info = {\n        \"endpoints\": {\n            \"/model/run\": {\n                \"method\": \"POST\",\n                \"description\": \"Executes the AI model with the provided input data.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_output_json_spec\"],\n                },\n            },\n            \"/model/profile_run\": {\n                \"method\": \"POST\",\n                \"description\": \"Profiles the AI model execution.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    \"profile_result\": \"Profiling results of the AI model execution.\",\n                    **service_endpoint_specs[\"profile_output_json_spec\"],\n                },\n            },\n            \"/initialization_duration\": {\n                \"method\": \"GET\",\n                \"description\": \"Retrieves the initialization duration of the AI model.\",\n                \"response\": {\n                    \"initialization_duration\": \"Time taken to initialize the model (in seconds).\",\n                    \"script_start_time\": \"Timestamp when the script started (in seconds since epoch).\",\n                },\n            },\n        }\n    }\n\n    if service_endpoint_specs[\"xai_model_input_form_spec\"] is not None:\n        help_info[\"endpoints\"][\"/xai_model/run\"] = {\n            \"method\": \"POST\",\n            \"description\": \"Executes the XAI model with the provided input data.\",\n            \"parameters\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_input_form_spec\"],\n            },\n            \"response\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_output_json_spec\"],\n            },\n        }\n\n        help_info[\"endpoints\"][\"/xai_model/profile_run\"] = {\n            \"method\": \"POST\",\n            \"description\": \"Profiles the XAI model execution.\",\n            \"parameters\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_input_form_spec\"],\n            },\n            \"response\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                \"profile_result\": \"Profiling results of the XAI model execution.\",\n                **service_endpoint_specs[\"xai_profile_output_json_spec\"],\n            },\n        }\n    \n    return JSONResponse(content=help_info)\n",
            "ai_client_script_content": "import base64\nimport json\nimport time\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nfrom ai_client_utils import (\n    prepare_ai_service_request_files,\n    prepare_ai_service_request_data,\n)\n\nXAI_GRADCAM_METHODS = [\n    \"GradCAM\",\n    \"HiResCAM\",\n    \"AblationCAM\",\n    \"XGradCAM\",\n    \"GradCAMPlusPlus\",\n    \"ScoreCAM\",\n    \"LayerCAM\",\n    \"EigenCAM\",\n    \"EigenGradCAM\",\n    \"KPCA_CAM\",\n    \"RandomCAM\",\n]\n\n# -------------------------------------\n# prompt for necessary inputs\n# -------------------------------------\nSERVER_URL = input(\"Please input server URL (default to http://localhost:9000): \")\nif SERVER_URL.strip() == \"\":\n    SERVER_URL = \"http://localhost:9000\"\nUE_ID = input(\"Please input UE_ID (default to 123456): \")\nif UE_ID.strip() == \"\":\n    UE_ID = \"123456\"\n\n\ndef send_post_request(url, data, files):\n    \"\"\"Send request to run AI service and display AI service responses.\"\"\"\n    try:\n        response = requests.post(url, files=files, data=data)\n        # get the process time, node id and k8s pod name from the response headers\n        process_time = response.headers.get(\"X-Process-Time\")\n        node_id = response.headers.get(\"X-NODE-ID\")\n        k8s_pod_name = response.headers.get(\"X-K8S-POD-NAME\")\n        if response.status_code == 200:\n            return response.json(), process_time, node_id, k8s_pod_name\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n\ndef send_get_request(url, params=None):\n    \"\"\"Send GET request to the specified URL and return the response.\"\"\"\n    try:\n        response = requests.get(url, params=params)\n        process_time = response.headers.get(\"X-Process-Time\")\n        node_id = response.headers.get(\"X-NODE-ID\")\n        k8s_pod_name = response.headers.get(\"X-K8S-POD-NAME\")\n        if response.status_code == 200:\n            return response.json(), process_time, node_id, k8s_pod_name\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n            return None\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n\nclass ProfileResultProcessor:\n\n    def __init__(self, server_url):\n        self.server_url = server_url\n        self.start_time = time.time()\n        self.service_initialization_duration = 0\n        self.response_counter = 0\n        self.profile_name = None\n        self.device_type = None\n        self.device_name = None\n        self.node_id = None\n        self.k8s_pod_name = None\n        self.cpu_memory_usage_bytes = 0\n        self.self_cpu_memory_usage_bytes = 0\n        self.device_memory_usage_bytes = 0\n        self.self_device_memory_usage_bytes = 0\n        self.cpu_time_total_us = 0\n        self.self_cpu_time_total_us = 0\n        self.device_time_total_us = 0\n        self.self_device_time_total_us = 0\n\n        # xai related\n        self.gradcam_method_name = None\n\n        self.fetch_service_initialization_duration()\n\n    def fetch_service_initialization_duration(self):\n        \"\"\"Fetch the service initialization duration from the server.\"\"\"\n        response, process_time, node_id, k8s_pod_name = send_get_request(\n            f\"{self.server_url}/initialization_duration\"\n        )\n        if response:\n            self.service_initialization_duration = response.get(\n                \"initialization_duration\", 0\n            )\n        else:\n            print(\"Failed to fetch initialization duration.\")\n            self.service_initialization_duration = 0\n\n    def process_new_response(\n        self,\n        profile_response,\n        process_time=None,\n        node_id=None,\n        k8s_pod_name=None,\n        gradcam_method_name=None,\n    ):\n        if not profile_response:\n            return\n\n        profile_result = profile_response[\"profile_result\"]\n\n        if not profile_result:\n            return\n\n        if self.profile_name is None:\n            self.profile_name = profile_result[\"name\"]\n        if self.device_type is None:\n            self.device_type = profile_result[\"device_type\"]\n        if self.device_name is None:\n            self.device_name = profile_result[\"device_name\"]\n        if self.node_id is None:\n            self.node_id = node_id\n        if self.k8s_pod_name is None:\n            self.k8s_pod_name = k8s_pod_name\n\n        if self.gradcam_method_name is None:\n            self.gradcam_method_name = gradcam_method_name\n\n        # update the max profile result\n        self.cpu_memory_usage_bytes = max(\n            self.cpu_memory_usage_bytes, profile_result[\"cpu_memory_usage\"]\n        )\n        # self cpu memory usage could be negative. here we take the value that has the max absolute value\n        if abs(profile_result[\"self_cpu_memory_usage\"]) > abs( self.self_cpu_memory_usage_bytes):\n            self.self_cpu_memory_usage_bytes = profile_result[\"self_cpu_memory_usage\"]\n        self.device_memory_usage_bytes = max(\n            self.device_memory_usage_bytes, profile_result[\"device_memory_usage\"]\n        )\n        # same as self cpu memory usage\n        if abs(profile_result[\"self_device_memory_usage\"]) > abs(self.self_device_memory_usage_bytes):\n            self.self_device_memory_usage_bytes = profile_result[\"self_device_memory_usage\"]\n        self.cpu_time_total_us = max(\n            self.cpu_time_total_us, profile_result[\"cpu_time_total\"]\n        )\n        self.self_cpu_time_total_us = max(\n            self.self_cpu_time_total_us, profile_result[\"self_cpu_time_total\"]\n        )\n        self.device_time_total_us = max(\n            self.device_time_total_us, profile_result[\"device_time_total\"]\n        )\n        self.self_device_time_total_us = max(\n            self.self_device_time_total_us, profile_result[\"self_device_time_total\"]\n        )\n\n        self.response_counter += 1\n\n    def complete_profile(self):\n        print(\"\\n--------- PROFILE EVENT ---------\\n\")\n        print(f\"Name: {self.profile_name}\")\n        print(f\"Device Type: {self.device_type}\")\n        print(f\"Device Name: {self.device_name}\")\n        print(f\"Node ID: {self.node_id}\")\n        print(f\"K8S_POD_NAME: {self.k8s_pod_name}\")\n\n        if self.gradcam_method_name:\n            print(f\"GradCAM Method Name: {self.gradcam_method_name}\")\n\n        print(\"\\n--------- LATENCY RESULT ---------\\n\")\n        print(\"Service Initialization Duration: \", self.service_initialization_duration)\n        print(\"Total Requests: \", self.response_counter)\n        print(f\"Total Time Taken: {time.time() - self.start_time:.2f} seconds\")\n        print(\n            f\"Average Time Taken: {(time.time() - self.start_time) / self.response_counter:.2f} seconds\"\n        )\n\n        print(\"\\n--------- RESOURCE USAGE ---------\\n\")\n        print(f\"CPU Memory Usage: {self.cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\")\n        print(\n            f\"Self CPU Memory Usage: {self.self_cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Device Memory Usage: {self.device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Self Device Memory Usage: {self.self_device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(f\"CPU Time Total: {self.cpu_time_total_us / 1000:.2f} ms\")\n        print(f\"Self CPU Time Total: {self.self_cpu_time_total_us / 1000:.2f} ms\")\n        print(f\"Device Time Total: {self.device_time_total_us / 1000:.2f} ms\")\n        print(f\"Self Device Time Total: {self.self_device_time_total_us / 1000:.2f} ms\")\n\n        # update the service_data.json automatically\n        with open(\"service_data.json\", \"r\") as f:\n            service_data = json.load(f)\n\n        if not self.gradcam_method_name:\n            complete_profile_data_to_save = {\n                \"node_id\": self.node_id,\n                \"device_type\": self.device_type,\n                \"device_name\": self.device_name,\n                \"initialization_time_ms\": self.service_initialization_duration * 1000,\n                \"eviction_time_ms\": 0,\n                \"initialization_cost\": 0,\n                \"keep_alive_cost\": 0,\n                \"energy_consumption_idle\": 0,\n                \"inference\": {\n                    \"cpu_time_ms\": self.cpu_time_total_us / 1000,\n                    \"device_time_ms\": self.device_time_total_us / 1000,\n                    \"cpu_memory_usage_MB\": self.cpu_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"self_cpu_memory_usage_MB\": self.self_cpu_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"device_memory_usage_MB\": self.device_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"self_device_memory_usage_MB\": self.self_device_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"energy_consumption_execution\": 0,\n                    \"disk_IO_MB\": 0,\n                    \"input_data_MB\": 0,\n                    \"output_data_MB\": 0,\n                    \"execution_time_ms\": (time.time() - self.start_time)\n                    / self.response_counter\n                    * 1000,\n                    \"execution_cost\": 0,\n                },\n            }\n\n            # check if there is already a profile for this node id\n            profile_found = False\n            for profile in service_data[\"profiles\"]:\n                if profile[\"node_id\"] == self.node_id:\n                    profile[\"inference\"] = complete_profile_data_to_save[\"inference\"]\n                    profile_found = True\n                    break\n            if not profile_found:\n                service_data[\"profiles\"].append(complete_profile_data_to_save)\n\n            # save the updated service_data.json\n            with open(\"service_data.json\", \"w\") as f:\n                json.dump(service_data, f, indent=4)\n            print(\"\\n--------- SERVICE DATA UPDATED ---------\\n\")\n\n        else:\n            complete_xai_profile_data_to_save = {\n                \"node_id\": self.node_id,\n                \"device_type\": self.device_type,\n                \"device_name\": self.device_name,\n                \"initialization_time_ms\": self.service_initialization_duration * 1000,\n                \"eviction_time_ms\": 0,\n                \"initialization_cost\": 0,\n                \"keep_alive_cost\": 0,\n                \"energy_consumption_idle\": 0,\n                \"xai\": [\n                    {\n                        \"xai_method\": self.gradcam_method_name,\n                        \"cpu_time_ms\": self.cpu_time_total_us / 1000,\n                        \"device_time_ms\": self.device_time_total_us / 1000,\n                        \"cpu_memory_usage_MB\": self.cpu_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"self_cpu_memory_usage_MB\": self.self_cpu_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"device_memory_usage_MB\": self.device_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"self_device_memory_usage_MB\": self.self_device_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"energy_consumption_execution\": 0,\n                        \"disk_IO_MB\": 0,\n                        \"input_data_MB\": 0,\n                        \"output_data_MB\": 0,\n                        \"execution_time_ms\": (time.time() - self.start_time)\n                        / self.response_counter\n                        * 1000,\n                        \"execution_cost\": 0,\n                    }\n                ],\n            }\n\n            # check if there is already a profile for this node id\n            profile_found = False\n            for profile in service_data[\"profiles\"]:\n                if profile[\"node_id\"] == self.node_id:\n                    profile_found = True\n\n                    # check if there is already a profile for this xai method\n                    xai_method_found = False\n                    if not profile.get(\"xai\"):\n                        profile[\"xai\"] = []\n                    for xai_profile in profile[\"xai\"]:\n                        if xai_profile[\"xai_method\"] == self.gradcam_method_name:\n                            xai_profile.update(complete_xai_profile_data_to_save[\"xai\"][0]) \n                            xai_method_found = True\n                            break\n                    if not xai_method_found:\n                        profile[\"xai\"].append(complete_xai_profile_data_to_save[\"xai\"][0])\n                    break\n\n            if not profile_found:\n                service_data[\"profiles\"].append(complete_xai_profile_data_to_save)\n\n            # save the updated service_data.json\n            with open(\"service_data.json\", \"w\") as f:\n                json.dump(service_data, f, indent=4)\n            print(\"\\n--------- SERVICE DATA UPDATED ---------\\n\")\n\n\ndef option_run():\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n    data = {**data, \"ue_id\": UE_ID}\n    response, process_time, node_id, pod_name = send_post_request(\n        f\"{SERVER_URL}/model/run\", data, files\n    )\n    print(\"Process Time: \", process_time)\n    print(\"Node ID: \", node_id)\n    print(\"K8S_POD_NAME: \", pod_name)\n    print(\"Response\")\n    print(json.dumps(response, indent=4))\n\n\ndef option_profile_run():\n    data = prepare_ai_service_request_data()\n    data = {**data, \"ue_id\": UE_ID}\n    files = prepare_ai_service_request_files()\n    num_requests = int(input(\"Enter the number of requests to send: \"))\n\n    try:\n        profile_result_processor = ProfileResultProcessor(SERVER_URL)\n\n        for _ in range(num_requests):\n            profile_response, process_time, node_id, k8s_node_name = send_post_request(\n                f\"{SERVER_URL}/model/profile_run\", data, files\n            )\n            print(\"Process Time: \", process_time)\n            print(\"Node ID: \", node_id)\n            print(\"K8S_POD_NAME: \", k8s_node_name)\n            print(\"Response\")\n            print(json.dumps(profile_response, indent=4))\n            if not profile_response:\n                print(\"No profile response received.\")\n                continue\n\n            profile_result_processor.process_new_response(\n                profile_response,\n                process_time=process_time,\n                node_id=node_id,\n                k8s_pod_name=k8s_node_name,\n            )\n\n        # Print the final profile result and update the service_data.json\n        profile_result_processor.complete_profile()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\n\ndef option_help():\n    \"\"\"Get help information from the server.\"\"\"\n    try:\n        response, process_time, node_id, pod_name = send_get_request(\n            f\"{SERVER_URL}/help\"\n        )\n        print(\"Process Time: \", process_time)\n        print(\"Node ID: \", node_id)\n        print(\"K8S_POD_NAME: \", pod_name)\n        print(\"Response\")\n        print(json.dumps(response, indent=4))\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n\n\ndef option_run_with_xai():\n    \"\"\"Run AI service with XAI.\"\"\"\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n\n    print(\n        \"Note that currently only GradCAM methods on image-classification models are supported.\"\n    )\n    while True:\n        gradcam_method_name = input(\n            f\"Please select a GradCAM method (options: {XAI_GRADCAM_METHODS}): \"\n        )\n        if gradcam_method_name not in XAI_GRADCAM_METHODS:\n            print(f\"Invalid GradCAM method. Please select again.\")\n        else:\n            break\n\n    # ask for target class for explanation\n    target_category_indexes = input(\n        \"Please input target category indexes for explanation (comma-separated, e.g., 111, 32, 44, ...): \"\n    )\n    if not target_category_indexes or not target_category_indexes.strip():\n        print(\n            \"No target category indexes provided. Defaulting to explaining the top confident category.\"\n        )\n        target_category_indexes = []\n    else:\n        target_category_indexes = [\n            int(i.strip()) for i in target_category_indexes.split(\",\")\n        ]\n\n    data = {\n        **data,\n        \"ue_id\": UE_ID,\n        \"gradcam_method_name\": gradcam_method_name,\n        \"target_category_indexes\": target_category_indexes,\n    }\n    print(\"Data: \", data)\n    response, process_time, node_id, k8s_pod_name = send_post_request(\n        f\"{SERVER_URL}/xai_model/run\", data, files\n    )\n    print(\"Process Time: \", process_time)\n    print(\"Node ID: \", node_id)\n    print(\"K8S_POD_NAME: \", k8s_pod_name)\n\n    # Handle JSON response\n    model_results = response.get(\"model_results\")\n    if model_results:\n        print(\"Model Results:\", json.dumps(model_results, indent=4))\n\n    xai_results = response.get(\"xai_results\")\n    if xai_results:\n        print(\"XAI Results Method:\", xai_results.get(\"xai_method\"))\n\n    # Handle binary image response\n    encoded_image = xai_results.get(\"image\")\n    if encoded_image:\n        image_bytes = base64.b64decode(encoded_image)\n\n        # Load the image into Pillow\n        image = Image.open(BytesIO(image_bytes))\n\n        # Save image to disk\n        image.save(\"xai_output.png\")\n\n        # Display the image using matplotlib\n        plt.imshow(image)\n        plt.axis(\"off\")\n        plt.show()\n\n\ndef option_profile_run_with_xai():\n    \"\"\"Run AI service with XAI and profile the run.\"\"\"\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n    print(\n        \"Note that currently only GradCAM methods on image-classification models are supported.\"\n    )\n\n    # ask for target class for explanation\n    target_category_indexes = input(\n        \"Please input target category indexes for explanation (comma-separated, e.g., 111, 32, 44, ...): \"\n    )\n    if not target_category_indexes or not target_category_indexes.strip():\n        print(\n            \"No target category indexes provided. Defaulting to explaining the top confident category.\"\n        )\n        target_category_indexes = []\n    else:\n        target_category_indexes = [\n            int(i.strip()) for i in target_category_indexes.split(\",\")\n        ]\n\n    num_requests = int(input(\"Enter the number of requests to send: \"))\n\n    try:\n        for gradcam_method_name in XAI_GRADCAM_METHODS:\n\n            data = {\n                **data,\n                \"ue_id\": UE_ID,\n                \"gradcam_method_name\": gradcam_method_name,\n                \"target_category_indexes\": target_category_indexes,\n            }\n            print(\"Data: \", data)\n\n            profile_result_processor = ProfileResultProcessor(SERVER_URL)\n\n            for _ in range(num_requests):\n                response, process_time, node_id, k8s_pod_name = send_post_request(\n                    f\"{SERVER_URL}/xai_model/profile_run\", data, files\n                )\n                print(\"Process Time: \", process_time)\n                print(\"Node ID: \", node_id)\n                print(\"K8S_POD_NAME: \", k8s_pod_name)\n                if not response:\n                    print(\"No profile response received.\")\n                    continue\n\n                # Handle JSON response\n                model_results = response.get(\"model_results\")\n                if model_results:\n                    print(\"Model Results:\", json.dumps(model_results, indent=4))\n\n                profile_result_processor.process_new_response(\n                    response,\n                    process_time=process_time,\n                    node_id=node_id,\n                    k8s_pod_name=k8s_pod_name,\n                    gradcam_method_name=gradcam_method_name,\n                )\n\n            # Print the final profile result and update the service_data.json\n            profile_result_processor.complete_profile()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\n\nOPTIONS = [\n    {\n        \"label\": \"Get help information\",\n        \"action\": option_help,\n    },\n    {\n        \"label\": \"Run AI service\",\n        \"action\": option_run,\n    },\n    {\n        \"label\": \"Profile AI service\",\n        \"action\": option_profile_run,\n    },\n    {\n        \"label\": \"Run AI service with XAI (only image-classification models)\",\n        \"action\": option_run_with_xai,\n    },\n    {\n        \"label\": \"Profile AI service with XAI (only image-classification models)\",\n        \"action\": option_profile_run_with_xai,\n    },\n]\n\n\nif __name__ == \"__main__\":\n    while True:\n        print(\"\\nOptions:\")\n        for i, option in enumerate(OPTIONS, start=1):\n            print(f\"{i}. {option['label']}\")\n        print(\"q. Quit\")\n        choice = input(\"Enter your choice: \")\n\n        if choice == \"q\":\n            print(\"Exiting the client. Goodbye!\")\n            break\n        else:\n            try:\n                choice = int(choice)\n                if 1 <= choice <= len(OPTIONS):\n                    OPTIONS[choice - 1][\"action\"]()\n                else:\n                    print(\"Invalid choice. Please try again.\")\n            except ValueError:\n                print(\"Invalid input. Please enter a number.\")\n",
            "ai_client_utils_script_content": "def prepare_ai_service_request_files():\n    \"\"\"Prepare the `files` part for the AI service request.\"\"\"\n    files = {}\n    image_file_path = input(\"Please input the image file path: \")\n    with open(image_file_path, \"rb\") as image_file:\n        files[\"file\"] = image_file.read()\n    return files\n\ndef prepare_ai_service_request_data():\n    \"\"\"Prepare the `data` part other than `ue_id` for the AI service request.\"\"\"\n    data = {}\n    # Prompt the user to input the unique execution ID\n    data[\"ue_id\"] = input(\"Please input the unique execution ID (ue_id): \")\n    return data",
            "model_script_content": "# import server utils\nfrom ai_server_utils import (\n    process_model_output_logits,\n    profile_activities,\n    prepare_profile_results,\n)\n# import profile utils\nfrom torch.profiler import profile, record_function\n\n# import necessary libs for AI model inference and request handling\nimport torch\nfrom fastapi import APIRouter, File, Form, UploadFile\nfrom fastapi.responses import JSONResponse\nfrom transformers import AutoImageProcessor, ResNetForImageClassification\nfrom PIL import Image\n\n# --------------------------------\n# Device configuration\n# --------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# --------------------------------\n# Model-specific configuration\n# make sure the variables `MODEL_NAME` and `model` are defined here.\n# --------------------------------\nMODEL_NAME = \"microsoft/resnet-50\"\nprocessor = AutoImageProcessor.from_pretrained(MODEL_NAME, use_fast=True)\nmodel = ResNetForImageClassification.from_pretrained(MODEL_NAME).to(device)\nmodel.eval()\n\n# Initialize the FastAPI router\nrouter = APIRouter()\n\n@router.post(\"/run\")\nasync def run_model(file: UploadFile = File(...), ue_id: str = Form(...)):\n    try:\n        # Prepare the model input\n        image = Image.open(file.file).convert(\"RGB\")\n        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n\n        # Perform inference\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        # Process the model outputs\n        predictions = process_model_output_logits(model, outputs.logits)\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"model_results\": predictions,\n            }\n        )\n    except Exception as e:\n        print(f\"Error processing file: {e}\")\n        return JSONResponse(\n            content={\"error\": \"Failed to process the image. {e}\".format(e=str(e))},\n            status_code=500,\n        )\n\n@router.post(\"/profile_run\")\nasync def profile_run(file: UploadFile = File(...), ue_id: str = Form(...)):\n    \"\"\"\n    Endpoint to profile the AI model execution.\n    \"\"\"\n    try:\n        # Prepare the model input\n        image = Image.open(file.file).convert(\"RGB\")\n        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n\n        # perform profiling\n        with profile(\n            activities=profile_activities,\n            profile_memory=True,\n        ) as prof:\n            with record_function(\"model_run\"):\n                with torch.no_grad():\n                    model_outputs = model(**inputs)\n\n        profile_result = prepare_profile_results(prof)\n\n        # Process the model outputs\n        predictions = process_model_output_logits(model, model_outputs.logits)\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"profile_result\": profile_result,\n                \"model_results\": predictions,\n            }\n        )\n\n    except Exception as e:\n        print(f\"Error processing request: {e}\")\n        return JSONResponse(\n            content={\"error\": f\"Failed to process the request. {e}\"},\n            status_code=500,\n        )\n\n# Below are the model input and output specifications to be used by the `/help` endpoint\nMODEL_INPUT_FORM_SPEC = {\n    \"file\": {\n        \"type\": \"file upload\",\n        \"description\": \"The image file to be classified.\",\n        \"required\": True,\n        \"example\": \"puppy.png\",\n    }\n}\n\nMODEL_OUTPUT_JSON_SPEC = {\n    \"ue_id\": \"unique execution ID\",\n    \"model_results\": [\n        {\n            \"category_id\": \"category id\",\n            \"label\": \"category label\",\n            \"probability\": \"probability value\",\n        }\n    ],\n}",
            "healthcheck_script_content": "import requests\n\nprint(requests.get(\"http://localhost:8000/help\"))"
        }
    },
    {
        "model_name": "google/vit-large-patch32-384",
        "model_url": "https://huggingface.co/google/vit-large-patch32-384",
        "task": "image-classification",
        "task_detail": "The Vision Transformer (ViT) Large Model with patch size 32x32 and resolution 384x384 is an advanced image classification model that leverages the transformer architecture, commonly noted for its success in natural language processing, to process and classify images. It is specifically designed to handle large-scale image classification tasks, initially trained on the extensive ImageNet-21k dataset, encompassing 14 million images and 21,843 classes. The model is further fine-tuned on the more targeted ImageNet 2012 dataset, consisting of 1 million images across 1,000 classes. \n\nFunctionality of the ViT Large Model:\n\n1. **Image Classification**: The primary capability of the ViT Large Model is to classify images into one of the predefined categories present in the ImageNet dataset. The model accepts images as input and outputs a classification label along with associated confidence scores for each class.\n\n2. **Input Format**: The model expects input images to be pre-processed in a specific manner before classification:\n    - Images should be resized and rescaled to the resolution of 384x384 pixels during the inference stage.\n    - Each image is divided into non-overlapping patches of size 32x32 pixels, which are then linearly embedded to form input tokens for the transformer.\n    - The images are normalized using a mean and standard deviation of (0.5, 0.5, 0.5) across the RGB channels to standardize the input data.\n\n3. **Embedding and Tokenization**: The model utilizes a unique process of tokenization where images are divided into fixed-size patches, unlike the grid-pixel approach of conventional convolutional neural networks. These patches are then embedded into a sequenced token form and augmented by additional positional encodings to inform the transformer of the original spatial arrangement.\n\n4. **Output Format**: The output of the model is the classification of the image into one of the 1,000 ImageNet classes. The model predicts by computing a set of logits for each class. The predicted class is defined by the index of the largest logit value, and the classification label is accessed using a mapping from class index to label description.\n\n5. **Applications and Use Cases**: Due to its high accuracy and broad training base, the model is highly suitable for applications requiring robust image classification capabilities. It can be used in domains such as autonomous vehicles, medical imaging, content moderation, and any industry where image-based categorization is crucial.\n\n6. **Pre-trained Features for Transfer Learning**: Besides out-of-the-box image classification, the ViT model's design allows it to serve as a feature extractor. By leveraging its pre-trained parameters, users can adapt it for other vision-related tasks, such as object detection and segmentation, through the addition of task-specific layers on top of its architecture.\n\n7. **Hardware and Compatibility**: The model is compatible with PyTorch, and pre-trained weights are readily available for deployment. Additional support for TensorFlow and JAX/FLAX frameworks is anticipated, facilitating integration into diverse machine learning pipelines.\n\nOverall, the ViT Large Model offers a flexible, robust solution for handling complex image classification challenges across various industry applications, providing enhanced accuracy and adaptability through its unique transformer-based architecture.",
        "accuracy_info": "No accuracy information found.",
        "image_repository_url": "",
        "service_disk_size_bytes": 3609289889.0,
        "profiles": [
            {
                "node_id": "ugurcan.celik",
                "device_type": "DeviceType.CPU",
                "device_name": "cuda",
                "initialization_time_ms": 15149.816989898682,
                "eviction_time_ms": 0.0,
                "initialization_cost": 0.0,
                "keep_alive_cost": 0.0,
                "energy_consumption_idle": 0.0,
                "idle_container_cpu_memory_usage": "4.3GB",
                "idle_container_device_memory_usage": "6.7GB",
                "inference": {
                    "cpu_time_ms": 99.84129999999999,
                    "device_time_ms": 40.98774599999936,
                    "cpu_memory_usage_MB": 0.0,
                    "self_cpu_memory_usage_MB": -5.340576171875e-05,
                    "device_memory_usage_MB": 8.12890625,
                    "self_device_memory_usage_MB": -287.73828125,
                    "energy_consumption_execution": 0.0,
                    "disk_IO_MB": 0.0,
                    "input_data_MB": 0.0,
                    "output_data_MB": 0.0,
                    "execution_time_ms": 387.38298416137695,
                    "execution_cost": 0.0
                },
                "xai": [
                    {
                        "xai_method": "GradCAM",
                        "cpu_time_ms": 103.48069199999999,
                        "device_time_ms": 12.721566000000005,
                        "cpu_memory_usage_MB": 1.1253662109375,
                        "self_cpu_memory_usage_MB": 0.558685302734375,
                        "device_memory_usage_MB": 1399.490234375,
                        "self_device_memory_usage_MB": 1109.5341796875,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1282.4353377024333,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "HiResCAM",
                        "cpu_time_ms": 55.937042999999996,
                        "device_time_ms": 12.733107000000022,
                        "cpu_memory_usage_MB": 1.1253662109375,
                        "self_cpu_memory_usage_MB": 0.558685302734375,
                        "device_memory_usage_MB": 221.654296875,
                        "self_device_memory_usage_MB": -69.0986328125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1253.926436106364,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "XGradCAM",
                        "cpu_time_ms": 50.28833300000001,
                        "device_time_ms": 12.74345499999998,
                        "cpu_memory_usage_MB": 1.1253662109375,
                        "self_cpu_memory_usage_MB": 0.558685302734375,
                        "device_memory_usage_MB": 220.982421875,
                        "self_device_memory_usage_MB": -69.7705078125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1230.7701110839844,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "GradCAMPlusPlus",
                        "cpu_time_ms": 56.319424,
                        "device_time_ms": 12.724396999999994,
                        "cpu_memory_usage_MB": 1.1253662109375,
                        "self_cpu_memory_usage_MB": 0.558685302734375,
                        "device_memory_usage_MB": 221.607421875,
                        "self_device_memory_usage_MB": -69.8642578125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1243.692954381307,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "ScoreCAM",
                        "cpu_time_ms": 9930.739076,
                        "device_time_ms": 9728.085490999953,
                        "cpu_memory_usage_MB": 576.5628662109375,
                        "self_cpu_memory_usage_MB": -0.0172882080078125,
                        "device_memory_usage_MB": 221.607421875,
                        "self_device_memory_usage_MB": -298225.5517578125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 35231.55371348063,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "LayerCAM",
                        "cpu_time_ms": 53.604966,
                        "device_time_ms": 12.759118999999991,
                        "cpu_memory_usage_MB": 1.1253662109375,
                        "self_cpu_memory_usage_MB": 0.558685302734375,
                        "device_memory_usage_MB": 221.607421875,
                        "self_device_memory_usage_MB": -69.0986328125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1075.8779048919678,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenCAM",
                        "cpu_time_ms": 260.171496,
                        "device_time_ms": 12.768182000000019,
                        "cpu_memory_usage_MB": 0.5628662109375,
                        "self_cpu_memory_usage_MB": -0.003814697265625,
                        "device_memory_usage_MB": 221.607421875,
                        "self_device_memory_usage_MB": -69.09765625,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 529.6404361724854,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenGradCAM",
                        "cpu_time_ms": 157.06494899999998,
                        "device_time_ms": 12.76062400000002,
                        "cpu_memory_usage_MB": 1.1253662109375,
                        "self_cpu_memory_usage_MB": 0.558685302734375,
                        "device_memory_usage_MB": 221.607421875,
                        "self_device_memory_usage_MB": -69.0986328125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 2171.0331439971924,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "KPCA_CAM",
                        "cpu_time_ms": 28.643735,
                        "device_time_ms": 12.756501999999998,
                        "cpu_memory_usage_MB": 0.5628662109375,
                        "self_cpu_memory_usage_MB": -0.003814697265625,
                        "device_memory_usage_MB": 221.607421875,
                        "self_device_memory_usage_MB": -69.09765625,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 366.17430051167804,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "RandomCAM",
                        "cpu_time_ms": 54.911069,
                        "device_time_ms": 12.763047999999985,
                        "cpu_memory_usage_MB": 1.1253662109375,
                        "self_cpu_memory_usage_MB": 0.558685302734375,
                        "device_memory_usage_MB": 221.607421875,
                        "self_device_memory_usage_MB": -69.0986328125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1376.2470086415608,
                        "execution_cost": 0.0
                    }
                ]
            },
            {
                "node_id": "LAP004262",
                "device_type": "DeviceType.CPU",
                "device_name": "None",
                "initialization_time_ms": 145240.5915260315,
                "eviction_time_ms": 0.0,
                "initialization_cost": 0.0,
                "keep_alive_cost": 0.0,
                "energy_consumption_idle": 0.0,
                "idle_container_cpu_memory_usage": "2.7GB",
                "idle_container_device_memory_usage": "0GB",
                "inference": {
                    "cpu_time_ms": 576.1604010000001,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 0.003814697265625,
                    "self_cpu_memory_usage_MB": -301.32421875,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0.0,
                    "disk_IO_MB": 0.0,
                    "input_data_MB": 0.0,
                    "output_data_MB": 0.0,
                    "execution_time_ms": 898.7720807393392,
                    "execution_cost": 0.0
                },
                "xai": [
                    {
                        "xai_method": "GradCAM",
                        "cpu_time_ms": 1870.9915449999999,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 1405.0242385864258,
                        "self_cpu_memory_usage_MB": -1239.3710098266602,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 2726.0491053263345,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "HiResCAM",
                        "cpu_time_ms": 2132.0065689999997,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 234.75089263916016,
                        "self_cpu_memory_usage_MB": -1239.3710098266602,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 3363.117218017578,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "XGradCAM",
                        "cpu_time_ms": 1744.339913,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 234.75089263916016,
                        "self_cpu_memory_usage_MB": -1239.3710098266602,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 2971.3034629821777,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "GradCAMPlusPlus",
                        "cpu_time_ms": 1907.582829,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 234.75089263916016,
                        "self_cpu_memory_usage_MB": -1239.3710098266602,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 3140.568415323893,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "LayerCAM",
                        "cpu_time_ms": 1889.090777,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 234.75089263916016,
                        "self_cpu_memory_usage_MB": -1239.3710098266602,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 2953.1263510386148,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenCAM",
                        "cpu_time_ms": 888.1809599999999,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 234.18448638916016,
                        "self_cpu_memory_usage_MB": -69.09765625,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1345.0966676076252,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenGradCAM",
                        "cpu_time_ms": 3277.031181,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 234.75089263916016,
                        "self_cpu_memory_usage_MB": -1239.3710098266602,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 4240.844011306763,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "KPCA_CAM",
                        "cpu_time_ms": 777.036159,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 234.18448638916016,
                        "self_cpu_memory_usage_MB": -69.09765625,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1083.1336975097656,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "RandomCAM",
                        "cpu_time_ms": 1792.9586270000002,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 234.75089263916016,
                        "self_cpu_memory_usage_MB": -1239.3710098266602,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 3061.691999435425,
                        "execution_cost": 0.0
                    }
                ]
            }
        ],
        "feedback": {
            "likes": [],
            "dislikes": [],
            "comments": []
        },
        "code": {
            "readme_content": "---\nlicense: apache-2.0\ntags:\n- image-classification\n- vision\ndatasets:\n- imagenet\n- imagenet-21k\n---\n\n# Vision Transformer (large-sized model) \n\nVision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 384x384. It was introduced in the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Dosovitskiy et al. and first released in [this repository](https://github.com/google-research/vision_transformer). However, the weights were converted from the [timm repository](https://github.com/rwightman/pytorch-image-models) by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him. \n\nDisclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. Next, the model was fine-tuned on ImageNet (also referred to as ILSVRC2012), a dataset comprising 1 million images and 1,000 classes, at a higher resolution of 384x384.\n\nImages are presented to the model as a sequence of fixed-size patches (resolution 32x32), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\n\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.\n\n## Intended uses & limitations\n\nYou can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=google/vit) to look for\nfine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\n\n```python\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-large-patch32-384')\nmodel = ViTForImageClassification.from_pretrained('google/vit-large-patch32-384')\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n```\n\nCurrently, both the feature extractor and model  support PyTorch. Tensorflow and JAX/FLAX are coming soon, and the API of ViTFeatureExtractor might change.\n\n## Training data\n\nThe ViT model was pretrained on [ImageNet-21k](http://www.image-net.org/), a dataset consisting of 14 million images and 21k classes, and fine-tuned on [ImageNet](http://www.image-net.org/challenges/LSVRC/2012/), a dataset consisting of 1 million images and 1k classes. \n\n## Training procedure\n\n### Preprocessing\n\nThe exact details of preprocessing of images during training/validation can be found [here](https://github.com/google-research/vision_transformer/blob/master/vit_jax/input_pipeline.py). \n\nImages are resized/rescaled to the same resolution (224x224 during pre-training, 384x384 during fine-tuning) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).\n\n### Pretraining\n\nThe model was trained on TPUv3 hardware (8 cores). All model variants are trained with a batch size of 4096 and learning rate warmup of 10k steps. For ImageNet, the authors found it beneficial to additionally apply gradient clipping at global norm 1. Pre-training resolution is 224.\n\n## Evaluation results\n\nFor evaluation results on several image classification benchmarks, we refer to tables 2 and 5 of the original paper. Note that for fine-tuning, the best results are obtained with a higher resolution (384x384). Of course, increasing the model size will result in better performance.\n\n### BibTeX entry and citation info\n\n```bibtex\n@misc{wu2020visual,\n      title={Visual Transformers: Token-based Image Representation and Processing for Computer Vision}, \n      author={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},\n      year={2020},\n      eprint={2006.03677},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n```bibtex\n@inproceedings{deng2009imagenet,\n  title={Imagenet: A large-scale hierarchical image database},\n  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\n  booktitle={2009 IEEE conference on computer vision and pattern recognition},\n  pages={248--255},\n  year={2009},\n  organization={Ieee}\n}\n```",
            "dockerfile_content": "FROM python3.12_ai_service_base:latest\n\nWORKDIR /app\n\nCOPY . .\n\nRUN pip install transformers\n\nEXPOSE 8000\n\nCMD [\"uvicorn\", \"ai_server:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--timeout-keep-alive\", \"600\"]",
            "ai_server_script_content": "import json\nimport os\nimport time\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import JSONResponse\nfrom contextlib import asynccontextmanager\nfrom ai_server_utils import (\n    PROFILE_OUTPUT_JSON_SPEC,\n    NODE_ID,\n    K8S_POD_NAME,\n)\n\n\n# -------------------------------------------\n# App Lifespan setup\n# -------------------------------------------\n# Record the script start time (when uvicorn starts the process)\nSCRIPT_START_TIME = time.time()\nINITIALIZATION_DURATION = 0.0\nservice_endpoint_specs = {\n    \"model_input_form_spec\": None,\n    \"model_output_json_spec\": None,\n    \"profile_output_json_spec\": None,\n    \"xai_model_input_form_spec\": None,\n    \"xai_model_output_json_spec\": None,\n    \"xai_profile_output_json_spec\": None,\n}\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n\n    global INITIALIZATION_DURATION\n    global SCRIPT_START_TIME\n    global service_endpoint_specs\n\n    # Load the AI model\n    print(\"Loading AI model...\")\n    from model import (\n        MODEL_INPUT_FORM_SPEC,\n        MODEL_OUTPUT_JSON_SPEC,\n        router as model_router,\n    )\n\n    service_endpoint_specs[\"model_input_form_spec\"] = MODEL_INPUT_FORM_SPEC\n    service_endpoint_specs[\"model_output_json_spec\"] = MODEL_OUTPUT_JSON_SPEC\n    service_endpoint_specs[\"profile_output_json_spec\"] = PROFILE_OUTPUT_JSON_SPEC\n\n    app.include_router(model_router, prefix=\"/model\", tags=[\"AI Model\"])\n\n    # Load the XAI model\n    if os.path.exists(os.path.dirname(__file__) + \"/xai_model.py\"):\n        print(\"Loading XAI model...\")\n        from xai_model import (\n            XAI_OUTPUT_JSON_SPEC,\n            router as xai_model_router,\n        )\n\n        # by default, the xai_model input form spec is the same as the model input form spec\n        service_endpoint_specs[\"xai_model_input_form_spec\"] = MODEL_INPUT_FORM_SPEC\n        service_endpoint_specs[\"xai_model_output_json_spec\"] = MODEL_OUTPUT_JSON_SPEC\n        service_endpoint_specs[\"xai_model_output_json_spec\"].update(\n            XAI_OUTPUT_JSON_SPEC\n        )\n        service_endpoint_specs[\"xai_profile_output_json_spec\"] = (\n            PROFILE_OUTPUT_JSON_SPEC\n        )\n        service_endpoint_specs[\"xai_profile_output_json_spec\"].update(\n            XAI_OUTPUT_JSON_SPEC\n        )\n\n        app.include_router(xai_model_router, prefix=\"/xai_model\", tags=[\"XAI Model\"])\n\n    # Record the initialization duration\n    INITIALIZATION_DURATION = time.time() - SCRIPT_START_TIME\n\n    print(f\"AI service loaded in {INITIALIZATION_DURATION:.2f} seconds.\")\n\n    yield\n\n    # Clean up the models and release the resources\n    service_endpoint_specs.clear()\n\n\n# -------------------------------------------\n# FastAPI application setup\n# -------------------------------------------\napp = FastAPI(lifespan=lifespan)\n\n\n# -------------------------------------------\n# Middlewares\n# -------------------------------------------\n@app.middleware(\"http\")\nasync def prepare_header_middleware(request: Request, call_next):\n    start_time = time.perf_counter()\n    response = await call_next(request)\n    process_time = time.perf_counter() - start_time\n    response.headers[\"X-Process-Time\"] = str(process_time)\n    response.headers[\"X-NODE-ID\"] = NODE_ID\n    response.headers[\"X-K8S-POD-NAME\"] = K8S_POD_NAME\n    return response\n\n\n# -------------------------------------------\n# General Endpoints\n# -------------------------------------------\n@app.get(\"/initialization_duration\")\ndef get_initialization_duration():\n    \"\"\"\n    Endpoint to retrieve the initialization duration of the AI model.\n    \"\"\"\n    global INITIALIZATION_DURATION\n\n    if INITIALIZATION_DURATION == 0.0:\n        return JSONResponse(\n            content={\"error\": \"Model not initialized.\"},\n            status_code=500,\n        )\n    return JSONResponse(\n        content={\n            \"initialization_duration\": INITIALIZATION_DURATION,\n            \"script_start_time\": SCRIPT_START_TIME,\n        }\n    )\n\n\n@app.get(\"/help\")\ndef get_help():\n    global service_endpoint_specs\n    help_info = {\n        \"endpoints\": {\n            \"/model/run\": {\n                \"method\": \"POST\",\n                \"description\": \"Executes the AI model with the provided input data.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_output_json_spec\"],\n                },\n            },\n            \"/model/profile_run\": {\n                \"method\": \"POST\",\n                \"description\": \"Profiles the AI model execution.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    \"profile_result\": \"Profiling results of the AI model execution.\",\n                    **service_endpoint_specs[\"profile_output_json_spec\"],\n                },\n            },\n            \"/initialization_duration\": {\n                \"method\": \"GET\",\n                \"description\": \"Retrieves the initialization duration of the AI model.\",\n                \"response\": {\n                    \"initialization_duration\": \"Time taken to initialize the model (in seconds).\",\n                    \"script_start_time\": \"Timestamp when the script started (in seconds since epoch).\",\n                },\n            },\n        }\n    }\n\n    if service_endpoint_specs[\"xai_model_input_form_spec\"] is not None:\n        help_info[\"endpoints\"][\"/xai_model/run\"] = {\n            \"method\": \"POST\",\n            \"description\": \"Executes the XAI model with the provided input data.\",\n            \"parameters\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_input_form_spec\"],\n            },\n            \"response\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_output_json_spec\"],\n            },\n        }\n\n        help_info[\"endpoints\"][\"/xai_model/profile_run\"] = {\n            \"method\": \"POST\",\n            \"description\": \"Profiles the XAI model execution.\",\n            \"parameters\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_input_form_spec\"],\n            },\n            \"response\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                \"profile_result\": \"Profiling results of the XAI model execution.\",\n                **service_endpoint_specs[\"xai_profile_output_json_spec\"],\n            },\n        }\n    \n    return JSONResponse(content=help_info)\n",
            "ai_client_script_content": "import base64\nimport json\nimport time\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nfrom ai_client_utils import (\n    prepare_ai_service_request_files,\n    prepare_ai_service_request_data,\n)\n\nXAI_GRADCAM_METHODS = [\n    \"GradCAM\",\n    \"HiResCAM\",\n    # \"AblationCAM\",\n    \"XGradCAM\",\n    \"GradCAMPlusPlus\",\n    \"ScoreCAM\",\n    \"LayerCAM\",\n    \"EigenCAM\",\n    \"EigenGradCAM\",\n    \"KPCA_CAM\",\n    \"RandomCAM\",\n]\n\n# -------------------------------------\n# prompt for necessary inputs\n# -------------------------------------\nSERVER_URL = input(\"Please input server URL (default to http://localhost:9000): \")\nif SERVER_URL.strip() == \"\":\n    SERVER_URL = \"http://localhost:9000\"\nUE_ID = input(\"Please input UE_ID (default to 123456): \")\nif UE_ID.strip() == \"\":\n    UE_ID = \"123456\"\n\n\ndef send_post_request(url, data, files):\n    \"\"\"Send request to run AI service and display AI service responses.\"\"\"\n    try:\n        response = requests.post(url, files=files, data=data)\n        # get the process time, node id and k8s pod name from the response headers\n        process_time = response.headers.get(\"X-Process-Time\")\n        node_id = response.headers.get(\"X-NODE-ID\")\n        k8s_pod_name = response.headers.get(\"X-K8S-POD-NAME\")\n        if response.status_code == 200:\n            return response.json(), process_time, node_id, k8s_pod_name\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n\ndef send_get_request(url, params=None):\n    \"\"\"Send GET request to the specified URL and return the response.\"\"\"\n    try:\n        response = requests.get(url, params=params)\n        process_time = response.headers.get(\"X-Process-Time\")\n        node_id = response.headers.get(\"X-NODE-ID\")\n        k8s_pod_name = response.headers.get(\"X-K8S-POD-NAME\")\n        if response.status_code == 200:\n            return response.json(), process_time, node_id, k8s_pod_name\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n            return None\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n\nclass ProfileResultProcessor:\n\n    def __init__(self, server_url):\n        self.server_url = server_url\n        self.start_time = time.time()\n        self.service_initialization_duration = 0\n        self.response_counter = 0\n        self.profile_name = None\n        self.device_type = None\n        self.device_name = None\n        self.node_id = None\n        self.k8s_pod_name = None\n        self.cpu_memory_usage_bytes = 0\n        self.self_cpu_memory_usage_bytes = 0\n        self.device_memory_usage_bytes = 0\n        self.self_device_memory_usage_bytes = 0\n        self.cpu_time_total_us = 0\n        self.self_cpu_time_total_us = 0\n        self.device_time_total_us = 0\n        self.self_device_time_total_us = 0\n\n        # xai related\n        self.gradcam_method_name = None\n\n        self.fetch_service_initialization_duration()\n\n    def fetch_service_initialization_duration(self):\n        \"\"\"Fetch the service initialization duration from the server.\"\"\"\n        response, process_time, node_id, k8s_pod_name = send_get_request(\n            f\"{self.server_url}/initialization_duration\"\n        )\n        if response:\n            self.service_initialization_duration = response.get(\n                \"initialization_duration\", 0\n            )\n        else:\n            print(\"Failed to fetch initialization duration.\")\n            self.service_initialization_duration = 0\n\n    def process_new_response(\n        self,\n        profile_response,\n        process_time=None,\n        node_id=None,\n        k8s_pod_name=None,\n        gradcam_method_name=None,\n    ):\n        if not profile_response:\n            return\n\n        profile_result = profile_response[\"profile_result\"]\n\n        if not profile_result:\n            return\n\n        if self.profile_name is None:\n            self.profile_name = profile_result[\"name\"]\n        if self.device_type is None:\n            self.device_type = profile_result[\"device_type\"]\n        if self.device_name is None:\n            self.device_name = profile_result[\"device_name\"]\n        if self.node_id is None:\n            self.node_id = node_id\n        if self.k8s_pod_name is None:\n            self.k8s_pod_name = k8s_pod_name\n\n        if self.gradcam_method_name is None:\n            self.gradcam_method_name = gradcam_method_name\n\n        # update the max profile result\n        self.cpu_memory_usage_bytes = max(\n            self.cpu_memory_usage_bytes, profile_result[\"cpu_memory_usage\"]\n        )\n        # self cpu memory usage could be negative. here we take the value that has the max absolute value\n        if abs(profile_result[\"self_cpu_memory_usage\"]) > abs( self.self_cpu_memory_usage_bytes):\n            self.self_cpu_memory_usage_bytes = profile_result[\"self_cpu_memory_usage\"]\n        self.device_memory_usage_bytes = max(\n            self.device_memory_usage_bytes, profile_result[\"device_memory_usage\"]\n        )\n        # same as self cpu memory usage\n        if abs(profile_result[\"self_device_memory_usage\"]) > abs(self.self_device_memory_usage_bytes):\n            self.self_device_memory_usage_bytes = profile_result[\"self_device_memory_usage\"]\n        self.cpu_time_total_us = max(\n            self.cpu_time_total_us, profile_result[\"cpu_time_total\"]\n        )\n        self.self_cpu_time_total_us = max(\n            self.self_cpu_time_total_us, profile_result[\"self_cpu_time_total\"]\n        )\n        self.device_time_total_us = max(\n            self.device_time_total_us, profile_result[\"device_time_total\"]\n        )\n        self.self_device_time_total_us = max(\n            self.self_device_time_total_us, profile_result[\"self_device_time_total\"]\n        )\n\n        self.response_counter += 1\n\n    def complete_profile(self):\n        print(\"\\n--------- PROFILE EVENT ---------\\n\")\n        print(f\"Name: {self.profile_name}\")\n        print(f\"Device Type: {self.device_type}\")\n        print(f\"Device Name: {self.device_name}\")\n        print(f\"Node ID: {self.node_id}\")\n        print(f\"K8S_POD_NAME: {self.k8s_pod_name}\")\n\n        if self.gradcam_method_name:\n            print(f\"GradCAM Method Name: {self.gradcam_method_name}\")\n\n        print(\"\\n--------- LATENCY RESULT ---------\\n\")\n        print(\"Service Initialization Duration: \", self.service_initialization_duration)\n        print(\"Total Requests: \", self.response_counter)\n        print(f\"Total Time Taken: {time.time() - self.start_time:.2f} seconds\")\n        print(\n            f\"Average Time Taken: {(time.time() - self.start_time) / self.response_counter:.2f} seconds\"\n        )\n\n        print(\"\\n--------- RESOURCE USAGE ---------\\n\")\n        print(f\"CPU Memory Usage: {self.cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\")\n        print(\n            f\"Self CPU Memory Usage: {self.self_cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Device Memory Usage: {self.device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Self Device Memory Usage: {self.self_device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(f\"CPU Time Total: {self.cpu_time_total_us / 1000:.2f} ms\")\n        print(f\"Self CPU Time Total: {self.self_cpu_time_total_us / 1000:.2f} ms\")\n        print(f\"Device Time Total: {self.device_time_total_us / 1000:.2f} ms\")\n        print(f\"Self Device Time Total: {self.self_device_time_total_us / 1000:.2f} ms\")\n\n        # update the service_data.json automatically\n        with open(\"service_data.json\", \"r\") as f:\n            service_data = json.load(f)\n\n        if not self.gradcam_method_name:\n            complete_profile_data_to_save = {\n                \"node_id\": self.node_id,\n                \"device_type\": self.device_type,\n                \"device_name\": self.device_name,\n                \"initialization_time_ms\": self.service_initialization_duration * 1000,\n                \"eviction_time_ms\": 0,\n                \"initialization_cost\": 0,\n                \"keep_alive_cost\": 0,\n                \"energy_consumption_idle\": 0,\n                \"inference\": {\n                    \"cpu_time_ms\": self.cpu_time_total_us / 1000,\n                    \"device_time_ms\": self.device_time_total_us / 1000,\n                    \"cpu_memory_usage_MB\": self.cpu_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"self_cpu_memory_usage_MB\": self.self_cpu_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"device_memory_usage_MB\": self.device_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"self_device_memory_usage_MB\": self.self_device_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"energy_consumption_execution\": 0,\n                    \"disk_IO_MB\": 0,\n                    \"input_data_MB\": 0,\n                    \"output_data_MB\": 0,\n                    \"execution_time_ms\": (time.time() - self.start_time)\n                    / self.response_counter\n                    * 1000,\n                    \"execution_cost\": 0,\n                },\n            }\n\n            # check if there is already a profile for this node id\n            profile_found = False\n            for profile in service_data[\"profiles\"]:\n                if profile[\"node_id\"] == self.node_id:\n                    profile[\"inference\"] = complete_profile_data_to_save[\"inference\"]\n                    profile_found = True\n                    break\n            if not profile_found:\n                service_data[\"profiles\"].append(complete_profile_data_to_save)\n\n            # save the updated service_data.json\n            with open(\"service_data.json\", \"w\") as f:\n                json.dump(service_data, f, indent=4)\n            print(\"\\n--------- SERVICE DATA UPDATED ---------\\n\")\n\n        else:\n            complete_xai_profile_data_to_save = {\n                \"node_id\": self.node_id,\n                \"device_type\": self.device_type,\n                \"device_name\": self.device_name,\n                \"initialization_time_ms\": self.service_initialization_duration * 1000,\n                \"eviction_time_ms\": 0,\n                \"initialization_cost\": 0,\n                \"keep_alive_cost\": 0,\n                \"energy_consumption_idle\": 0,\n                \"xai\": [\n                    {\n                        \"xai_method\": self.gradcam_method_name,\n                        \"cpu_time_ms\": self.cpu_time_total_us / 1000,\n                        \"device_time_ms\": self.device_time_total_us / 1000,\n                        \"cpu_memory_usage_MB\": self.cpu_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"self_cpu_memory_usage_MB\": self.self_cpu_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"device_memory_usage_MB\": self.device_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"self_device_memory_usage_MB\": self.self_device_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"energy_consumption_execution\": 0,\n                        \"disk_IO_MB\": 0,\n                        \"input_data_MB\": 0,\n                        \"output_data_MB\": 0,\n                        \"execution_time_ms\": (time.time() - self.start_time)\n                        / self.response_counter\n                        * 1000,\n                        \"execution_cost\": 0,\n                    }\n                ],\n            }\n\n            # check if there is already a profile for this node id\n            profile_found = False\n            for profile in service_data[\"profiles\"]:\n                if profile[\"node_id\"] == self.node_id:\n                    profile_found = True\n\n                    # check if there is already a profile for this xai method\n                    xai_method_found = False\n                    if not profile.get(\"xai\"):\n                        profile[\"xai\"] = []\n                    for xai_profile in profile[\"xai\"]:\n                        if xai_profile[\"xai_method\"] == self.gradcam_method_name:\n                            xai_profile.update(complete_xai_profile_data_to_save[\"xai\"][0]) \n                            xai_method_found = True\n                            break\n                    if not xai_method_found:\n                        profile[\"xai\"].append(complete_xai_profile_data_to_save[\"xai\"][0])\n                    break\n\n            if not profile_found:\n                service_data[\"profiles\"].append(complete_xai_profile_data_to_save)\n\n            # save the updated service_data.json\n            with open(\"service_data.json\", \"w\") as f:\n                json.dump(service_data, f, indent=4)\n            print(\"\\n--------- SERVICE DATA UPDATED ---------\\n\")\n\n\ndef option_run():\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n    data = {**data, \"ue_id\": UE_ID}\n    response, process_time, node_id, pod_name = send_post_request(\n        f\"{SERVER_URL}/model/run\", data, files\n    )\n    print(\"Process Time: \", process_time)\n    print(\"Node ID: \", node_id)\n    print(\"K8S_POD_NAME: \", pod_name)\n    print(\"Response\")\n    print(json.dumps(response, indent=4))\n\n\ndef option_profile_run():\n    data = prepare_ai_service_request_data()\n    data = {**data, \"ue_id\": UE_ID}\n    files = prepare_ai_service_request_files()\n    num_requests = int(input(\"Enter the number of requests to send: \"))\n\n    try:\n        profile_result_processor = ProfileResultProcessor(SERVER_URL)\n\n        for _ in range(num_requests):\n            profile_response, process_time, node_id, k8s_node_name = send_post_request(\n                f\"{SERVER_URL}/model/profile_run\", data, files\n            )\n            print(\"Process Time: \", process_time)\n            print(\"Node ID: \", node_id)\n            print(\"K8S_POD_NAME: \", k8s_node_name)\n            print(\"Response\")\n            print(json.dumps(profile_response, indent=4))\n            if not profile_response:\n                print(\"No profile response received.\")\n                continue\n\n            profile_result_processor.process_new_response(\n                profile_response,\n                process_time=process_time,\n                node_id=node_id,\n                k8s_pod_name=k8s_node_name,\n            )\n\n        # Print the final profile result and update the service_data.json\n        profile_result_processor.complete_profile()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\n\ndef option_help():\n    \"\"\"Get help information from the server.\"\"\"\n    try:\n        response, process_time, node_id, pod_name = send_get_request(\n            f\"{SERVER_URL}/help\"\n        )\n        print(\"Process Time: \", process_time)\n        print(\"Node ID: \", node_id)\n        print(\"K8S_POD_NAME: \", pod_name)\n        print(\"Response\")\n        print(json.dumps(response, indent=4))\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n\n\ndef option_run_with_xai():\n    \"\"\"Run AI service with XAI.\"\"\"\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n\n    print(\n        \"Note that currently only GradCAM methods on image-classification models are supported.\"\n    )\n    while True:\n        gradcam_method_name = input(\n            f\"Please select a GradCAM method (options: {XAI_GRADCAM_METHODS}): \"\n        )\n        if gradcam_method_name not in XAI_GRADCAM_METHODS:\n            print(f\"Invalid GradCAM method. Please select again.\")\n        else:\n            break\n\n    # ask for target class for explanation\n    target_category_indexes = input(\n        \"Please input target category indexes for explanation (comma-separated, e.g., 111, 32, 44, ...): \"\n    )\n    if not target_category_indexes or not target_category_indexes.strip():\n        print(\n            \"No target category indexes provided. Defaulting to explaining the top confident category.\"\n        )\n        target_category_indexes = []\n    else:\n        target_category_indexes = [\n            int(i.strip()) for i in target_category_indexes.split(\",\")\n        ]\n\n    data = {\n        **data,\n        \"ue_id\": UE_ID,\n        \"gradcam_method_name\": gradcam_method_name,\n        \"target_category_indexes\": target_category_indexes,\n    }\n    print(\"Data: \", data)\n    response, process_time, node_id, k8s_pod_name = send_post_request(\n        f\"{SERVER_URL}/xai_model/run\", data, files\n    )\n    print(\"Process Time: \", process_time)\n    print(\"Node ID: \", node_id)\n    print(\"K8S_POD_NAME: \", k8s_pod_name)\n\n    # Handle JSON response\n    model_results = response.get(\"model_results\")\n    if model_results:\n        print(\"Model Results:\", json.dumps(model_results, indent=4))\n\n    xai_results = response.get(\"xai_results\")\n    if xai_results:\n        print(\"XAI Results Method:\", xai_results.get(\"xai_method\"))\n\n    # Handle binary image response\n    encoded_image = xai_results.get(\"image\")\n    if encoded_image:\n        image_bytes = base64.b64decode(encoded_image)\n\n        # Load the image into Pillow\n        image = Image.open(BytesIO(image_bytes))\n\n        # Save image to disk\n        image.save(\"xai_output.png\")\n\n        # Display the image using matplotlib\n        plt.imshow(image)\n        plt.axis(\"off\")\n        plt.show()\n\n\ndef option_profile_run_with_xai():\n    \"\"\"Run AI service with XAI and profile the run.\"\"\"\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n    print(\n        \"Note that currently only GradCAM methods on image-classification models are supported.\"\n    )\n\n    # ask for target class for explanation\n    target_category_indexes = input(\n        \"Please input target category indexes for explanation (comma-separated, e.g., 111, 32, 44, ...): \"\n    )\n    if not target_category_indexes or not target_category_indexes.strip():\n        print(\n            \"No target category indexes provided. Defaulting to explaining the top confident category.\"\n        )\n        target_category_indexes = []\n    else:\n        target_category_indexes = [\n            int(i.strip()) for i in target_category_indexes.split(\",\")\n        ]\n\n    num_requests = int(input(\"Enter the number of requests to send: \"))\n\n    try:\n        for gradcam_method_name in XAI_GRADCAM_METHODS:\n\n            data = {\n                **data,\n                \"ue_id\": UE_ID,\n                \"gradcam_method_name\": gradcam_method_name,\n                \"target_category_indexes\": target_category_indexes,\n            }\n            print(\"Data: \", data)\n\n            profile_result_processor = ProfileResultProcessor(SERVER_URL)\n\n            for _ in range(num_requests):\n                response, process_time, node_id, k8s_pod_name = send_post_request(\n                    f\"{SERVER_URL}/xai_model/profile_run\", data, files\n                )\n                print(\"Process Time: \", process_time)\n                print(\"Node ID: \", node_id)\n                print(\"K8S_POD_NAME: \", k8s_pod_name)\n                if not response:\n                    print(\"No profile response received.\")\n                    continue\n\n                # Handle JSON response\n                model_results = response.get(\"model_results\")\n                if model_results:\n                    print(\"Model Results:\", json.dumps(model_results, indent=4))\n\n                profile_result_processor.process_new_response(\n                    response,\n                    process_time=process_time,\n                    node_id=node_id,\n                    k8s_pod_name=k8s_pod_name,\n                    gradcam_method_name=gradcam_method_name,\n                )\n\n            # Print the final profile result and update the service_data.json\n            profile_result_processor.complete_profile()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\n\nOPTIONS = [\n    {\n        \"label\": \"Get help information\",\n        \"action\": option_help,\n    },\n    {\n        \"label\": \"Run AI service\",\n        \"action\": option_run,\n    },\n    {\n        \"label\": \"Profile AI service\",\n        \"action\": option_profile_run,\n    },\n    {\n        \"label\": \"Run AI service with XAI (only image-classification models)\",\n        \"action\": option_run_with_xai,\n    },\n    {\n        \"label\": \"Profile AI service with XAI (only image-classification models)\",\n        \"action\": option_profile_run_with_xai,\n    },\n]\n\n\nif __name__ == \"__main__\":\n    while True:\n        print(\"\\nOptions:\")\n        for i, option in enumerate(OPTIONS, start=1):\n            print(f\"{i}. {option['label']}\")\n        print(\"q. Quit\")\n        choice = input(\"Enter your choice: \")\n\n        if choice == \"q\":\n            print(\"Exiting the client. Goodbye!\")\n            break\n        else:\n            try:\n                choice = int(choice)\n                if 1 <= choice <= len(OPTIONS):\n                    OPTIONS[choice - 1][\"action\"]()\n                else:\n                    print(\"Invalid choice. Please try again.\")\n            except ValueError:\n                print(\"Invalid input. Please enter a number.\")\n",
            "ai_client_utils_script_content": "def prepare_ai_service_request_files():\n    \"\"\"Prepare the `files` part for the AI service request.\"\"\"\n    files = {}\n    image_file_path = input(\"Please input the image file path: \")\n    with open(image_file_path, \"rb\") as image_file:\n        files[\"file\"] = image_file.read()\n    return files\n\ndef prepare_ai_service_request_data():\n    \"\"\"Prepare the `data` part other than `ue_id` for the AI service request.\"\"\"\n    data = {\"ue_id\": input(\"Please input the unique execution ID (ue_id): \")}\n    return data",
            "model_script_content": "# import server utils\nfrom ai_server_utils import (\n    process_model_output_logits,\n    profile_activities,\n    prepare_profile_results,\n)\n\n# import profile utils\nfrom torch.profiler import profile, record_function\n\n# import necessary libs for AI model inference and request handling\nimport torch\nfrom fastapi import APIRouter, File, Form, UploadFile\nfrom fastapi.responses import JSONResponse\nfrom transformers import ViTImageProcessor, ViTForImageClassification\nfrom PIL import Image\n\n# --------------------------------\n# Device configuration\n# --------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# --------------------------------\n# Model-specific configuration\n# make sure the variables `MODEL_NAME` and `model` are defined here.\n# --------------------------------\nMODEL_NAME = \"google/vit-large-patch32-384\"\nprocessor = ViTImageProcessor.from_pretrained(MODEL_NAME)\nmodel = ViTForImageClassification.from_pretrained(MODEL_NAME).to(device)\nmodel.eval()\n\n\n# Initialize the FastAPI router\nrouter = APIRouter()\n\n\n@router.post(\"/run\")\nasync def run_model(file: UploadFile = File(...), ue_id: str = Form(...)):\n    try:\n        # Prepare the model input\n        image = Image.open(file.file).convert(\"RGB\")\n        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n\n        # Perform inference\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        # Process the model outputs\n        predictions = process_model_output_logits(model, outputs.logits)\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"model_results\": predictions,\n            }\n        )\n    except Exception as e:\n        print(f\"Error processing file: {e}\")\n        return JSONResponse(\n            content={\"error\": \"Failed to process the image. {e}\".format(e=str(e))},\n            status_code=500,\n        )\n\n\n@router.post(\"/profile_run\")\nasync def profile_run(file: UploadFile = File(...), ue_id: str = Form(...)):\n    \"\"\"\n    Endpoint to profile the AI model execution.\n    \"\"\"\n    try:\n        # Prepare the model input\n        image = Image.open(file.file).convert(\"RGB\")\n        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n\n        # perform profiling\n        with profile(\n            activities=profile_activities,\n            profile_memory=True,\n        ) as prof:\n            with record_function(\"model_run\"):\n                with torch.no_grad():\n                    model_outputs = model(**inputs)\n\n        profile_result = prepare_profile_results(prof)\n\n        # Process the model outputs\n        predictions = process_model_output_logits(model, model_outputs.logits)\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"profile_result\": profile_result,\n                \"model_results\": predictions,\n            }\n        )\n\n    except Exception as e:\n        print(f\"Error processing request: {e}\")\n        return JSONResponse(\n            content={\"error\": f\"Failed to process the request. {e}\"},\n            status_code=500,\n        )\n\n\n# Below are the model input and output specifications to be used by the `/help` endpoint\nMODEL_INPUT_FORM_SPEC = {\n    \"file\": {\n        \"type\": \"file upload\",\n        \"description\": \"The image file to be classified.\",\n        \"required\": True,\n        \"example\": \"puppy.png\",\n    }\n}\n\nMODEL_OUTPUT_JSON_SPEC = {\n    \"ue_id\": \"unique execution ID\",\n    \"model_results\": [\n        {\n            \"category_id\": \"category id\",\n            \"label\": \"category label\",\n            \"probability\": \"probability value\",\n        }\n    ],\n}\n",
            "healthcheck_script_content": "import requests\n\nprint(requests.get(\"http://localhost:8000/help\"))"
        }
    },
    {
        "model_name": "facebook/convnext-tiny-224",
        "model_url": "https://huggingface.co/facebook/convnext-tiny-224",
        "task": "image-classification",
        "task_detail": "The \"facebook/convnext-tiny-224\" is a pre-trained AI model developed primarily for the purpose of image classification. This model is a member of the ConvNeXT family, an innovative convolutional neural network (ConvNet) that draws inspiration from Vision Transformers. Originating from the architectural concepts of the ResNet and modernized with elements from the Swin Transformer, ConvNeXT aims to deliver superior performance by enhancing the traditional convolutional architecture.\n\n**Functionality Overview:**\n\n1. **Image Classification Task**: \n   - The primary function of the ConvNeXT tiny model is to perform image classification. It categorizes input images into one of the 1,000 distinct classes found in the ImageNet dataset, a widely used dataset that covers a vast array of common objects.\n\n2. **Input Requirements**: \n   - The model accepts images with a resolution of 224x224 pixels, a standard size ensuring compatibility and optimal performance. \n   - Images must be preprocessed to adhere to the model's input specifications, involving transformations like resizing, normalization, and possibly augmentation for bespoke dataset variations.\n\n3. **Output Specifications**: \n   - The model outputs a set of logits corresponding to the probability distribution over the 1,000 ImageNet classes. \n   - The class with the highest logit value is selected as the predicted label, representing the model's classification for the input image.\n\n4. **Model Architecture and Innovations**: \n   - ConvNeXT is notable for being a ConvNet specifically adapted to incorporate transformative features inspired by vision transformers. These design enhancements enable ConvNeXT to potentially surpass traditional transformer models in image classification tasks.\n\n5. **Use Cases and Applications**: \n   - This model is well-suited for applications requiring the categorization of standard object images from large datasets in industries like retail (for product categorization), automated surveillance systems (for identifying various objects), and digital asset management (for metadata generation).\n\n6. **Limitations and Considerations**: \n   - While powerful, the model is limited to classifying images into predefined categories within the ImageNet dataset. For tasks requiring recognition of niche or uncommon classes outside ImageNet, fine-tuning or alternative models might be necessary.\n   - The model's performance is contingent on the quality and characteristics of the input data, highlighting the importance of preprocessing in accordance with specified guidelines for accuracy.\n\nBy utilizing the described functionality and leveraging the underlying architecture, users can effectively implement image classification solutions tailored to various domain-specific applications, enhancing automated visual intelligence systems with state-of-the-art convolutional network processing.",
        "accuracy_info": "No accuracy information found.",
        "image_repository_url": "",
        "service_disk_size_bytes": 3742761930.0,
        "profiles": [
            {
                "node_id": "ugurcan.celik",
                "device_type": "DeviceType.CPU",
                "device_name": "cuda",
                "initialization_time_ms": 4855.448484420776,
                "eviction_time_ms": 0.0,
                "initialization_cost": 0.0,
                "keep_alive_cost": 0.0,
                "energy_consumption_idle": 0.0,
                "idle_container_cpu_memory_usage": "1.7GB",
                "idle_container_device_memory_usage": "1.5GB",
                "inference": {
                    "cpu_time_ms": 195.967384,
                    "device_time_ms": 3.83635600000036,
                    "cpu_memory_usage_MB": 0.0,
                    "self_cpu_memory_usage_MB": 0.0,
                    "device_memory_usage_MB": 8.12890625,
                    "self_device_memory_usage_MB": -164.2216796875,
                    "energy_consumption_execution": 0.0,
                    "disk_IO_MB": 0.0,
                    "input_data_MB": 0.0,
                    "output_data_MB": 0.0,
                    "execution_time_ms": 269.6194648742676,
                    "execution_cost": 0.0
                },
                "xai": [
                    {
                        "xai_method": "GradCAM",
                        "cpu_time_ms": 110.95485400000001,
                        "device_time_ms": 2.179908000000004,
                        "cpu_memory_usage_MB": 0.287109375,
                        "self_cpu_memory_usage_MB": 0.139739990234375,
                        "device_memory_usage_MB": 233.728515625,
                        "self_device_memory_usage_MB": 97.9990234375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 524.3589878082275,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "HiResCAM",
                        "cpu_time_ms": 28.461125,
                        "device_time_ms": 2.167148000000002,
                        "cpu_memory_usage_MB": 0.287109375,
                        "self_cpu_memory_usage_MB": 0.139739990234375,
                        "device_memory_usage_MB": 119.16162109375,
                        "self_device_memory_usage_MB": -20.47412109375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 497.93092409769696,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "XGradCAM",
                        "cpu_time_ms": 26.10082,
                        "device_time_ms": 2.158987000000004,
                        "cpu_memory_usage_MB": 0.287109375,
                        "self_cpu_memory_usage_MB": 0.139739990234375,
                        "device_memory_usage_MB": 119.16162109375,
                        "self_device_memory_usage_MB": -20.47412109375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 481.01480801900226,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "GradCAMPlusPlus",
                        "cpu_time_ms": 25.894832,
                        "device_time_ms": 2.151534999999994,
                        "cpu_memory_usage_MB": 0.287109375,
                        "self_cpu_memory_usage_MB": 0.139739990234375,
                        "device_memory_usage_MB": 119.16162109375,
                        "self_device_memory_usage_MB": -20.47412109375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 530.4195880889893,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "ScoreCAM",
                        "cpu_time_ms": 1274.5239719999997,
                        "device_time_ms": 1187.6075319999995,
                        "cpu_memory_usage_MB": 110.3935546875,
                        "self_cpu_memory_usage_MB": -0.012603759765625,
                        "device_memory_usage_MB": 119.16162109375,
                        "self_device_memory_usage_MB": -98973.61865234375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 8535.228808720907,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "LayerCAM",
                        "cpu_time_ms": 26.671025,
                        "device_time_ms": 2.1924300000000003,
                        "cpu_memory_usage_MB": 0.287109375,
                        "self_cpu_memory_usage_MB": 0.139739990234375,
                        "device_memory_usage_MB": 119.16162109375,
                        "self_device_memory_usage_MB": -20.47412109375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 629.3320655822754,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenCAM",
                        "cpu_time_ms": 42.259555,
                        "device_time_ms": 2.185064,
                        "cpu_memory_usage_MB": 0.1435546875,
                        "self_cpu_memory_usage_MB": -0.003814697265625,
                        "device_memory_usage_MB": 119.16162109375,
                        "self_device_memory_usage_MB": -20.47314453125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 189.32437896728516,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenGradCAM",
                        "cpu_time_ms": 53.776909999999994,
                        "device_time_ms": 2.1868229999999986,
                        "cpu_memory_usage_MB": 0.287109375,
                        "self_cpu_memory_usage_MB": 0.139739990234375,
                        "device_memory_usage_MB": 119.16162109375,
                        "self_device_memory_usage_MB": -20.47412109375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 592.478354771932,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "KPCA_CAM",
                        "cpu_time_ms": 16.246592,
                        "device_time_ms": 2.186576999999995,
                        "cpu_memory_usage_MB": 0.1435546875,
                        "self_cpu_memory_usage_MB": -0.003814697265625,
                        "device_memory_usage_MB": 119.16162109375,
                        "self_device_memory_usage_MB": -20.47314453125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 146.12611134847006,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "RandomCAM",
                        "cpu_time_ms": 26.241576000000002,
                        "device_time_ms": 2.184076999999989,
                        "cpu_memory_usage_MB": 0.287109375,
                        "self_cpu_memory_usage_MB": 0.139739990234375,
                        "device_memory_usage_MB": 119.16162109375,
                        "self_device_memory_usage_MB": -20.47412109375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 512.9798253377279,
                        "execution_cost": 0.0
                    }
                ]
            },
            {
                "node_id": "LAP004262",
                "device_type": "DeviceType.CPU",
                "device_name": "None",
                "initialization_time_ms": 17231.818914413452,
                "eviction_time_ms": 0.0,
                "initialization_cost": 0.0,
                "keep_alive_cost": 0.0,
                "energy_consumption_idle": 0.0,
                "idle_container_cpu_memory_usage": "1.2GB",
                "idle_container_device_memory_usage": "0GB",
                "inference": {
                    "cpu_time_ms": 85.44635799999999,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 0.003814697265625,
                    "self_cpu_memory_usage_MB": -127.59326553344727,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0.0,
                    "disk_IO_MB": 0.0,
                    "input_data_MB": 0.0,
                    "output_data_MB": 0.0,
                    "execution_time_ms": 218.8145319620768,
                    "execution_cost": 0.0
                },
                "xai": [
                    {
                        "xai_method": "GradCAM",
                        "cpu_time_ms": 232.838176,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 219.7376251220703,
                        "self_cpu_memory_usage_MB": -126.7990951538086,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 541.5751139322916,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "HiResCAM",
                        "cpu_time_ms": 276.581901,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 110.67875671386719,
                        "self_cpu_memory_usage_MB": -126.7990951538086,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 602.9783884684244,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "XGradCAM",
                        "cpu_time_ms": 217.516437,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 110.67875671386719,
                        "self_cpu_memory_usage_MB": -126.7990951538086,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 547.3000208536783,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "GradCAMPlusPlus",
                        "cpu_time_ms": 218.30684,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 110.67875671386719,
                        "self_cpu_memory_usage_MB": -126.8110580444336,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 555.9306144714355,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "LayerCAM",
                        "cpu_time_ms": 207.729663,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 110.67875671386719,
                        "self_cpu_memory_usage_MB": -126.79984283447266,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 545.7899570465088,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenCAM",
                        "cpu_time_ms": 120.83205500000001,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 110.53520202636719,
                        "self_cpu_memory_usage_MB": -17.740219116210938,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 234.97390747070312,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenGradCAM",
                        "cpu_time_ms": 470.71107,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 110.67875671386719,
                        "self_cpu_memory_usage_MB": -126.7990951538086,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 710.67214012146,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "KPCA_CAM",
                        "cpu_time_ms": 71.203519,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 110.53520202636719,
                        "self_cpu_memory_usage_MB": -17.740219116210938,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 277.9032389322917,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "RandomCAM",
                        "cpu_time_ms": 328.268732,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 110.67875671386719,
                        "self_cpu_memory_usage_MB": -126.80208587646484,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 603.1723817189535,
                        "execution_cost": 0.0
                    }
                ]
            }
        ],
        "feedback": {
            "likes": [],
            "dislikes": [],
            "comments": []
        },
        "code": {
            "readme_content": "---\nlicense: apache-2.0\ntags:\n- vision\n- image-classification\ndatasets:\n- imagenet-1k\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\n  example_title: Tiger\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg\n  example_title: Teapot\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg\n  example_title: Palace\n---\n\n# ConvNeXT (tiny-sized model) \n\nConvNeXT model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545) by Liu et al. and first released in [this repository](https://github.com/facebookresearch/ConvNeXt). \n\nDisclaimer: The team releasing ConvNeXT did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and \"modernized\" its design by taking the Swin Transformer as inspiration.\n\n![model image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convnext_architecture.png)\n\n## Intended uses & limitations\n\nYou can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=convnext) to look for\nfine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\n\n```python\nfrom transformers import ConvNextImageProcessor, ConvNextForImageClassification\nimport torch\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingface/cats-image\")\nimage = dataset[\"test\"][\"image\"][0]\n\nprocessor = ConvNextImageProcessor.from_pretrained(\"facebook/convnext-tiny-224\")\nmodel = ConvNextForImageClassification.from_pretrained(\"facebook/convnext-tiny-224\")\n\ninputs = processor(image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label]),\n```\n\nFor more code examples, we refer to the [documentation](https://huggingface.co/docs/transformers/master/en/model_doc/convnext).\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-2201-03545,\n  author    = {Zhuang Liu and\n               Hanzi Mao and\n               Chao{-}Yuan Wu and\n               Christoph Feichtenhofer and\n               Trevor Darrell and\n               Saining Xie},\n  title     = {A ConvNet for the 2020s},\n  journal   = {CoRR},\n  volume    = {abs/2201.03545},\n  year      = {2022},\n  url       = {https://arxiv.org/abs/2201.03545},\n  eprinttype = {arXiv},\n  eprint    = {2201.03545},\n  timestamp = {Thu, 20 Jan 2022 14:21:35 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2201-03545.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```",
            "dockerfile_content": "# Base image for AI service powered by HuggingFace pre-trained AI models.\n# the image has the following packages/libraries installed already:\n# - python3.12, pip, git, fastapi, uvicorn, torch, torchvision, opencv-python, transformers, python-multipart, Pillow, requests\nFROM python3.12_ai_service_base:latest\n\n# Set working directory\nWORKDIR /app\n\n# Copy application code\nCOPY . .\n\n# Install additional dependencies\nRUN pip install datasets\n\n# Expose port 8000\nEXPOSE 8000\n\n# Start the FastAPI server\nCMD [\"uvicorn\", \"ai_server:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--timeout-keep-alive\", \"600\"]",
            "ai_server_script_content": "import json\nimport os\nimport time\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import JSONResponse\nfrom contextlib import asynccontextmanager\nfrom ai_server_utils import (\n    PROFILE_OUTPUT_JSON_SPEC,\n    NODE_ID,\n    K8S_POD_NAME,\n)\n\n\n# -------------------------------------------\n# App Lifespan setup\n# -------------------------------------------\n# Record the script start time (when uvicorn starts the process)\nSCRIPT_START_TIME = time.time()\nINITIALIZATION_DURATION = 0.0\nservice_endpoint_specs = {\n    \"model_input_form_spec\": None,\n    \"model_output_json_spec\": None,\n    \"profile_output_json_spec\": None,\n    \"xai_model_input_form_spec\": None,\n    \"xai_model_output_json_spec\": None,\n    \"xai_profile_output_json_spec\": None,\n}\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n\n    global INITIALIZATION_DURATION\n    global SCRIPT_START_TIME\n    global service_endpoint_specs\n\n    # Load the AI model\n    print(\"Loading AI model...\")\n    from model import (\n        MODEL_INPUT_FORM_SPEC,\n        MODEL_OUTPUT_JSON_SPEC,\n        router as model_router,\n    )\n\n    service_endpoint_specs[\"model_input_form_spec\"] = MODEL_INPUT_FORM_SPEC\n    service_endpoint_specs[\"model_output_json_spec\"] = MODEL_OUTPUT_JSON_SPEC\n    service_endpoint_specs[\"profile_output_json_spec\"] = PROFILE_OUTPUT_JSON_SPEC\n\n    app.include_router(model_router, prefix=\"/model\", tags=[\"AI Model\"])\n\n    # Load the XAI model\n    if os.path.exists(os.path.dirname(__file__) + \"/xai_model.py\"):\n        print(\"Loading XAI model...\")\n        from xai_model import (\n            XAI_OUTPUT_JSON_SPEC,\n            router as xai_model_router,\n        )\n\n        # by default, the xai_model input form spec is the same as the model input form spec\n        service_endpoint_specs[\"xai_model_input_form_spec\"] = MODEL_INPUT_FORM_SPEC\n        service_endpoint_specs[\"xai_model_output_json_spec\"] = MODEL_OUTPUT_JSON_SPEC\n        service_endpoint_specs[\"xai_model_output_json_spec\"].update(\n            XAI_OUTPUT_JSON_SPEC\n        )\n        service_endpoint_specs[\"xai_profile_output_json_spec\"] = (\n            PROFILE_OUTPUT_JSON_SPEC\n        )\n        service_endpoint_specs[\"xai_profile_output_json_spec\"].update(\n            XAI_OUTPUT_JSON_SPEC\n        )\n\n        app.include_router(xai_model_router, prefix=\"/xai_model\", tags=[\"XAI Model\"])\n\n    # Record the initialization duration\n    INITIALIZATION_DURATION = time.time() - SCRIPT_START_TIME\n\n    print(f\"AI service loaded in {INITIALIZATION_DURATION:.2f} seconds.\")\n\n    yield\n\n    # Clean up the models and release the resources\n    service_endpoint_specs.clear()\n\n\n# -------------------------------------------\n# FastAPI application setup\n# -------------------------------------------\napp = FastAPI(lifespan=lifespan)\n\n\n# -------------------------------------------\n# Middlewares\n# -------------------------------------------\n@app.middleware(\"http\")\nasync def prepare_header_middleware(request: Request, call_next):\n    start_time = time.perf_counter()\n    response = await call_next(request)\n    process_time = time.perf_counter() - start_time\n    response.headers[\"X-Process-Time\"] = str(process_time)\n    response.headers[\"X-NODE-ID\"] = NODE_ID\n    response.headers[\"X-K8S-POD-NAME\"] = K8S_POD_NAME\n    return response\n\n\n# -------------------------------------------\n# General Endpoints\n# -------------------------------------------\n@app.get(\"/initialization_duration\")\ndef get_initialization_duration():\n    \"\"\"\n    Endpoint to retrieve the initialization duration of the AI model.\n    \"\"\"\n    global INITIALIZATION_DURATION\n\n    if INITIALIZATION_DURATION == 0.0:\n        return JSONResponse(\n            content={\"error\": \"Model not initialized.\"},\n            status_code=500,\n        )\n    return JSONResponse(\n        content={\n            \"initialization_duration\": INITIALIZATION_DURATION,\n            \"script_start_time\": SCRIPT_START_TIME,\n        }\n    )\n\n\n@app.get(\"/help\")\ndef get_help():\n    global service_endpoint_specs\n    help_info = {\n        \"endpoints\": {\n            \"/model/run\": {\n                \"method\": \"POST\",\n                \"description\": \"Executes the AI model with the provided input data.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_output_json_spec\"],\n                },\n            },\n            \"/model/profile_run\": {\n                \"method\": \"POST\",\n                \"description\": \"Profiles the AI model execution.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    \"profile_result\": \"Profiling results of the AI model execution.\",\n                    **service_endpoint_specs[\"profile_output_json_spec\"],\n                },\n            },\n            \"/initialization_duration\": {\n                \"method\": \"GET\",\n                \"description\": \"Retrieves the initialization duration of the AI model.\",\n                \"response\": {\n                    \"initialization_duration\": \"Time taken to initialize the model (in seconds).\",\n                    \"script_start_time\": \"Timestamp when the script started (in seconds since epoch).\",\n                },\n            },\n        }\n    }\n\n    if service_endpoint_specs[\"xai_model_input_form_spec\"] is not None:\n        help_info[\"endpoints\"][\"/xai_model/run\"] = {\n            \"method\": \"POST\",\n            \"description\": \"Executes the XAI model with the provided input data.\",\n            \"parameters\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_input_form_spec\"],\n            },\n            \"response\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_output_json_spec\"],\n            },\n        }\n\n        help_info[\"endpoints\"][\"/xai_model/profile_run\"] = {\n            \"method\": \"POST\",\n            \"description\": \"Profiles the XAI model execution.\",\n            \"parameters\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_input_form_spec\"],\n            },\n            \"response\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                \"profile_result\": \"Profiling results of the XAI model execution.\",\n                **service_endpoint_specs[\"xai_profile_output_json_spec\"],\n            },\n        }\n    \n    return JSONResponse(content=help_info)\n",
            "ai_client_script_content": "import base64\nimport json\nimport time\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nfrom ai_client_utils import (\n    prepare_ai_service_request_files,\n    prepare_ai_service_request_data,\n)\n\nXAI_GRADCAM_METHODS = [\n    \"GradCAM\",\n    \"HiResCAM\",\n    # \"AblationCAM\",\n    \"XGradCAM\",\n    \"GradCAMPlusPlus\",\n    \"ScoreCAM\",\n    \"LayerCAM\",\n    \"EigenCAM\",\n    \"EigenGradCAM\",\n    \"KPCA_CAM\",\n    \"RandomCAM\",\n]\n\n# -------------------------------------\n# prompt for necessary inputs\n# -------------------------------------\nSERVER_URL = input(\"Please input server URL (default to http://localhost:9000): \")\nif SERVER_URL.strip() == \"\":\n    SERVER_URL = \"http://localhost:9000\"\nUE_ID = input(\"Please input UE_ID (default to 123456): \")\nif UE_ID.strip() == \"\":\n    UE_ID = \"123456\"\n\n\ndef send_post_request(url, data, files):\n    \"\"\"Send request to run AI service and display AI service responses.\"\"\"\n    try:\n        response = requests.post(url, files=files, data=data)\n        # get the process time, node id and k8s pod name from the response headers\n        process_time = response.headers.get(\"X-Process-Time\")\n        node_id = response.headers.get(\"X-NODE-ID\")\n        k8s_pod_name = response.headers.get(\"X-K8S-POD-NAME\")\n        if response.status_code == 200:\n            return response.json(), process_time, node_id, k8s_pod_name\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n\ndef send_get_request(url, params=None):\n    \"\"\"Send GET request to the specified URL and return the response.\"\"\"\n    try:\n        response = requests.get(url, params=params)\n        process_time = response.headers.get(\"X-Process-Time\")\n        node_id = response.headers.get(\"X-NODE-ID\")\n        k8s_pod_name = response.headers.get(\"X-K8S-POD-NAME\")\n        if response.status_code == 200:\n            return response.json(), process_time, node_id, k8s_pod_name\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n            return None\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n\nclass ProfileResultProcessor:\n\n    def __init__(self, server_url):\n        self.server_url = server_url\n        self.start_time = time.time()\n        self.service_initialization_duration = 0\n        self.response_counter = 0\n        self.profile_name = None\n        self.device_type = None\n        self.device_name = None\n        self.node_id = None\n        self.k8s_pod_name = None\n        self.cpu_memory_usage_bytes = 0\n        self.self_cpu_memory_usage_bytes = 0\n        self.device_memory_usage_bytes = 0\n        self.self_device_memory_usage_bytes = 0\n        self.cpu_time_total_us = 0\n        self.self_cpu_time_total_us = 0\n        self.device_time_total_us = 0\n        self.self_device_time_total_us = 0\n\n        # xai related\n        self.gradcam_method_name = None\n\n        self.fetch_service_initialization_duration()\n\n    def fetch_service_initialization_duration(self):\n        \"\"\"Fetch the service initialization duration from the server.\"\"\"\n        response, process_time, node_id, k8s_pod_name = send_get_request(\n            f\"{self.server_url}/initialization_duration\"\n        )\n        if response:\n            self.service_initialization_duration = response.get(\n                \"initialization_duration\", 0\n            )\n        else:\n            print(\"Failed to fetch initialization duration.\")\n            self.service_initialization_duration = 0\n\n    def process_new_response(\n        self,\n        profile_response,\n        process_time=None,\n        node_id=None,\n        k8s_pod_name=None,\n        gradcam_method_name=None,\n    ):\n        if not profile_response:\n            return\n\n        profile_result = profile_response[\"profile_result\"]\n\n        if not profile_result:\n            return\n\n        if self.profile_name is None:\n            self.profile_name = profile_result[\"name\"]\n        if self.device_type is None:\n            self.device_type = profile_result[\"device_type\"]\n        if self.device_name is None:\n            self.device_name = profile_result[\"device_name\"]\n        if self.node_id is None:\n            self.node_id = node_id\n        if self.k8s_pod_name is None:\n            self.k8s_pod_name = k8s_pod_name\n\n        if self.gradcam_method_name is None:\n            self.gradcam_method_name = gradcam_method_name\n\n        # update the max profile result\n        self.cpu_memory_usage_bytes = max(\n            self.cpu_memory_usage_bytes, profile_result[\"cpu_memory_usage\"]\n        )\n        # self cpu memory usage could be negative. here we take the value that has the max absolute value\n        if abs(profile_result[\"self_cpu_memory_usage\"]) > abs( self.self_cpu_memory_usage_bytes):\n            self.self_cpu_memory_usage_bytes = profile_result[\"self_cpu_memory_usage\"]\n        self.device_memory_usage_bytes = max(\n            self.device_memory_usage_bytes, profile_result[\"device_memory_usage\"]\n        )\n        # same as self cpu memory usage\n        if abs(profile_result[\"self_device_memory_usage\"]) > abs(self.self_device_memory_usage_bytes):\n            self.self_device_memory_usage_bytes = profile_result[\"self_device_memory_usage\"]\n        self.cpu_time_total_us = max(\n            self.cpu_time_total_us, profile_result[\"cpu_time_total\"]\n        )\n        self.self_cpu_time_total_us = max(\n            self.self_cpu_time_total_us, profile_result[\"self_cpu_time_total\"]\n        )\n        self.device_time_total_us = max(\n            self.device_time_total_us, profile_result[\"device_time_total\"]\n        )\n        self.self_device_time_total_us = max(\n            self.self_device_time_total_us, profile_result[\"self_device_time_total\"]\n        )\n\n        self.response_counter += 1\n\n    def complete_profile(self):\n        print(\"\\n--------- PROFILE EVENT ---------\\n\")\n        print(f\"Name: {self.profile_name}\")\n        print(f\"Device Type: {self.device_type}\")\n        print(f\"Device Name: {self.device_name}\")\n        print(f\"Node ID: {self.node_id}\")\n        print(f\"K8S_POD_NAME: {self.k8s_pod_name}\")\n\n        if self.gradcam_method_name:\n            print(f\"GradCAM Method Name: {self.gradcam_method_name}\")\n\n        print(\"\\n--------- LATENCY RESULT ---------\\n\")\n        print(\"Service Initialization Duration: \", self.service_initialization_duration)\n        print(\"Total Requests: \", self.response_counter)\n        print(f\"Total Time Taken: {time.time() - self.start_time:.2f} seconds\")\n        print(\n            f\"Average Time Taken: {(time.time() - self.start_time) / self.response_counter:.2f} seconds\"\n        )\n\n        print(\"\\n--------- RESOURCE USAGE ---------\\n\")\n        print(f\"CPU Memory Usage: {self.cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\")\n        print(\n            f\"Self CPU Memory Usage: {self.self_cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Device Memory Usage: {self.device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Self Device Memory Usage: {self.self_device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(f\"CPU Time Total: {self.cpu_time_total_us / 1000:.2f} ms\")\n        print(f\"Self CPU Time Total: {self.self_cpu_time_total_us / 1000:.2f} ms\")\n        print(f\"Device Time Total: {self.device_time_total_us / 1000:.2f} ms\")\n        print(f\"Self Device Time Total: {self.self_device_time_total_us / 1000:.2f} ms\")\n\n        # update the service_data.json automatically\n        with open(\"service_data.json\", \"r\") as f:\n            service_data = json.load(f)\n\n        if not self.gradcam_method_name:\n            complete_profile_data_to_save = {\n                \"node_id\": self.node_id,\n                \"device_type\": self.device_type,\n                \"device_name\": self.device_name,\n                \"initialization_time_ms\": self.service_initialization_duration * 1000,\n                \"eviction_time_ms\": 0,\n                \"initialization_cost\": 0,\n                \"keep_alive_cost\": 0,\n                \"energy_consumption_idle\": 0,\n                \"inference\": {\n                    \"cpu_time_ms\": self.cpu_time_total_us / 1000,\n                    \"device_time_ms\": self.device_time_total_us / 1000,\n                    \"cpu_memory_usage_MB\": self.cpu_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"self_cpu_memory_usage_MB\": self.self_cpu_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"device_memory_usage_MB\": self.device_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"self_device_memory_usage_MB\": self.self_device_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"energy_consumption_execution\": 0,\n                    \"disk_IO_MB\": 0,\n                    \"input_data_MB\": 0,\n                    \"output_data_MB\": 0,\n                    \"execution_time_ms\": (time.time() - self.start_time)\n                    / self.response_counter\n                    * 1000,\n                    \"execution_cost\": 0,\n                },\n            }\n\n            # check if there is already a profile for this node id\n            profile_found = False\n            for profile in service_data[\"profiles\"]:\n                if profile[\"node_id\"] == self.node_id:\n                    profile[\"inference\"] = complete_profile_data_to_save[\"inference\"]\n                    profile_found = True\n                    break\n            if not profile_found:\n                service_data[\"profiles\"].append(complete_profile_data_to_save)\n\n            # save the updated service_data.json\n            with open(\"service_data.json\", \"w\") as f:\n                json.dump(service_data, f, indent=4)\n            print(\"\\n--------- SERVICE DATA UPDATED ---------\\n\")\n\n        else:\n            complete_xai_profile_data_to_save = {\n                \"node_id\": self.node_id,\n                \"device_type\": self.device_type,\n                \"device_name\": self.device_name,\n                \"initialization_time_ms\": self.service_initialization_duration * 1000,\n                \"eviction_time_ms\": 0,\n                \"initialization_cost\": 0,\n                \"keep_alive_cost\": 0,\n                \"energy_consumption_idle\": 0,\n                \"xai\": [\n                    {\n                        \"xai_method\": self.gradcam_method_name,\n                        \"cpu_time_ms\": self.cpu_time_total_us / 1000,\n                        \"device_time_ms\": self.device_time_total_us / 1000,\n                        \"cpu_memory_usage_MB\": self.cpu_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"self_cpu_memory_usage_MB\": self.self_cpu_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"device_memory_usage_MB\": self.device_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"self_device_memory_usage_MB\": self.self_device_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"energy_consumption_execution\": 0,\n                        \"disk_IO_MB\": 0,\n                        \"input_data_MB\": 0,\n                        \"output_data_MB\": 0,\n                        \"execution_time_ms\": (time.time() - self.start_time)\n                        / self.response_counter\n                        * 1000,\n                        \"execution_cost\": 0,\n                    }\n                ],\n            }\n\n            # check if there is already a profile for this node id\n            profile_found = False\n            for profile in service_data[\"profiles\"]:\n                if profile[\"node_id\"] == self.node_id:\n                    profile_found = True\n\n                    # check if there is already a profile for this xai method\n                    xai_method_found = False\n                    if not profile.get(\"xai\"):\n                        profile[\"xai\"] = []\n                    for xai_profile in profile[\"xai\"]:\n                        if xai_profile[\"xai_method\"] == self.gradcam_method_name:\n                            xai_profile.update(complete_xai_profile_data_to_save[\"xai\"][0]) \n                            xai_method_found = True\n                            break\n                    if not xai_method_found:\n                        profile[\"xai\"].append(complete_xai_profile_data_to_save[\"xai\"][0])\n                    break\n\n            if not profile_found:\n                service_data[\"profiles\"].append(complete_xai_profile_data_to_save)\n\n            # save the updated service_data.json\n            with open(\"service_data.json\", \"w\") as f:\n                json.dump(service_data, f, indent=4)\n            print(\"\\n--------- SERVICE DATA UPDATED ---------\\n\")\n\n\ndef option_run():\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n    data = {**data, \"ue_id\": UE_ID}\n    response, process_time, node_id, pod_name = send_post_request(\n        f\"{SERVER_URL}/model/run\", data, files\n    )\n    print(\"Process Time: \", process_time)\n    print(\"Node ID: \", node_id)\n    print(\"K8S_POD_NAME: \", pod_name)\n    print(\"Response\")\n    print(json.dumps(response, indent=4))\n\n\ndef option_profile_run():\n    data = prepare_ai_service_request_data()\n    data = {**data, \"ue_id\": UE_ID}\n    files = prepare_ai_service_request_files()\n    num_requests = int(input(\"Enter the number of requests to send: \"))\n\n    try:\n        profile_result_processor = ProfileResultProcessor(SERVER_URL)\n\n        for _ in range(num_requests):\n            profile_response, process_time, node_id, k8s_node_name = send_post_request(\n                f\"{SERVER_URL}/model/profile_run\", data, files\n            )\n            print(\"Process Time: \", process_time)\n            print(\"Node ID: \", node_id)\n            print(\"K8S_POD_NAME: \", k8s_node_name)\n            print(\"Response\")\n            print(json.dumps(profile_response, indent=4))\n            if not profile_response:\n                print(\"No profile response received.\")\n                continue\n\n            profile_result_processor.process_new_response(\n                profile_response,\n                process_time=process_time,\n                node_id=node_id,\n                k8s_pod_name=k8s_node_name,\n            )\n\n        # Print the final profile result and update the service_data.json\n        profile_result_processor.complete_profile()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\n\ndef option_help():\n    \"\"\"Get help information from the server.\"\"\"\n    try:\n        response, process_time, node_id, pod_name = send_get_request(\n            f\"{SERVER_URL}/help\"\n        )\n        print(\"Process Time: \", process_time)\n        print(\"Node ID: \", node_id)\n        print(\"K8S_POD_NAME: \", pod_name)\n        print(\"Response\")\n        print(json.dumps(response, indent=4))\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n\n\ndef option_run_with_xai():\n    \"\"\"Run AI service with XAI.\"\"\"\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n\n    print(\n        \"Note that currently only GradCAM methods on image-classification models are supported.\"\n    )\n    while True:\n        gradcam_method_name = input(\n            f\"Please select a GradCAM method (options: {XAI_GRADCAM_METHODS}): \"\n        )\n        if gradcam_method_name not in XAI_GRADCAM_METHODS:\n            print(f\"Invalid GradCAM method. Please select again.\")\n        else:\n            break\n\n    # ask for target class for explanation\n    target_category_indexes = input(\n        \"Please input target category indexes for explanation (comma-separated, e.g., 111, 32, 44, ...): \"\n    )\n    if not target_category_indexes or not target_category_indexes.strip():\n        print(\n            \"No target category indexes provided. Defaulting to explaining the top confident category.\"\n        )\n        target_category_indexes = []\n    else:\n        target_category_indexes = [\n            int(i.strip()) for i in target_category_indexes.split(\",\")\n        ]\n\n    data = {\n        **data,\n        \"ue_id\": UE_ID,\n        \"gradcam_method_name\": gradcam_method_name,\n        \"target_category_indexes\": target_category_indexes,\n    }\n    print(\"Data: \", data)\n    response, process_time, node_id, k8s_pod_name = send_post_request(\n        f\"{SERVER_URL}/xai_model/run\", data, files\n    )\n    print(\"Process Time: \", process_time)\n    print(\"Node ID: \", node_id)\n    print(\"K8S_POD_NAME: \", k8s_pod_name)\n\n    # Handle JSON response\n    model_results = response.get(\"model_results\")\n    if model_results:\n        print(\"Model Results:\", json.dumps(model_results, indent=4))\n\n    xai_results = response.get(\"xai_results\")\n    if xai_results:\n        print(\"XAI Results Method:\", xai_results.get(\"xai_method\"))\n\n    # Handle binary image response\n    encoded_image = xai_results.get(\"image\")\n    if encoded_image:\n        image_bytes = base64.b64decode(encoded_image)\n\n        # Load the image into Pillow\n        image = Image.open(BytesIO(image_bytes))\n\n        # Save image to disk\n        image.save(\"xai_output.png\")\n\n        # Display the image using matplotlib\n        plt.imshow(image)\n        plt.axis(\"off\")\n        plt.show()\n\n\ndef option_profile_run_with_xai():\n    \"\"\"Run AI service with XAI and profile the run.\"\"\"\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n    print(\n        \"Note that currently only GradCAM methods on image-classification models are supported.\"\n    )\n\n    # ask for target class for explanation\n    target_category_indexes = input(\n        \"Please input target category indexes for explanation (comma-separated, e.g., 111, 32, 44, ...): \"\n    )\n    if not target_category_indexes or not target_category_indexes.strip():\n        print(\n            \"No target category indexes provided. Defaulting to explaining the top confident category.\"\n        )\n        target_category_indexes = []\n    else:\n        target_category_indexes = [\n            int(i.strip()) for i in target_category_indexes.split(\",\")\n        ]\n\n    num_requests = int(input(\"Enter the number of requests to send: \"))\n\n    try:\n        for gradcam_method_name in XAI_GRADCAM_METHODS:\n\n            data = {\n                **data,\n                \"ue_id\": UE_ID,\n                \"gradcam_method_name\": gradcam_method_name,\n                \"target_category_indexes\": target_category_indexes,\n            }\n            print(\"Data: \", data)\n\n            profile_result_processor = ProfileResultProcessor(SERVER_URL)\n\n            for _ in range(num_requests):\n                response, process_time, node_id, k8s_pod_name = send_post_request(\n                    f\"{SERVER_URL}/xai_model/profile_run\", data, files\n                )\n                print(\"Process Time: \", process_time)\n                print(\"Node ID: \", node_id)\n                print(\"K8S_POD_NAME: \", k8s_pod_name)\n                if not response:\n                    print(\"No profile response received.\")\n                    continue\n\n                # Handle JSON response\n                model_results = response.get(\"model_results\")\n                if model_results:\n                    print(\"Model Results:\", json.dumps(model_results, indent=4))\n\n                profile_result_processor.process_new_response(\n                    response,\n                    process_time=process_time,\n                    node_id=node_id,\n                    k8s_pod_name=k8s_pod_name,\n                    gradcam_method_name=gradcam_method_name,\n                )\n\n            # Print the final profile result and update the service_data.json\n            profile_result_processor.complete_profile()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\n\nOPTIONS = [\n    {\n        \"label\": \"Get help information\",\n        \"action\": option_help,\n    },\n    {\n        \"label\": \"Run AI service\",\n        \"action\": option_run,\n    },\n    {\n        \"label\": \"Profile AI service\",\n        \"action\": option_profile_run,\n    },\n    {\n        \"label\": \"Run AI service with XAI (only image-classification models)\",\n        \"action\": option_run_with_xai,\n    },\n    {\n        \"label\": \"Profile AI service with XAI (only image-classification models)\",\n        \"action\": option_profile_run_with_xai,\n    },\n]\n\n\nif __name__ == \"__main__\":\n    while True:\n        print(\"\\nOptions:\")\n        for i, option in enumerate(OPTIONS, start=1):\n            print(f\"{i}. {option['label']}\")\n        print(\"q. Quit\")\n        choice = input(\"Enter your choice: \")\n\n        if choice == \"q\":\n            print(\"Exiting the client. Goodbye!\")\n            break\n        else:\n            try:\n                choice = int(choice)\n                if 1 <= choice <= len(OPTIONS):\n                    OPTIONS[choice - 1][\"action\"]()\n                else:\n                    print(\"Invalid choice. Please try again.\")\n            except ValueError:\n                print(\"Invalid input. Please enter a number.\")\n",
            "ai_client_utils_script_content": "def prepare_ai_service_request_files():\n    \"\"\"Prepare the `files` part for the AI service request.\"\"\"\n    files = {}\n    image_file_path = input(\"Please input the image file path: \")\n    with open(image_file_path, \"rb\") as image_file:\n        files[\"file\"] = image_file.read()\n    return files\n\ndef prepare_ai_service_request_data():\n    \"\"\"Prepare the `data` part including `ue_id` for the AI service request.\"\"\"\n    data = {}\n    data[\"ue_id\"] = input(\"Please input the unique execution ID (ue_id): \")\n    return data",
            "model_script_content": "# import server utils\nfrom ai_server_utils import (\n    process_model_output_logits,\n    profile_activities,\n    prepare_profile_results,\n)\n\n# import profile utils\nfrom torch.profiler import profile, record_function\n\n# import necessary libs for AI model inference and request handling\nimport torch\nfrom fastapi import APIRouter, File, Form, UploadFile\nfrom fastapi.responses import JSONResponse\nfrom transformers import ConvNextImageProcessor, ConvNextForImageClassification\nfrom PIL import Image\n\n# --------------------------------\n# Device configuration\n# --------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# --------------------------------\n# Model-specific configuration\n# make sure the variables `MODEL_NAME` and `model` are defined here.\n# --------------------------------\nMODEL_NAME = \"facebook/convnext-tiny-224\"\nprocessor = ConvNextImageProcessor.from_pretrained(MODEL_NAME)\nmodel = ConvNextForImageClassification.from_pretrained(MODEL_NAME).to(device)\nmodel.eval()\n\n# Initialize the FastAPI router\nrouter = APIRouter()\n\n\n@router.post(\"/run\")\nasync def run_model(file: UploadFile = File(...), ue_id: str = Form(...)):\n    try:\n        # Prepare the model input\n        image = Image.open(file.file).convert(\"RGB\")\n        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n\n        # Perform inference\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        # Process the model outputs\n        predictions = process_model_output_logits(model, outputs.logits)\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"model_results\": predictions,\n            }\n        )\n    except Exception as e:\n        print(f\"Error processing file: {e}\")\n        return JSONResponse(\n            content={\"error\": \"Failed to process the image. {e}\".format(e=str(e))},\n            status_code=500,\n        )\n\n\n@router.post(\"/profile_run\")\nasync def profile_run(file: UploadFile = File(...), ue_id: str = Form(...)):\n    \"\"\"\n    Endpoint to profile the AI model execution.\n    \"\"\"\n    try:\n        # Prepare the model input\n        image = Image.open(file.file).convert(\"RGB\")\n        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n\n        # perform profiling\n        with profile(\n            activities=profile_activities,\n            profile_memory=True,\n        ) as prof:\n            with record_function(\"model_run\"):\n                with torch.no_grad():\n                    model_outputs = model(**inputs)\n\n        profile_result = prepare_profile_results(prof)\n\n        # Process the model outputs\n        predictions = process_model_output_logits(model, model_outputs.logits)\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"profile_result\": profile_result,\n                \"model_results\": predictions,\n            }\n        )\n\n    except Exception as e:\n        print(f\"Error processing request: {e}\")\n        return JSONResponse(\n            content={\"error\": f\"Failed to process the request. {e}\"},\n            status_code=500,\n        )\n\n\n# Below are the model input and output specifications to be used by the `/help` endpoint\nMODEL_INPUT_FORM_SPEC = {\n    \"file\": {\n        \"type\": \"file upload\",\n        \"description\": \"The image file to be classified.\",\n        \"required\": True,\n        \"example\": \"puppy.png\",\n    }\n}\n\nMODEL_OUTPUT_JSON_SPEC = {\n    \"ue_id\": \"unique execution ID\",\n    \"model_results\": [\n        {\n            \"category_id\": \"category id\",\n            \"label\": \"category label\",\n            \"probability\": \"probability value\",\n        }\n    ],\n}\n",
            "healthcheck_script_content": "import requests\n\nprint(requests.get(\"http://localhost:8000/help\"))"
        }
    },
    {
        "model_name": "facebook/regnet-y-040",
        "model_url": "https://huggingface.co/facebook/regnet-y-040",
        "task": "image-classification",
        "task_detail": "The pre-trained AI model `facebook/regnet-y-040` is a sophisticated convolutional neural network designed for image classification tasks. This model has been trained on the extensive ImageNet-1k dataset, making it capable of classifying images into one of 1000 different categories, ranging from various animals, objects, scenes, and more. The inherent structure of RegNet models originates from a study focused on devising optimal neural architecture search spaces, where constraints are gradually applied to reduce the dimensionality of the search space based on empirical observations and model performance metrics.\n\nKey functionalities of the RegNet model include its ability to process and analyze visual data efficiently, leveraging its architecture to recognize diverse patterns and features across numerous image categories. The model accepts input images in standard formats, typically as tensors that can be generated from raw images using a feature extractor. The expected input is a pre-processed image tensor that is compatible with PyTorch's data handling requirements.\n\nUpon receiving an input image, the model processes it through multiple layers, extracting features and passing them through linear classifiers to produce class predictions. The output of the model is a tensor of logits, representing the likelihood of the image belonging to each of the 1000 predefined classes. The class with the highest score is chosen as the model\u2019s predicted label, which corresponds to a human-readable category name obtained from the model\u2019s configuration.\n\nThe RegNet model's capabilities are particularly beneficial in scenarios requiring precise and efficient image classification, such as automated tagging, image-based search systems, and integrated visual recognition in applications. Due to its robust training foundation on the ImageNet dataset, the model performs effectively across a wide variety of real-world image classification tasks.\n\nPotential users can apply the RegNet model directly for generic image classification or look for fine-tuned versions tailored to specific tasks. The Hugging Face library offers an accessible interface to load both the feature extractor and the model itself, allowing users to integrate RegNet seamlessly into their image processing pipelines. Furthermore, the model's performance can be augmented by additional fine-tuning, aligning the model's predictive focus with specific domains or datasets outside its original training domain.",
        "accuracy_info": "No accuracy information found.",
        "image_repository_url": "",
        "service_disk_size_bytes": 3742763044.0,
        "profiles": [
            {
                "node_id": "ugurcan.celik",
                "device_type": "DeviceType.CPU",
                "device_name": "cuda",
                "initialization_time_ms": 5193.088054656982,
                "eviction_time_ms": 0.0,
                "initialization_cost": 0.0,
                "keep_alive_cost": 0.0,
                "energy_consumption_idle": 0.0,
                "idle_container_cpu_memory_usage": "2.8GB",
                "idle_container_device_memory_usage": "1.8GB",
                "inference": {
                    "cpu_time_ms": 219.72628999999998,
                    "device_time_ms": 3.215011999999806,
                    "cpu_memory_usage_MB": 0.0,
                    "self_cpu_memory_usage_MB": 0.0,
                    "device_memory_usage_MB": 8.12890625,
                    "self_device_memory_usage_MB": -149.8447265625,
                    "energy_consumption_execution": 0.0,
                    "disk_IO_MB": 0.0,
                    "input_data_MB": 0.0,
                    "output_data_MB": 0.0,
                    "execution_time_ms": 486.8327776590983,
                    "execution_cost": 0.0
                },
                "xai": [
                    {
                        "xai_method": "GradCAM",
                        "cpu_time_ms": 238.73406699999998,
                        "device_time_ms": 1.996701000000014,
                        "cpu_memory_usage_MB": 0.40673828125,
                        "self_cpu_memory_usage_MB": 0.199554443359375,
                        "device_memory_usage_MB": 191.07177734375,
                        "self_device_memory_usage_MB": -48.4697265625,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1199.729363123576,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "HiResCAM",
                        "cpu_time_ms": 46.233169000000004,
                        "device_time_ms": 1.9862120000000296,
                        "cpu_memory_usage_MB": 0.40673828125,
                        "self_cpu_memory_usage_MB": 0.199554443359375,
                        "device_memory_usage_MB": 108.4951171875,
                        "self_device_memory_usage_MB": -51.8994140625,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1025.1832008361816,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "XGradCAM",
                        "cpu_time_ms": 49.645351999999995,
                        "device_time_ms": 1.9811869999999885,
                        "cpu_memory_usage_MB": 0.40673828125,
                        "self_cpu_memory_usage_MB": 0.199554443359375,
                        "device_memory_usage_MB": 107.7900390625,
                        "self_device_memory_usage_MB": -51.1064453125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1009.2851320902507,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "GradCAMPlusPlus",
                        "cpu_time_ms": 48.852695999999995,
                        "device_time_ms": 1.9782630000000132,
                        "cpu_memory_usage_MB": 0.40673828125,
                        "self_cpu_memory_usage_MB": 0.199554443359375,
                        "device_memory_usage_MB": 106.4189453125,
                        "self_device_memory_usage_MB": -48.6416015625,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1088.4379545847576,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "ScoreCAM",
                        "cpu_time_ms": 1589.004655,
                        "device_time_ms": 1512.8805330000093,
                        "cpu_memory_usage_MB": 221.468994140625,
                        "self_cpu_memory_usage_MB": -0.016265869140625,
                        "device_memory_usage_MB": 104.8525390625,
                        "self_device_memory_usage_MB": -164026.59521484375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 28103.729168574017,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "LayerCAM",
                        "cpu_time_ms": 44.662627,
                        "device_time_ms": 1.9946639999999962,
                        "cpu_memory_usage_MB": 0.40673828125,
                        "self_cpu_memory_usage_MB": 0.199554443359375,
                        "device_memory_usage_MB": 107.1494140625,
                        "self_device_memory_usage_MB": -48.4462890625,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1724.8480319976807,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenCAM",
                        "cpu_time_ms": 80.320826,
                        "device_time_ms": 1.9910779999999904,
                        "cpu_memory_usage_MB": 0.203369140625,
                        "self_cpu_memory_usage_MB": -0.003814697265625,
                        "device_memory_usage_MB": 107.1494140625,
                        "self_device_memory_usage_MB": -47.375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 381.12791379292804,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenGradCAM",
                        "cpu_time_ms": 146.129822,
                        "device_time_ms": 1.9931249999999958,
                        "cpu_memory_usage_MB": 0.40673828125,
                        "self_cpu_memory_usage_MB": 0.199554443359375,
                        "device_memory_usage_MB": 108.5595703125,
                        "self_device_memory_usage_MB": -47.3759765625,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1186.2209637959797,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "KPCA_CAM",
                        "cpu_time_ms": 24.108463,
                        "device_time_ms": 1.9934810000000007,
                        "cpu_memory_usage_MB": 0.203369140625,
                        "self_cpu_memory_usage_MB": -0.003814697265625,
                        "device_memory_usage_MB": 104.8994140625,
                        "self_device_memory_usage_MB": -49.04296875,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 405.35275141398114,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "RandomCAM",
                        "cpu_time_ms": 45.61374000000001,
                        "device_time_ms": 1.9952040000000002,
                        "cpu_memory_usage_MB": 0.40673828125,
                        "self_cpu_memory_usage_MB": 0.199554443359375,
                        "device_memory_usage_MB": 107.1962890625,
                        "self_device_memory_usage_MB": -49.3603515625,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1025.1701672871907,
                        "execution_cost": 0.0
                    }
                ]
            },
            {
                "node_id": "LAP004262",
                "device_type": "DeviceType.CPU",
                "device_name": "None",
                "initialization_time_ms": 14281.233072280884,
                "eviction_time_ms": 0.0,
                "initialization_cost": 0.0,
                "keep_alive_cost": 0.0,
                "energy_consumption_idle": 0.0,
                "idle_container_cpu_memory_usage": "1.2GB",
                "idle_container_device_memory_usage": "0GB",
                "inference": {
                    "cpu_time_ms": 72.249398,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 0.003814697265625,
                    "self_cpu_memory_usage_MB": -149.41693115234375,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0.0,
                    "disk_IO_MB": 0.0,
                    "input_data_MB": 0.0,
                    "output_data_MB": 0.0,
                    "execution_time_ms": 288.8631025950114,
                    "execution_cost": 0.0
                },
                "xai": [
                    {
                        "xai_method": "GradCAM",
                        "cpu_time_ms": 245.704992,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 182.0909423828125,
                        "self_cpu_memory_usage_MB": -125.6288833618164,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 653.8693110148112,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "HiResCAM",
                        "cpu_time_ms": 200.908825,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 103.3302001953125,
                        "self_cpu_memory_usage_MB": -125.6288833618164,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 774.3633588155111,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "XGradCAM",
                        "cpu_time_ms": 192.11159300000003,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 103.3302001953125,
                        "self_cpu_memory_usage_MB": -125.6288833618164,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 761.1779371897379,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "GradCAMPlusPlus",
                        "cpu_time_ms": 243.167851,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 103.3302001953125,
                        "self_cpu_memory_usage_MB": -125.6288833618164,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 777.3995399475098,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "LayerCAM",
                        "cpu_time_ms": 217.121417,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 103.3302001953125,
                        "self_cpu_memory_usage_MB": -125.6288833618164,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 804.8938910166422,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenCAM",
                        "cpu_time_ms": 292.45896300000004,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 103.1268310546875,
                        "self_cpu_memory_usage_MB": -46.868133544921875,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 479.8453648885091,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenGradCAM",
                        "cpu_time_ms": 576.874726,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 103.3302001953125,
                        "self_cpu_memory_usage_MB": -125.6288833618164,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1020.3526020050049,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "KPCA_CAM",
                        "cpu_time_ms": 72.49305,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 103.1268310546875,
                        "self_cpu_memory_usage_MB": -46.868133544921875,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 224.89190101623535,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "RandomCAM",
                        "cpu_time_ms": 231.280731,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 103.3302001953125,
                        "self_cpu_memory_usage_MB": -125.6288833618164,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 810.3094895680746,
                        "execution_cost": 0.0
                    }
                ]
            }
        ],
        "feedback": {
            "likes": [],
            "dislikes": [],
            "comments": []
        },
        "code": {
            "readme_content": "---\nlicense: apache-2.0\ntags:\n- vision\n- image-classification\n\ndatasets:\n- imagenet-1k\n\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\n  example_title: Tiger\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg\n  example_title: Teapot\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg\n  example_title: Palace\n\n---\n\n# RegNet\n\nRegNet model trained on imagenet-1k. It was introduced in the paper [Designing Network Design Spaces](https://arxiv.org/abs/2003.13678) and first released in [this repository](https://github.com/facebookresearch/pycls). \n\nDisclaimer: The team releasing RegNet did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThe authors design search spaces to perform Neural Architecture Search (NAS). They first start from a high dimensional search space and iteratively reduce the search space by empirically applying constraints based on the best-performing models sampled by the current search space.\n\n![model image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/regnet_architecture.png)\n\n## Intended uses & limitations\n\nYou can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=regnet) to look for\nfine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model:\n\n```python\n>>> from transformers import AutoFeatureExtractor, RegNetForImageClassification\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"huggingface/cats-image\")\n>>> image = dataset[\"test\"][\"image\"][0]\n\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/regnet-y-040\")\n>>> model = RegNetForImageClassification.from_pretrained(\"facebook/regnet-y-040\")\n\n>>> inputs = feature_extractor(image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> # model predicts one of the 1000 ImageNet classes\n>>> predicted_label = logits.argmax(-1).item()\n>>> print(model.config.id2label[predicted_label])\n'tabby, tabby cat'\n```\n\nFor more code examples, we refer to the [documentation](https://huggingface.co/docs/transformers/master/en/model_doc/regnet).",
            "dockerfile_content": "# Base image for AI service powered by HuggingFace pre-trained AI models.\n# the image has the following packages/libraries installed already:\n# - python3.12, pip, git, fastapi, uvicorn, torch, torchvision, opencv-python, transformers, python-multipart, Pillow, requests\nFROM python3.12_ai_service_base:latest\n\n# Set working directory\nWORKDIR /app\n\n# Copy application code\nCOPY . .\n\n# Install additional dependencies\nRUN pip install datasets\n\n# Expose port 8000\nEXPOSE 8000\n\n# Start the FastAPI server\nCMD [\"uvicorn\", \"ai_server:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--timeout-keep-alive\", \"600\"]",
            "ai_server_script_content": "import json\nimport os\nimport time\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import JSONResponse\nfrom contextlib import asynccontextmanager\nfrom ai_server_utils import (\n    PROFILE_OUTPUT_JSON_SPEC,\n    NODE_ID,\n    K8S_POD_NAME,\n)\n\n\n# -------------------------------------------\n# App Lifespan setup\n# -------------------------------------------\n# Record the script start time (when uvicorn starts the process)\nSCRIPT_START_TIME = time.time()\nINITIALIZATION_DURATION = 0.0\nservice_endpoint_specs = {\n    \"model_input_form_spec\": None,\n    \"model_output_json_spec\": None,\n    \"profile_output_json_spec\": None,\n    \"xai_model_input_form_spec\": None,\n    \"xai_model_output_json_spec\": None,\n    \"xai_profile_output_json_spec\": None,\n}\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n\n    global INITIALIZATION_DURATION\n    global SCRIPT_START_TIME\n    global service_endpoint_specs\n\n    # Load the AI model\n    print(\"Loading AI model...\")\n    from model import (\n        MODEL_INPUT_FORM_SPEC,\n        MODEL_OUTPUT_JSON_SPEC,\n        router as model_router,\n    )\n\n    service_endpoint_specs[\"model_input_form_spec\"] = MODEL_INPUT_FORM_SPEC\n    service_endpoint_specs[\"model_output_json_spec\"] = MODEL_OUTPUT_JSON_SPEC\n    service_endpoint_specs[\"profile_output_json_spec\"] = PROFILE_OUTPUT_JSON_SPEC\n\n    app.include_router(model_router, prefix=\"/model\", tags=[\"AI Model\"])\n\n    # Load the XAI model\n    if os.path.exists(os.path.dirname(__file__) + \"/xai_model.py\"):\n        print(\"Loading XAI model...\")\n        from xai_model import (\n            XAI_OUTPUT_JSON_SPEC,\n            router as xai_model_router,\n        )\n\n        # by default, the xai_model input form spec is the same as the model input form spec\n        service_endpoint_specs[\"xai_model_input_form_spec\"] = MODEL_INPUT_FORM_SPEC\n        service_endpoint_specs[\"xai_model_output_json_spec\"] = MODEL_OUTPUT_JSON_SPEC\n        service_endpoint_specs[\"xai_model_output_json_spec\"].update(\n            XAI_OUTPUT_JSON_SPEC\n        )\n        service_endpoint_specs[\"xai_profile_output_json_spec\"] = (\n            PROFILE_OUTPUT_JSON_SPEC\n        )\n        service_endpoint_specs[\"xai_profile_output_json_spec\"].update(\n            XAI_OUTPUT_JSON_SPEC\n        )\n\n        app.include_router(xai_model_router, prefix=\"/xai_model\", tags=[\"XAI Model\"])\n\n    # Record the initialization duration\n    INITIALIZATION_DURATION = time.time() - SCRIPT_START_TIME\n\n    print(f\"AI service loaded in {INITIALIZATION_DURATION:.2f} seconds.\")\n\n    yield\n\n    # Clean up the models and release the resources\n    service_endpoint_specs.clear()\n\n\n# -------------------------------------------\n# FastAPI application setup\n# -------------------------------------------\napp = FastAPI(lifespan=lifespan)\n\n\n# -------------------------------------------\n# Middlewares\n# -------------------------------------------\n@app.middleware(\"http\")\nasync def prepare_header_middleware(request: Request, call_next):\n    start_time = time.perf_counter()\n    response = await call_next(request)\n    process_time = time.perf_counter() - start_time\n    response.headers[\"X-Process-Time\"] = str(process_time)\n    response.headers[\"X-NODE-ID\"] = NODE_ID\n    response.headers[\"X-K8S-POD-NAME\"] = K8S_POD_NAME\n    return response\n\n\n# -------------------------------------------\n# General Endpoints\n# -------------------------------------------\n@app.get(\"/initialization_duration\")\ndef get_initialization_duration():\n    \"\"\"\n    Endpoint to retrieve the initialization duration of the AI model.\n    \"\"\"\n    global INITIALIZATION_DURATION\n\n    if INITIALIZATION_DURATION == 0.0:\n        return JSONResponse(\n            content={\"error\": \"Model not initialized.\"},\n            status_code=500,\n        )\n    return JSONResponse(\n        content={\n            \"initialization_duration\": INITIALIZATION_DURATION,\n            \"script_start_time\": SCRIPT_START_TIME,\n        }\n    )\n\n\n@app.get(\"/help\")\ndef get_help():\n    global service_endpoint_specs\n    help_info = {\n        \"endpoints\": {\n            \"/model/run\": {\n                \"method\": \"POST\",\n                \"description\": \"Executes the AI model with the provided input data.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_output_json_spec\"],\n                },\n            },\n            \"/model/profile_run\": {\n                \"method\": \"POST\",\n                \"description\": \"Profiles the AI model execution.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    \"profile_result\": \"Profiling results of the AI model execution.\",\n                    **service_endpoint_specs[\"profile_output_json_spec\"],\n                },\n            },\n            \"/initialization_duration\": {\n                \"method\": \"GET\",\n                \"description\": \"Retrieves the initialization duration of the AI model.\",\n                \"response\": {\n                    \"initialization_duration\": \"Time taken to initialize the model (in seconds).\",\n                    \"script_start_time\": \"Timestamp when the script started (in seconds since epoch).\",\n                },\n            },\n        }\n    }\n\n    if service_endpoint_specs[\"xai_model_input_form_spec\"] is not None:\n        help_info[\"endpoints\"][\"/xai_model/run\"] = {\n            \"method\": \"POST\",\n            \"description\": \"Executes the XAI model with the provided input data.\",\n            \"parameters\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_input_form_spec\"],\n            },\n            \"response\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_output_json_spec\"],\n            },\n        }\n\n        help_info[\"endpoints\"][\"/xai_model/profile_run\"] = {\n            \"method\": \"POST\",\n            \"description\": \"Profiles the XAI model execution.\",\n            \"parameters\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_input_form_spec\"],\n            },\n            \"response\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                \"profile_result\": \"Profiling results of the XAI model execution.\",\n                **service_endpoint_specs[\"xai_profile_output_json_spec\"],\n            },\n        }\n    \n    return JSONResponse(content=help_info)\n",
            "ai_client_script_content": "import base64\nimport json\nimport time\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nfrom ai_client_utils import (\n    prepare_ai_service_request_files,\n    prepare_ai_service_request_data,\n)\n\nXAI_GRADCAM_METHODS = [\n    \"GradCAM\",\n    \"HiResCAM\",\n    # \"AblationCAM\",\n    \"XGradCAM\",\n    \"GradCAMPlusPlus\",\n    \"ScoreCAM\",\n    \"LayerCAM\",\n    \"EigenCAM\",\n    \"EigenGradCAM\",\n    \"KPCA_CAM\",\n    \"RandomCAM\",\n]\n\n# -------------------------------------\n# prompt for necessary inputs\n# -------------------------------------\nSERVER_URL = input(\"Please input server URL (default to http://localhost:9000): \")\nif SERVER_URL.strip() == \"\":\n    SERVER_URL = \"http://localhost:9000\"\nUE_ID = input(\"Please input UE_ID (default to 123456): \")\nif UE_ID.strip() == \"\":\n    UE_ID = \"123456\"\n\n\ndef send_post_request(url, data, files):\n    \"\"\"Send request to run AI service and display AI service responses.\"\"\"\n    try:\n        response = requests.post(url, files=files, data=data)\n        # get the process time, node id and k8s pod name from the response headers\n        process_time = response.headers.get(\"X-Process-Time\")\n        node_id = response.headers.get(\"X-NODE-ID\")\n        k8s_pod_name = response.headers.get(\"X-K8S-POD-NAME\")\n        if response.status_code == 200:\n            return response.json(), process_time, node_id, k8s_pod_name\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n\ndef send_get_request(url, params=None):\n    \"\"\"Send GET request to the specified URL and return the response.\"\"\"\n    try:\n        response = requests.get(url, params=params)\n        process_time = response.headers.get(\"X-Process-Time\")\n        node_id = response.headers.get(\"X-NODE-ID\")\n        k8s_pod_name = response.headers.get(\"X-K8S-POD-NAME\")\n        if response.status_code == 200:\n            return response.json(), process_time, node_id, k8s_pod_name\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n            return None\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n\nclass ProfileResultProcessor:\n\n    def __init__(self, server_url):\n        self.server_url = server_url\n        self.start_time = time.time()\n        self.service_initialization_duration = 0\n        self.response_counter = 0\n        self.profile_name = None\n        self.device_type = None\n        self.device_name = None\n        self.node_id = None\n        self.k8s_pod_name = None\n        self.cpu_memory_usage_bytes = 0\n        self.self_cpu_memory_usage_bytes = 0\n        self.device_memory_usage_bytes = 0\n        self.self_device_memory_usage_bytes = 0\n        self.cpu_time_total_us = 0\n        self.self_cpu_time_total_us = 0\n        self.device_time_total_us = 0\n        self.self_device_time_total_us = 0\n\n        # xai related\n        self.gradcam_method_name = None\n\n        self.fetch_service_initialization_duration()\n\n    def fetch_service_initialization_duration(self):\n        \"\"\"Fetch the service initialization duration from the server.\"\"\"\n        response, process_time, node_id, k8s_pod_name = send_get_request(\n            f\"{self.server_url}/initialization_duration\"\n        )\n        if response:\n            self.service_initialization_duration = response.get(\n                \"initialization_duration\", 0\n            )\n        else:\n            print(\"Failed to fetch initialization duration.\")\n            self.service_initialization_duration = 0\n\n    def process_new_response(\n        self,\n        profile_response,\n        process_time=None,\n        node_id=None,\n        k8s_pod_name=None,\n        gradcam_method_name=None,\n    ):\n        if not profile_response:\n            return\n\n        profile_result = profile_response[\"profile_result\"]\n\n        if not profile_result:\n            return\n\n        if self.profile_name is None:\n            self.profile_name = profile_result[\"name\"]\n        if self.device_type is None:\n            self.device_type = profile_result[\"device_type\"]\n        if self.device_name is None:\n            self.device_name = profile_result[\"device_name\"]\n        if self.node_id is None:\n            self.node_id = node_id\n        if self.k8s_pod_name is None:\n            self.k8s_pod_name = k8s_pod_name\n\n        if self.gradcam_method_name is None:\n            self.gradcam_method_name = gradcam_method_name\n\n        # update the max profile result\n        self.cpu_memory_usage_bytes = max(\n            self.cpu_memory_usage_bytes, profile_result[\"cpu_memory_usage\"]\n        )\n        # self cpu memory usage could be negative. here we take the value that has the max absolute value\n        if abs(profile_result[\"self_cpu_memory_usage\"]) > abs( self.self_cpu_memory_usage_bytes):\n            self.self_cpu_memory_usage_bytes = profile_result[\"self_cpu_memory_usage\"]\n        self.device_memory_usage_bytes = max(\n            self.device_memory_usage_bytes, profile_result[\"device_memory_usage\"]\n        )\n        # same as self cpu memory usage\n        if abs(profile_result[\"self_device_memory_usage\"]) > abs(self.self_device_memory_usage_bytes):\n            self.self_device_memory_usage_bytes = profile_result[\"self_device_memory_usage\"]\n        self.cpu_time_total_us = max(\n            self.cpu_time_total_us, profile_result[\"cpu_time_total\"]\n        )\n        self.self_cpu_time_total_us = max(\n            self.self_cpu_time_total_us, profile_result[\"self_cpu_time_total\"]\n        )\n        self.device_time_total_us = max(\n            self.device_time_total_us, profile_result[\"device_time_total\"]\n        )\n        self.self_device_time_total_us = max(\n            self.self_device_time_total_us, profile_result[\"self_device_time_total\"]\n        )\n\n        self.response_counter += 1\n\n    def complete_profile(self):\n        print(\"\\n--------- PROFILE EVENT ---------\\n\")\n        print(f\"Name: {self.profile_name}\")\n        print(f\"Device Type: {self.device_type}\")\n        print(f\"Device Name: {self.device_name}\")\n        print(f\"Node ID: {self.node_id}\")\n        print(f\"K8S_POD_NAME: {self.k8s_pod_name}\")\n\n        if self.gradcam_method_name:\n            print(f\"GradCAM Method Name: {self.gradcam_method_name}\")\n\n        print(\"\\n--------- LATENCY RESULT ---------\\n\")\n        print(\"Service Initialization Duration: \", self.service_initialization_duration)\n        print(\"Total Requests: \", self.response_counter)\n        print(f\"Total Time Taken: {time.time() - self.start_time:.2f} seconds\")\n        print(\n            f\"Average Time Taken: {(time.time() - self.start_time) / self.response_counter:.2f} seconds\"\n        )\n\n        print(\"\\n--------- RESOURCE USAGE ---------\\n\")\n        print(f\"CPU Memory Usage: {self.cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\")\n        print(\n            f\"Self CPU Memory Usage: {self.self_cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Device Memory Usage: {self.device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Self Device Memory Usage: {self.self_device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(f\"CPU Time Total: {self.cpu_time_total_us / 1000:.2f} ms\")\n        print(f\"Self CPU Time Total: {self.self_cpu_time_total_us / 1000:.2f} ms\")\n        print(f\"Device Time Total: {self.device_time_total_us / 1000:.2f} ms\")\n        print(f\"Self Device Time Total: {self.self_device_time_total_us / 1000:.2f} ms\")\n\n        # update the service_data.json automatically\n        with open(\"service_data.json\", \"r\") as f:\n            service_data = json.load(f)\n\n        if not self.gradcam_method_name:\n            complete_profile_data_to_save = {\n                \"node_id\": self.node_id,\n                \"device_type\": self.device_type,\n                \"device_name\": self.device_name,\n                \"initialization_time_ms\": self.service_initialization_duration * 1000,\n                \"eviction_time_ms\": 0,\n                \"initialization_cost\": 0,\n                \"keep_alive_cost\": 0,\n                \"energy_consumption_idle\": 0,\n                \"inference\": {\n                    \"cpu_time_ms\": self.cpu_time_total_us / 1000,\n                    \"device_time_ms\": self.device_time_total_us / 1000,\n                    \"cpu_memory_usage_MB\": self.cpu_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"self_cpu_memory_usage_MB\": self.self_cpu_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"device_memory_usage_MB\": self.device_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"self_device_memory_usage_MB\": self.self_device_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"energy_consumption_execution\": 0,\n                    \"disk_IO_MB\": 0,\n                    \"input_data_MB\": 0,\n                    \"output_data_MB\": 0,\n                    \"execution_time_ms\": (time.time() - self.start_time)\n                    / self.response_counter\n                    * 1000,\n                    \"execution_cost\": 0,\n                },\n            }\n\n            # check if there is already a profile for this node id\n            profile_found = False\n            for profile in service_data[\"profiles\"]:\n                if profile[\"node_id\"] == self.node_id:\n                    profile[\"inference\"] = complete_profile_data_to_save[\"inference\"]\n                    profile_found = True\n                    break\n            if not profile_found:\n                service_data[\"profiles\"].append(complete_profile_data_to_save)\n\n            # save the updated service_data.json\n            with open(\"service_data.json\", \"w\") as f:\n                json.dump(service_data, f, indent=4)\n            print(\"\\n--------- SERVICE DATA UPDATED ---------\\n\")\n\n        else:\n            complete_xai_profile_data_to_save = {\n                \"node_id\": self.node_id,\n                \"device_type\": self.device_type,\n                \"device_name\": self.device_name,\n                \"initialization_time_ms\": self.service_initialization_duration * 1000,\n                \"eviction_time_ms\": 0,\n                \"initialization_cost\": 0,\n                \"keep_alive_cost\": 0,\n                \"energy_consumption_idle\": 0,\n                \"xai\": [\n                    {\n                        \"xai_method\": self.gradcam_method_name,\n                        \"cpu_time_ms\": self.cpu_time_total_us / 1000,\n                        \"device_time_ms\": self.device_time_total_us / 1000,\n                        \"cpu_memory_usage_MB\": self.cpu_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"self_cpu_memory_usage_MB\": self.self_cpu_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"device_memory_usage_MB\": self.device_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"self_device_memory_usage_MB\": self.self_device_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"energy_consumption_execution\": 0,\n                        \"disk_IO_MB\": 0,\n                        \"input_data_MB\": 0,\n                        \"output_data_MB\": 0,\n                        \"execution_time_ms\": (time.time() - self.start_time)\n                        / self.response_counter\n                        * 1000,\n                        \"execution_cost\": 0,\n                    }\n                ],\n            }\n\n            # check if there is already a profile for this node id\n            profile_found = False\n            for profile in service_data[\"profiles\"]:\n                if profile[\"node_id\"] == self.node_id:\n                    profile_found = True\n\n                    # check if there is already a profile for this xai method\n                    xai_method_found = False\n                    if not profile.get(\"xai\"):\n                        profile[\"xai\"] = []\n                    for xai_profile in profile[\"xai\"]:\n                        if xai_profile[\"xai_method\"] == self.gradcam_method_name:\n                            xai_profile.update(complete_xai_profile_data_to_save[\"xai\"][0]) \n                            xai_method_found = True\n                            break\n                    if not xai_method_found:\n                        profile[\"xai\"].append(complete_xai_profile_data_to_save[\"xai\"][0])\n                    break\n\n            if not profile_found:\n                service_data[\"profiles\"].append(complete_xai_profile_data_to_save)\n\n            # save the updated service_data.json\n            with open(\"service_data.json\", \"w\") as f:\n                json.dump(service_data, f, indent=4)\n            print(\"\\n--------- SERVICE DATA UPDATED ---------\\n\")\n\n\ndef option_run():\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n    data = {**data, \"ue_id\": UE_ID}\n    response, process_time, node_id, pod_name = send_post_request(\n        f\"{SERVER_URL}/model/run\", data, files\n    )\n    print(\"Process Time: \", process_time)\n    print(\"Node ID: \", node_id)\n    print(\"K8S_POD_NAME: \", pod_name)\n    print(\"Response\")\n    print(json.dumps(response, indent=4))\n\n\ndef option_profile_run():\n    data = prepare_ai_service_request_data()\n    data = {**data, \"ue_id\": UE_ID}\n    files = prepare_ai_service_request_files()\n    num_requests = int(input(\"Enter the number of requests to send: \"))\n\n    try:\n        profile_result_processor = ProfileResultProcessor(SERVER_URL)\n\n        for _ in range(num_requests):\n            profile_response, process_time, node_id, k8s_node_name = send_post_request(\n                f\"{SERVER_URL}/model/profile_run\", data, files\n            )\n            print(\"Process Time: \", process_time)\n            print(\"Node ID: \", node_id)\n            print(\"K8S_POD_NAME: \", k8s_node_name)\n            print(\"Response\")\n            print(json.dumps(profile_response, indent=4))\n            if not profile_response:\n                print(\"No profile response received.\")\n                continue\n\n            profile_result_processor.process_new_response(\n                profile_response,\n                process_time=process_time,\n                node_id=node_id,\n                k8s_pod_name=k8s_node_name,\n            )\n\n        # Print the final profile result and update the service_data.json\n        profile_result_processor.complete_profile()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\n\ndef option_help():\n    \"\"\"Get help information from the server.\"\"\"\n    try:\n        response, process_time, node_id, pod_name = send_get_request(\n            f\"{SERVER_URL}/help\"\n        )\n        print(\"Process Time: \", process_time)\n        print(\"Node ID: \", node_id)\n        print(\"K8S_POD_NAME: \", pod_name)\n        print(\"Response\")\n        print(json.dumps(response, indent=4))\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n\n\ndef option_run_with_xai():\n    \"\"\"Run AI service with XAI.\"\"\"\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n\n    print(\n        \"Note that currently only GradCAM methods on image-classification models are supported.\"\n    )\n    while True:\n        gradcam_method_name = input(\n            f\"Please select a GradCAM method (options: {XAI_GRADCAM_METHODS}): \"\n        )\n        if gradcam_method_name not in XAI_GRADCAM_METHODS:\n            print(f\"Invalid GradCAM method. Please select again.\")\n        else:\n            break\n\n    # ask for target class for explanation\n    target_category_indexes = input(\n        \"Please input target category indexes for explanation (comma-separated, e.g., 111, 32, 44, ...): \"\n    )\n    if not target_category_indexes or not target_category_indexes.strip():\n        print(\n            \"No target category indexes provided. Defaulting to explaining the top confident category.\"\n        )\n        target_category_indexes = []\n    else:\n        target_category_indexes = [\n            int(i.strip()) for i in target_category_indexes.split(\",\")\n        ]\n\n    data = {\n        **data,\n        \"ue_id\": UE_ID,\n        \"gradcam_method_name\": gradcam_method_name,\n        \"target_category_indexes\": target_category_indexes,\n    }\n    print(\"Data: \", data)\n    response, process_time, node_id, k8s_pod_name = send_post_request(\n        f\"{SERVER_URL}/xai_model/run\", data, files\n    )\n    print(\"Process Time: \", process_time)\n    print(\"Node ID: \", node_id)\n    print(\"K8S_POD_NAME: \", k8s_pod_name)\n\n    # Handle JSON response\n    model_results = response.get(\"model_results\")\n    if model_results:\n        print(\"Model Results:\", json.dumps(model_results, indent=4))\n\n    xai_results = response.get(\"xai_results\")\n    if xai_results:\n        print(\"XAI Results Method:\", xai_results.get(\"xai_method\"))\n\n    # Handle binary image response\n    encoded_image = xai_results.get(\"image\")\n    if encoded_image:\n        image_bytes = base64.b64decode(encoded_image)\n\n        # Load the image into Pillow\n        image = Image.open(BytesIO(image_bytes))\n\n        # Save image to disk\n        image.save(\"xai_output.png\")\n\n        # Display the image using matplotlib\n        plt.imshow(image)\n        plt.axis(\"off\")\n        plt.show()\n\n\ndef option_profile_run_with_xai():\n    \"\"\"Run AI service with XAI and profile the run.\"\"\"\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n    print(\n        \"Note that currently only GradCAM methods on image-classification models are supported.\"\n    )\n\n    # ask for target class for explanation\n    target_category_indexes = input(\n        \"Please input target category indexes for explanation (comma-separated, e.g., 111, 32, 44, ...): \"\n    )\n    if not target_category_indexes or not target_category_indexes.strip():\n        print(\n            \"No target category indexes provided. Defaulting to explaining the top confident category.\"\n        )\n        target_category_indexes = []\n    else:\n        target_category_indexes = [\n            int(i.strip()) for i in target_category_indexes.split(\",\")\n        ]\n\n    num_requests = int(input(\"Enter the number of requests to send: \"))\n\n    try:\n        for gradcam_method_name in XAI_GRADCAM_METHODS:\n\n            data = {\n                **data,\n                \"ue_id\": UE_ID,\n                \"gradcam_method_name\": gradcam_method_name,\n                \"target_category_indexes\": target_category_indexes,\n            }\n            print(\"Data: \", data)\n\n            profile_result_processor = ProfileResultProcessor(SERVER_URL)\n\n            for _ in range(num_requests):\n                response, process_time, node_id, k8s_pod_name = send_post_request(\n                    f\"{SERVER_URL}/xai_model/profile_run\", data, files\n                )\n                print(\"Process Time: \", process_time)\n                print(\"Node ID: \", node_id)\n                print(\"K8S_POD_NAME: \", k8s_pod_name)\n                if not response:\n                    print(\"No profile response received.\")\n                    continue\n\n                # Handle JSON response\n                model_results = response.get(\"model_results\")\n                if model_results:\n                    print(\"Model Results:\", json.dumps(model_results, indent=4))\n\n                profile_result_processor.process_new_response(\n                    response,\n                    process_time=process_time,\n                    node_id=node_id,\n                    k8s_pod_name=k8s_pod_name,\n                    gradcam_method_name=gradcam_method_name,\n                )\n\n            # Print the final profile result and update the service_data.json\n            profile_result_processor.complete_profile()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\n\nOPTIONS = [\n    {\n        \"label\": \"Get help information\",\n        \"action\": option_help,\n    },\n    {\n        \"label\": \"Run AI service\",\n        \"action\": option_run,\n    },\n    {\n        \"label\": \"Profile AI service\",\n        \"action\": option_profile_run,\n    },\n    {\n        \"label\": \"Run AI service with XAI (only image-classification models)\",\n        \"action\": option_run_with_xai,\n    },\n    {\n        \"label\": \"Profile AI service with XAI (only image-classification models)\",\n        \"action\": option_profile_run_with_xai,\n    },\n]\n\n\nif __name__ == \"__main__\":\n    while True:\n        print(\"\\nOptions:\")\n        for i, option in enumerate(OPTIONS, start=1):\n            print(f\"{i}. {option['label']}\")\n        print(\"q. Quit\")\n        choice = input(\"Enter your choice: \")\n\n        if choice == \"q\":\n            print(\"Exiting the client. Goodbye!\")\n            break\n        else:\n            try:\n                choice = int(choice)\n                if 1 <= choice <= len(OPTIONS):\n                    OPTIONS[choice - 1][\"action\"]()\n                else:\n                    print(\"Invalid choice. Please try again.\")\n            except ValueError:\n                print(\"Invalid input. Please enter a number.\")\n",
            "ai_client_utils_script_content": "def prepare_ai_service_request_files():\n    \"\"\"Prepare the `files` part for the AI service request.\"\"\"\n    files = {}\n    image_file_path = input(\"Please input the image file path: \")\n    with open(image_file_path, \"rb\") as image_file:\n        files[\"file\"] = image_file.read()\n    return files\n\n\ndef prepare_ai_service_request_data():\n    \"\"\"Prepare the `data` part other than `ue_id` for the AI service request.\"\"\"\n    data = {}\n    ue_id = input(\"Please input the unique execution ID (ue_id): \")\n    data[\"ue_id\"] = ue_id\n    return data",
            "model_script_content": "# import server utils\nfrom ai_server_utils import (\n    process_model_output_logits,\n    profile_activities,\n    prepare_profile_results,\n)\n# import profile utils\nfrom torch.profiler import profile, record_function\n\n# import necessary libs for AI model inference and request handling\nimport torch\nfrom fastapi import APIRouter, File, Form, UploadFile\nfrom fastapi.responses import JSONResponse\nfrom transformers import AutoImageProcessor, RegNetForImageClassification\nfrom PIL import Image\n\n# --------------------------------\n# Device configuration\n# --------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# --------------------------------\n# Model-specific configuration\n# make sure the variables `MODEL_NAME` and `model` are defined here.\n# --------------------------------\nMODEL_NAME = \"facebook/regnet-y-040\"\nprocessor = AutoImageProcessor.from_pretrained(MODEL_NAME)\nmodel = RegNetForImageClassification.from_pretrained(MODEL_NAME).to(device)\nmodel.eval()\n\n# Initialize the FastAPI router\nrouter = APIRouter()\n\n@router.post(\"/run\")\nasync def run_model(file: UploadFile = File(...), ue_id: str = Form(...)):\n    try:\n        # Prepare the model input\n        image = Image.open(file.file).convert(\"RGB\")\n        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n\n        # Perform inference\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        # Process the model outputs\n        predictions = process_model_output_logits(model, outputs.logits)\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"model_results\": predictions,\n            }\n        )\n    except Exception as e:\n        print(f\"Error processing file: {e}\")\n        return JSONResponse(\n            content={\"error\": \"Failed to process the image. {e}\".format(e=str(e))},\n            status_code=500,\n        )\n\n@router.post(\"/profile_run\")\nasync def profile_run(file: UploadFile = File(...), ue_id: str = Form(...)):\n    \"\"\"\n    Endpoint to profile the AI model execution.\n    \"\"\"\n    try:\n        # Prepare the model input\n        image = Image.open(file.file).convert(\"RGB\")\n        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n\n        # perform profiling\n        with profile(\n            activities=profile_activities,\n            profile_memory=True,\n        ) as prof:\n            with record_function(\"model_run\"):\n                with torch.no_grad():\n                    model_outputs = model(**inputs)\n\n        profile_result = prepare_profile_results(prof)\n\n        # Process the model outputs\n        predictions = process_model_output_logits(model, model_outputs.logits)\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"profile_result\": profile_result,\n                \"model_results\": predictions,\n            }\n        )\n\n    except Exception as e:\n        print(f\"Error processing request: {e}\")\n        return JSONResponse(\n            content={\"error\": f\"Failed to process the request. {e}\"},\n            status_code=500,\n        )\n\n# Below are the model input and output specifications to be used by the `/help` endpoint\nMODEL_INPUT_FORM_SPEC = {\n    \"file\": {\n        \"type\": \"file upload\",\n        \"description\": \"The image file to be classified.\",\n        \"required\": True,\n        \"example\": \"puppy.png\",\n    }\n}\n\nMODEL_OUTPUT_JSON_SPEC = {\n    \"ue_id\": \"unique execution ID\",\n    \"model_results\": [\n        {\n            \"category_id\": \"category id\",\n            \"label\": \"category label\",\n            \"probability\": \"probability value\",\n        }\n    ],\n}",
            "healthcheck_script_content": "import requests\n\nprint(requests.get(\"http://localhost:8000/help\"))"
        }
    },
    {
        "model_name": "apple/mobilevit-small",
        "model_url": "https://huggingface.co/apple/mobilevit-small",
        "task": "image-classification",
        "task_detail": "The MobileViT-small model represents a state-of-the-art approach in the field of image classification, specifically designed for efficiency and adaptability on lightweight and mobile platforms. Engineered using a fusion of MobileNetV2-style convolutional layers and Vision Transformers, the model effectively processes image data by converting it into flattened patches for global analysis and subsequently reforming them into feature maps.\n\nThis model's functionality extends to classifying images into one of the 1,000 distinct classes of the ImageNet-1k dataset. These classes encompass a wide array of categories, including animals, objects, scenes, and more, thus enabling the model to be versatile across different image types. The input expected by the model consists of image files resized to a resolution of 288x288 pixels and center-cropped to 256x256, with pixel values normalized within the range of [0, 1]. The model specifically requires images in BGR pixel order, aligning with common practices in computer vision preprocessing.\n\nMobileViT-small outputs predictions in the form of logits, which are unnormalized scores reflecting the likelihood of each class. The class with the highest score, indicated by the position of the maximum logit, represents the model's classification decision. With a top-1 accuracy of 78.4% and a top-5 accuracy of 94.1% on the ImageNet dataset, the model delivers highly accurate predictions.\n\nFurthermore, the model supports PyTorch, ensuring seamless integration into existing data pipelines for users employing this popular machine learning library. Its design, combining the strengths of convolutional neural networks for effective local feature extraction and transformers for global context understanding, provides an advanced solution for mobile-friendly, real-time image classification tasks. This makes MobileViT-small particularly suitable for applications requiring efficient deployment and computation, such as smartphone apps, IoT devices, and other embedded systems.",
        "accuracy_info": "The MobileViT-S model, which is a small-sized version of the MobileViT family, achieved a top-1 accuracy of 78.4% and a top-5 accuracy of 94.1% on the ImageNet dataset. Additionally, the model contains 5.6 million parameters.",
        "image_repository_url": "",
        "service_disk_size_bytes": 3609155433.0,
        "profiles": [
            {
                "node_id": "ugurcan.celik",
                "device_type": "DeviceType.CPU",
                "device_name": "cuda",
                "initialization_time_ms": 3858.9730262756348,
                "eviction_time_ms": 0.0,
                "initialization_cost": 0.0,
                "keep_alive_cost": 0.0,
                "energy_consumption_idle": 0.0,
                "idle_container_cpu_memory_usage": "1.5GB",
                "idle_container_device_memory_usage": "1.2GB",
                "inference": {
                    "cpu_time_ms": 299.346874,
                    "device_time_ms": 3.0291639999987092,
                    "cpu_memory_usage_MB": 0.0,
                    "self_cpu_memory_usage_MB": 0.0,
                    "device_memory_usage_MB": 8.12890625,
                    "self_device_memory_usage_MB": -209.60498046875,
                    "energy_consumption_execution": 0.0,
                    "disk_IO_MB": 0.0,
                    "input_data_MB": 0.0,
                    "output_data_MB": 0.0,
                    "execution_time_ms": 431.34323755900067,
                    "execution_cost": 0.0
                },
                "xai": [
                    {
                        "xai_method": "GradCAM",
                        "cpu_time_ms": 120.90178700000001,
                        "device_time_ms": 1.70979699999998,
                        "cpu_memory_usage_MB": 0.07819366455078125,
                        "self_cpu_memory_usage_MB": 0.03531646728515625,
                        "device_memory_usage_MB": 211.4775390625,
                        "self_device_memory_usage_MB": -35.5205078125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 896.0187435150146,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "HiResCAM",
                        "cpu_time_ms": 44.496955,
                        "device_time_ms": 1.6905640000000028,
                        "cpu_memory_usage_MB": 0.07819366455078125,
                        "self_cpu_memory_usage_MB": 0.03531646728515625,
                        "device_memory_usage_MB": 182.48388671875,
                        "self_device_memory_usage_MB": -35.5205078125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 792.5668557484945,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "XGradCAM",
                        "cpu_time_ms": 45.023877,
                        "device_time_ms": 1.6909819999999995,
                        "cpu_memory_usage_MB": 0.07819366455078125,
                        "self_cpu_memory_usage_MB": 0.03531646728515625,
                        "device_memory_usage_MB": 182.48388671875,
                        "self_device_memory_usage_MB": -35.5205078125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 876.8561681111654,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "GradCAMPlusPlus",
                        "cpu_time_ms": 39.875871999999994,
                        "device_time_ms": 1.6877189999999984,
                        "cpu_memory_usage_MB": 0.07819366455078125,
                        "self_cpu_memory_usage_MB": 0.03531646728515625,
                        "device_memory_usage_MB": 182.48388671875,
                        "self_device_memory_usage_MB": -35.5205078125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 802.745501200358,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "ScoreCAM",
                        "cpu_time_ms": 291.30442100000005,
                        "device_time_ms": 243.9046550000006,
                        "cpu_memory_usage_MB": 6.289131164550781,
                        "self_cpu_memory_usage_MB": -0.00557708740234375,
                        "device_memory_usage_MB": 182.48388671875,
                        "self_device_memory_usage_MB": -33897.48046875,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 3311.6561571756997,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "LayerCAM",
                        "cpu_time_ms": 40.04684499999999,
                        "device_time_ms": 1.733407999999994,
                        "cpu_memory_usage_MB": 0.07819366455078125,
                        "self_cpu_memory_usage_MB": 0.03531646728515625,
                        "device_memory_usage_MB": 182.48388671875,
                        "self_device_memory_usage_MB": -35.5205078125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 885.6911659240723,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenCAM",
                        "cpu_time_ms": 19.232039999999998,
                        "device_time_ms": 1.7333490000000056,
                        "cpu_memory_usage_MB": 0.03913116455078125,
                        "self_cpu_memory_usage_MB": -0.00374603271484375,
                        "device_memory_usage_MB": 182.48388671875,
                        "self_device_memory_usage_MB": -35.51953125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 332.17279116312665,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenGradCAM",
                        "cpu_time_ms": 122.06884600000001,
                        "device_time_ms": 1.7354699999999914,
                        "cpu_memory_usage_MB": 0.07819366455078125,
                        "self_cpu_memory_usage_MB": 0.03531646728515625,
                        "device_memory_usage_MB": 182.48388671875,
                        "self_device_memory_usage_MB": -35.5205078125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 873.1032212575277,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "KPCA_CAM",
                        "cpu_time_ms": 20.014594999999996,
                        "device_time_ms": 1.7322949999999968,
                        "cpu_memory_usage_MB": 0.03913116455078125,
                        "self_cpu_memory_usage_MB": -0.00374603271484375,
                        "device_memory_usage_MB": 182.48388671875,
                        "self_device_memory_usage_MB": -35.51953125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 312.1198018391927,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "RandomCAM",
                        "cpu_time_ms": 39.869029,
                        "device_time_ms": 1.7334139999999816,
                        "cpu_memory_usage_MB": 0.07819366455078125,
                        "self_cpu_memory_usage_MB": 0.03531646728515625,
                        "device_memory_usage_MB": 182.48388671875,
                        "self_device_memory_usage_MB": -35.5205078125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 808.178186416626,
                        "execution_cost": 0.0
                    }
                ]
            },
            {
                "node_id": "LAP004262",
                "device_type": "DeviceType.CPU",
                "device_name": "None",
                "initialization_time_ms": 10368.60990524292,
                "eviction_time_ms": 0.0,
                "initialization_cost": 0.0,
                "keep_alive_cost": 0.0,
                "energy_consumption_idle": 0.0,
                "idle_container_cpu_memory_usage": "1.4GB",
                "idle_container_device_memory_usage": "0GB",
                "inference": {
                    "cpu_time_ms": 122.16381600000001,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 0.003814697265625,
                    "self_cpu_memory_usage_MB": -208.925537109375,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0.0,
                    "disk_IO_MB": 0.0,
                    "input_data_MB": 0.0,
                    "output_data_MB": 0.0,
                    "execution_time_ms": 273.1734911600749,
                    "execution_cost": 0.0
                },
                "xai": [
                    {
                        "xai_method": "GradCAM",
                        "cpu_time_ms": 232.49418599999998,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 201.69092559814453,
                        "self_cpu_memory_usage_MB": -56.800262451171875,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 651.5867710113525,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "HiResCAM",
                        "cpu_time_ms": 212.59690799999998,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 180.4101333618164,
                        "self_cpu_memory_usage_MB": -56.917449951171875,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 782.6214631398519,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "XGradCAM",
                        "cpu_time_ms": 445.327102,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 180.4101333618164,
                        "self_cpu_memory_usage_MB": -56.858856201171875,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 791.3525104522705,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "GradCAMPlusPlus",
                        "cpu_time_ms": 229.88156899999998,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 180.4101333618164,
                        "self_cpu_memory_usage_MB": -56.800262451171875,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 886.4126205444336,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "LayerCAM",
                        "cpu_time_ms": 184.015051,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 180.4101333618164,
                        "self_cpu_memory_usage_MB": -56.987762451171875,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 726.715882619222,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenCAM",
                        "cpu_time_ms": 140.38075899999998,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 180.3710708618164,
                        "self_cpu_memory_usage_MB": -35.51946258544922,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 363.9918963114421,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenGradCAM",
                        "cpu_time_ms": 209.74815300000003,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 180.4101333618164,
                        "self_cpu_memory_usage_MB": -56.987762451171875,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 704.825242360433,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "KPCA_CAM",
                        "cpu_time_ms": 98.573284,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 180.3710708618164,
                        "self_cpu_memory_usage_MB": -35.51946258544922,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 319.38735644022626,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "RandomCAM",
                        "cpu_time_ms": 182.13635499999998,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 180.4101333618164,
                        "self_cpu_memory_usage_MB": -56.858856201171875,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 745.1318105061849,
                        "execution_cost": 0.0
                    }
                ]
            }
        ],
        "feedback": {
            "likes": [],
            "dislikes": [],
            "comments": []
        },
        "code": {
            "readme_content": "---\nlicense: other\ntags:\n- vision\n- image-classification\ndatasets:\n- imagenet-1k\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\n  example_title: Tiger\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg\n  example_title: Teapot\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg\n  example_title: Palace\n---\n\n# MobileViT (small-sized model)\n\nMobileViT model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178) by Sachin Mehta and Mohammad Rastegari, and first released in [this repository](https://github.com/apple/ml-cvnets). The license used is [Apple sample code license](https://github.com/apple/ml-cvnets/blob/main/LICENSE).\n\nDisclaimer: The team releasing MobileViT did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nMobileViT is a light-weight, low latency convolutional neural network that combines MobileNetV2-style layers with a new block that replaces local processing in convolutions with global processing using transformers. As with ViT (Vision Transformer), the image data is converted into flattened patches before it is processed by the transformer layers. Afterwards, the patches are \"unflattened\" back into feature maps. This allows the MobileViT-block to be placed anywhere inside a CNN. MobileViT does not require any positional embeddings.\n\n## Intended uses & limitations\n\nYou can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=mobilevit) to look for fine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\n\n```python\nfrom transformers import MobileViTFeatureExtractor, MobileViTForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = MobileViTFeatureExtractor.from_pretrained(\"apple/mobilevit-small\")\nmodel = MobileViTForImageClassification.from_pretrained(\"apple/mobilevit-small\")\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\n\noutputs = model(**inputs)\nlogits = outputs.logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n```\n\nCurrently, both the feature extractor and model support PyTorch.\n\n## Training data\n\nThe MobileViT model was pretrained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k), a dataset consisting of 1 million images and 1,000 classes. \n\n## Training procedure\n\n### Preprocessing\n\nTraining requires only basic data augmentation, i.e. random resized cropping and horizontal flipping. \n\nTo learn multi-scale representations without requiring fine-tuning, a multi-scale sampler was used during training, with image sizes randomly sampled from: (160, 160), (192, 192), (256, 256), (288, 288), (320, 320).\n\nAt inference time, images are resized/rescaled to the same resolution (288x288), and center-cropped at 256x256.\n\nPixels are normalized to the range [0, 1]. Images are expected to be in BGR pixel order, not RGB.\n\n### Pretraining\n\nThe MobileViT networks are trained from scratch for 300 epochs on ImageNet-1k on 8 NVIDIA GPUs with an effective batch size of 1024 and learning rate warmup for 3k steps, followed by cosine annealing. Also used were label smoothing cross-entropy loss and L2 weight decay. Training resolution varies from 160x160 to 320x320, using multi-scale sampling.\n\n## Evaluation results\n\n| Model            | ImageNet top-1 accuracy | ImageNet top-5 accuracy | # params  | URL                                             |\n|------------------|-------------------------|-------------------------|-----------|-------------------------------------------------|\n| MobileViT-XXS    | 69.0                    | 88.9                    | 1.3 M     | https://huggingface.co/apple/mobilevit-xx-small |\n| MobileViT-XS     | 74.8                    | 92.3                    | 2.3 M     | https://huggingface.co/apple/mobilevit-x-small  |\n| **MobileViT-S**  | **78.4**                | **94.1**                | **5.6 M** | https://huggingface.co/apple/mobilevit-small    |\n\n### BibTeX entry and citation info\n\n```bibtex\n@inproceedings{vision-transformer,\ntitle = {MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer},\nauthor = {Sachin Mehta and Mohammad Rastegari},\nyear = {2022},\nURL = {https://arxiv.org/abs/2110.02178}\n}\n```\n",
            "dockerfile_content": "# Base image for AI service powered by HuggingFace pre-trained AI models.\n# the image has the following packages/libraries installed already:\n# - python3.12, pip, git, fastapi, uvicorn, torch, torchvision, opencv-python, transformers, python-multipart, Pillow, requests\nFROM python3.12_ai_service_base:latest\n\n# Set working directory\nWORKDIR /app\n\n# Copy application code\nCOPY . .\n\n# Install additional dependencies\nRUN pip install --no-cache-dir transformers\n\n# Expose port 8000\nEXPOSE 8000\n\n# Start the FastAPI server\nCMD [\"uvicorn\", \"ai_server:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--timeout-keep-alive\", \"600\"]",
            "ai_server_script_content": "import json\nimport os\nimport time\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import JSONResponse\nfrom contextlib import asynccontextmanager\nfrom ai_server_utils import (\n    PROFILE_OUTPUT_JSON_SPEC,\n    NODE_ID,\n    K8S_POD_NAME,\n)\n\n\n# -------------------------------------------\n# App Lifespan setup\n# -------------------------------------------\n# Record the script start time (when uvicorn starts the process)\nSCRIPT_START_TIME = time.time()\nINITIALIZATION_DURATION = 0.0\nservice_endpoint_specs = {\n    \"model_input_form_spec\": None,\n    \"model_output_json_spec\": None,\n    \"profile_output_json_spec\": None,\n    \"xai_model_input_form_spec\": None,\n    \"xai_model_output_json_spec\": None,\n    \"xai_profile_output_json_spec\": None,\n}\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n\n    global INITIALIZATION_DURATION\n    global SCRIPT_START_TIME\n    global service_endpoint_specs\n\n    # Load the AI model\n    print(\"Loading AI model...\")\n    from model import (\n        MODEL_INPUT_FORM_SPEC,\n        MODEL_OUTPUT_JSON_SPEC,\n        router as model_router,\n    )\n\n    service_endpoint_specs[\"model_input_form_spec\"] = MODEL_INPUT_FORM_SPEC\n    service_endpoint_specs[\"model_output_json_spec\"] = MODEL_OUTPUT_JSON_SPEC\n    service_endpoint_specs[\"profile_output_json_spec\"] = PROFILE_OUTPUT_JSON_SPEC\n\n    app.include_router(model_router, prefix=\"/model\", tags=[\"AI Model\"])\n\n    # Load the XAI model\n    if os.path.exists(os.path.dirname(__file__) + \"/xai_model.py\"):\n        print(\"Loading XAI model...\")\n        from xai_model import (\n            XAI_OUTPUT_JSON_SPEC,\n            router as xai_model_router,\n        )\n\n        # by default, the xai_model input form spec is the same as the model input form spec\n        service_endpoint_specs[\"xai_model_input_form_spec\"] = MODEL_INPUT_FORM_SPEC\n        service_endpoint_specs[\"xai_model_output_json_spec\"] = MODEL_OUTPUT_JSON_SPEC\n        service_endpoint_specs[\"xai_model_output_json_spec\"].update(\n            XAI_OUTPUT_JSON_SPEC\n        )\n        service_endpoint_specs[\"xai_profile_output_json_spec\"] = (\n            PROFILE_OUTPUT_JSON_SPEC\n        )\n        service_endpoint_specs[\"xai_profile_output_json_spec\"].update(\n            XAI_OUTPUT_JSON_SPEC\n        )\n\n        app.include_router(xai_model_router, prefix=\"/xai_model\", tags=[\"XAI Model\"])\n\n    # Record the initialization duration\n    INITIALIZATION_DURATION = time.time() - SCRIPT_START_TIME\n\n    print(f\"AI service loaded in {INITIALIZATION_DURATION:.2f} seconds.\")\n\n    yield\n\n    # Clean up the models and release the resources\n    service_endpoint_specs.clear()\n\n\n# -------------------------------------------\n# FastAPI application setup\n# -------------------------------------------\napp = FastAPI(lifespan=lifespan)\n\n\n# -------------------------------------------\n# Middlewares\n# -------------------------------------------\n@app.middleware(\"http\")\nasync def prepare_header_middleware(request: Request, call_next):\n    start_time = time.perf_counter()\n    response = await call_next(request)\n    process_time = time.perf_counter() - start_time\n    response.headers[\"X-Process-Time\"] = str(process_time)\n    response.headers[\"X-NODE-ID\"] = NODE_ID\n    response.headers[\"X-K8S-POD-NAME\"] = K8S_POD_NAME\n    return response\n\n\n# -------------------------------------------\n# General Endpoints\n# -------------------------------------------\n@app.get(\"/initialization_duration\")\ndef get_initialization_duration():\n    \"\"\"\n    Endpoint to retrieve the initialization duration of the AI model.\n    \"\"\"\n    global INITIALIZATION_DURATION\n\n    if INITIALIZATION_DURATION == 0.0:\n        return JSONResponse(\n            content={\"error\": \"Model not initialized.\"},\n            status_code=500,\n        )\n    return JSONResponse(\n        content={\n            \"initialization_duration\": INITIALIZATION_DURATION,\n            \"script_start_time\": SCRIPT_START_TIME,\n        }\n    )\n\n\n@app.get(\"/help\")\ndef get_help():\n    global service_endpoint_specs\n    help_info = {\n        \"endpoints\": {\n            \"/model/run\": {\n                \"method\": \"POST\",\n                \"description\": \"Executes the AI model with the provided input data.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_output_json_spec\"],\n                },\n            },\n            \"/model/profile_run\": {\n                \"method\": \"POST\",\n                \"description\": \"Profiles the AI model execution.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    \"profile_result\": \"Profiling results of the AI model execution.\",\n                    **service_endpoint_specs[\"profile_output_json_spec\"],\n                },\n            },\n            \"/initialization_duration\": {\n                \"method\": \"GET\",\n                \"description\": \"Retrieves the initialization duration of the AI model.\",\n                \"response\": {\n                    \"initialization_duration\": \"Time taken to initialize the model (in seconds).\",\n                    \"script_start_time\": \"Timestamp when the script started (in seconds since epoch).\",\n                },\n            },\n        }\n    }\n\n    if service_endpoint_specs[\"xai_model_input_form_spec\"] is not None:\n        help_info[\"endpoints\"][\"/xai_model/run\"] = {\n            \"method\": \"POST\",\n            \"description\": \"Executes the XAI model with the provided input data.\",\n            \"parameters\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_input_form_spec\"],\n            },\n            \"response\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_output_json_spec\"],\n            },\n        }\n\n        help_info[\"endpoints\"][\"/xai_model/profile_run\"] = {\n            \"method\": \"POST\",\n            \"description\": \"Profiles the XAI model execution.\",\n            \"parameters\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_input_form_spec\"],\n            },\n            \"response\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                \"profile_result\": \"Profiling results of the XAI model execution.\",\n                **service_endpoint_specs[\"xai_profile_output_json_spec\"],\n            },\n        }\n    \n    return JSONResponse(content=help_info)\n",
            "ai_client_script_content": "import base64\nimport json\nimport time\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nfrom ai_client_utils import (\n    prepare_ai_service_request_files,\n    prepare_ai_service_request_data,\n)\n\nXAI_GRADCAM_METHODS = [\n    \"GradCAM\",\n    \"HiResCAM\",\n    # \"AblationCAM\",\n    \"XGradCAM\",\n    \"GradCAMPlusPlus\",\n    \"ScoreCAM\",\n    \"LayerCAM\",\n    \"EigenCAM\",\n    \"EigenGradCAM\",\n    \"KPCA_CAM\",\n    \"RandomCAM\",\n]\n\n# -------------------------------------\n# prompt for necessary inputs\n# -------------------------------------\nSERVER_URL = input(\"Please input server URL (default to http://localhost:9000): \")\nif SERVER_URL.strip() == \"\":\n    SERVER_URL = \"http://localhost:9000\"\nUE_ID = input(\"Please input UE_ID (default to 123456): \")\nif UE_ID.strip() == \"\":\n    UE_ID = \"123456\"\n\n\ndef send_post_request(url, data, files):\n    \"\"\"Send request to run AI service and display AI service responses.\"\"\"\n    try:\n        response = requests.post(url, files=files, data=data)\n        # get the process time, node id and k8s pod name from the response headers\n        process_time = response.headers.get(\"X-Process-Time\")\n        node_id = response.headers.get(\"X-NODE-ID\")\n        k8s_pod_name = response.headers.get(\"X-K8S-POD-NAME\")\n        if response.status_code == 200:\n            return response.json(), process_time, node_id, k8s_pod_name\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n\ndef send_get_request(url, params=None):\n    \"\"\"Send GET request to the specified URL and return the response.\"\"\"\n    try:\n        response = requests.get(url, params=params)\n        process_time = response.headers.get(\"X-Process-Time\")\n        node_id = response.headers.get(\"X-NODE-ID\")\n        k8s_pod_name = response.headers.get(\"X-K8S-POD-NAME\")\n        if response.status_code == 200:\n            return response.json(), process_time, node_id, k8s_pod_name\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n            return None\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n\nclass ProfileResultProcessor:\n\n    def __init__(self, server_url):\n        self.server_url = server_url\n        self.start_time = time.time()\n        self.service_initialization_duration = 0\n        self.response_counter = 0\n        self.profile_name = None\n        self.device_type = None\n        self.device_name = None\n        self.node_id = None\n        self.k8s_pod_name = None\n        self.cpu_memory_usage_bytes = 0\n        self.self_cpu_memory_usage_bytes = 0\n        self.device_memory_usage_bytes = 0\n        self.self_device_memory_usage_bytes = 0\n        self.cpu_time_total_us = 0\n        self.self_cpu_time_total_us = 0\n        self.device_time_total_us = 0\n        self.self_device_time_total_us = 0\n\n        # xai related\n        self.gradcam_method_name = None\n\n        self.fetch_service_initialization_duration()\n\n    def fetch_service_initialization_duration(self):\n        \"\"\"Fetch the service initialization duration from the server.\"\"\"\n        response, process_time, node_id, k8s_pod_name = send_get_request(\n            f\"{self.server_url}/initialization_duration\"\n        )\n        if response:\n            self.service_initialization_duration = response.get(\n                \"initialization_duration\", 0\n            )\n        else:\n            print(\"Failed to fetch initialization duration.\")\n            self.service_initialization_duration = 0\n\n    def process_new_response(\n        self,\n        profile_response,\n        process_time=None,\n        node_id=None,\n        k8s_pod_name=None,\n        gradcam_method_name=None,\n    ):\n        if not profile_response:\n            return\n\n        profile_result = profile_response[\"profile_result\"]\n\n        if not profile_result:\n            return\n\n        if self.profile_name is None:\n            self.profile_name = profile_result[\"name\"]\n        if self.device_type is None:\n            self.device_type = profile_result[\"device_type\"]\n        if self.device_name is None:\n            self.device_name = profile_result[\"device_name\"]\n        if self.node_id is None:\n            self.node_id = node_id\n        if self.k8s_pod_name is None:\n            self.k8s_pod_name = k8s_pod_name\n\n        if self.gradcam_method_name is None:\n            self.gradcam_method_name = gradcam_method_name\n\n        # update the max profile result\n        self.cpu_memory_usage_bytes = max(\n            self.cpu_memory_usage_bytes, profile_result[\"cpu_memory_usage\"]\n        )\n        # self cpu memory usage could be negative. here we take the value that has the max absolute value\n        if abs(profile_result[\"self_cpu_memory_usage\"]) > abs( self.self_cpu_memory_usage_bytes):\n            self.self_cpu_memory_usage_bytes = profile_result[\"self_cpu_memory_usage\"]\n        self.device_memory_usage_bytes = max(\n            self.device_memory_usage_bytes, profile_result[\"device_memory_usage\"]\n        )\n        # same as self cpu memory usage\n        if abs(profile_result[\"self_device_memory_usage\"]) > abs(self.self_device_memory_usage_bytes):\n            self.self_device_memory_usage_bytes = profile_result[\"self_device_memory_usage\"]\n        self.cpu_time_total_us = max(\n            self.cpu_time_total_us, profile_result[\"cpu_time_total\"]\n        )\n        self.self_cpu_time_total_us = max(\n            self.self_cpu_time_total_us, profile_result[\"self_cpu_time_total\"]\n        )\n        self.device_time_total_us = max(\n            self.device_time_total_us, profile_result[\"device_time_total\"]\n        )\n        self.self_device_time_total_us = max(\n            self.self_device_time_total_us, profile_result[\"self_device_time_total\"]\n        )\n\n        self.response_counter += 1\n\n    def complete_profile(self):\n        print(\"\\n--------- PROFILE EVENT ---------\\n\")\n        print(f\"Name: {self.profile_name}\")\n        print(f\"Device Type: {self.device_type}\")\n        print(f\"Device Name: {self.device_name}\")\n        print(f\"Node ID: {self.node_id}\")\n        print(f\"K8S_POD_NAME: {self.k8s_pod_name}\")\n\n        if self.gradcam_method_name:\n            print(f\"GradCAM Method Name: {self.gradcam_method_name}\")\n\n        print(\"\\n--------- LATENCY RESULT ---------\\n\")\n        print(\"Service Initialization Duration: \", self.service_initialization_duration)\n        print(\"Total Requests: \", self.response_counter)\n        print(f\"Total Time Taken: {time.time() - self.start_time:.2f} seconds\")\n        print(\n            f\"Average Time Taken: {(time.time() - self.start_time) / self.response_counter:.2f} seconds\"\n        )\n\n        print(\"\\n--------- RESOURCE USAGE ---------\\n\")\n        print(f\"CPU Memory Usage: {self.cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\")\n        print(\n            f\"Self CPU Memory Usage: {self.self_cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Device Memory Usage: {self.device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Self Device Memory Usage: {self.self_device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(f\"CPU Time Total: {self.cpu_time_total_us / 1000:.2f} ms\")\n        print(f\"Self CPU Time Total: {self.self_cpu_time_total_us / 1000:.2f} ms\")\n        print(f\"Device Time Total: {self.device_time_total_us / 1000:.2f} ms\")\n        print(f\"Self Device Time Total: {self.self_device_time_total_us / 1000:.2f} ms\")\n\n        # update the service_data.json automatically\n        with open(\"service_data.json\", \"r\") as f:\n            service_data = json.load(f)\n\n        if not self.gradcam_method_name:\n            complete_profile_data_to_save = {\n                \"node_id\": self.node_id,\n                \"device_type\": self.device_type,\n                \"device_name\": self.device_name,\n                \"initialization_time_ms\": self.service_initialization_duration * 1000,\n                \"eviction_time_ms\": 0,\n                \"initialization_cost\": 0,\n                \"keep_alive_cost\": 0,\n                \"energy_consumption_idle\": 0,\n                \"inference\": {\n                    \"cpu_time_ms\": self.cpu_time_total_us / 1000,\n                    \"device_time_ms\": self.device_time_total_us / 1000,\n                    \"cpu_memory_usage_MB\": self.cpu_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"self_cpu_memory_usage_MB\": self.self_cpu_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"device_memory_usage_MB\": self.device_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"self_device_memory_usage_MB\": self.self_device_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"energy_consumption_execution\": 0,\n                    \"disk_IO_MB\": 0,\n                    \"input_data_MB\": 0,\n                    \"output_data_MB\": 0,\n                    \"execution_time_ms\": (time.time() - self.start_time)\n                    / self.response_counter\n                    * 1000,\n                    \"execution_cost\": 0,\n                },\n            }\n\n            # check if there is already a profile for this node id\n            profile_found = False\n            for profile in service_data[\"profiles\"]:\n                if profile[\"node_id\"] == self.node_id:\n                    profile[\"inference\"] = complete_profile_data_to_save[\"inference\"]\n                    profile_found = True\n                    break\n            if not profile_found:\n                service_data[\"profiles\"].append(complete_profile_data_to_save)\n\n            # save the updated service_data.json\n            with open(\"service_data.json\", \"w\") as f:\n                json.dump(service_data, f, indent=4)\n            print(\"\\n--------- SERVICE DATA UPDATED ---------\\n\")\n\n        else:\n            complete_xai_profile_data_to_save = {\n                \"node_id\": self.node_id,\n                \"device_type\": self.device_type,\n                \"device_name\": self.device_name,\n                \"initialization_time_ms\": self.service_initialization_duration * 1000,\n                \"eviction_time_ms\": 0,\n                \"initialization_cost\": 0,\n                \"keep_alive_cost\": 0,\n                \"energy_consumption_idle\": 0,\n                \"xai\": [\n                    {\n                        \"xai_method\": self.gradcam_method_name,\n                        \"cpu_time_ms\": self.cpu_time_total_us / 1000,\n                        \"device_time_ms\": self.device_time_total_us / 1000,\n                        \"cpu_memory_usage_MB\": self.cpu_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"self_cpu_memory_usage_MB\": self.self_cpu_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"device_memory_usage_MB\": self.device_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"self_device_memory_usage_MB\": self.self_device_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"energy_consumption_execution\": 0,\n                        \"disk_IO_MB\": 0,\n                        \"input_data_MB\": 0,\n                        \"output_data_MB\": 0,\n                        \"execution_time_ms\": (time.time() - self.start_time)\n                        / self.response_counter\n                        * 1000,\n                        \"execution_cost\": 0,\n                    }\n                ],\n            }\n\n            # check if there is already a profile for this node id\n            profile_found = False\n            for profile in service_data[\"profiles\"]:\n                if profile[\"node_id\"] == self.node_id:\n                    profile_found = True\n\n                    # check if there is already a profile for this xai method\n                    xai_method_found = False\n                    if not profile.get(\"xai\"):\n                        profile[\"xai\"] = []\n                    for xai_profile in profile[\"xai\"]:\n                        if xai_profile[\"xai_method\"] == self.gradcam_method_name:\n                            xai_profile.update(complete_xai_profile_data_to_save[\"xai\"][0]) \n                            xai_method_found = True\n                            break\n                    if not xai_method_found:\n                        profile[\"xai\"].append(complete_xai_profile_data_to_save[\"xai\"][0])\n                    break\n\n            if not profile_found:\n                service_data[\"profiles\"].append(complete_xai_profile_data_to_save)\n\n            # save the updated service_data.json\n            with open(\"service_data.json\", \"w\") as f:\n                json.dump(service_data, f, indent=4)\n            print(\"\\n--------- SERVICE DATA UPDATED ---------\\n\")\n\n\ndef option_run():\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n    data = {**data, \"ue_id\": UE_ID}\n    response, process_time, node_id, pod_name = send_post_request(\n        f\"{SERVER_URL}/model/run\", data, files\n    )\n    print(\"Process Time: \", process_time)\n    print(\"Node ID: \", node_id)\n    print(\"K8S_POD_NAME: \", pod_name)\n    print(\"Response\")\n    print(json.dumps(response, indent=4))\n\n\ndef option_profile_run():\n    data = prepare_ai_service_request_data()\n    data = {**data, \"ue_id\": UE_ID}\n    files = prepare_ai_service_request_files()\n    num_requests = int(input(\"Enter the number of requests to send: \"))\n\n    try:\n        profile_result_processor = ProfileResultProcessor(SERVER_URL)\n\n        for _ in range(num_requests):\n            profile_response, process_time, node_id, k8s_node_name = send_post_request(\n                f\"{SERVER_URL}/model/profile_run\", data, files\n            )\n            print(\"Process Time: \", process_time)\n            print(\"Node ID: \", node_id)\n            print(\"K8S_POD_NAME: \", k8s_node_name)\n            print(\"Response\")\n            print(json.dumps(profile_response, indent=4))\n            if not profile_response:\n                print(\"No profile response received.\")\n                continue\n\n            profile_result_processor.process_new_response(\n                profile_response,\n                process_time=process_time,\n                node_id=node_id,\n                k8s_pod_name=k8s_node_name,\n            )\n\n        # Print the final profile result and update the service_data.json\n        profile_result_processor.complete_profile()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\n\ndef option_help():\n    \"\"\"Get help information from the server.\"\"\"\n    try:\n        response, process_time, node_id, pod_name = send_get_request(\n            f\"{SERVER_URL}/help\"\n        )\n        print(\"Process Time: \", process_time)\n        print(\"Node ID: \", node_id)\n        print(\"K8S_POD_NAME: \", pod_name)\n        print(\"Response\")\n        print(json.dumps(response, indent=4))\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n\n\ndef option_run_with_xai():\n    \"\"\"Run AI service with XAI.\"\"\"\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n\n    print(\n        \"Note that currently only GradCAM methods on image-classification models are supported.\"\n    )\n    while True:\n        gradcam_method_name = input(\n            f\"Please select a GradCAM method (options: {XAI_GRADCAM_METHODS}): \"\n        )\n        if gradcam_method_name not in XAI_GRADCAM_METHODS:\n            print(f\"Invalid GradCAM method. Please select again.\")\n        else:\n            break\n\n    # ask for target class for explanation\n    target_category_indexes = input(\n        \"Please input target category indexes for explanation (comma-separated, e.g., 111, 32, 44, ...): \"\n    )\n    if not target_category_indexes or not target_category_indexes.strip():\n        print(\n            \"No target category indexes provided. Defaulting to explaining the top confident category.\"\n        )\n        target_category_indexes = []\n    else:\n        target_category_indexes = [\n            int(i.strip()) for i in target_category_indexes.split(\",\")\n        ]\n\n    data = {\n        **data,\n        \"ue_id\": UE_ID,\n        \"gradcam_method_name\": gradcam_method_name,\n        \"target_category_indexes\": target_category_indexes,\n    }\n    print(\"Data: \", data)\n    response, process_time, node_id, k8s_pod_name = send_post_request(\n        f\"{SERVER_URL}/xai_model/run\", data, files\n    )\n    print(\"Process Time: \", process_time)\n    print(\"Node ID: \", node_id)\n    print(\"K8S_POD_NAME: \", k8s_pod_name)\n\n    # Handle JSON response\n    model_results = response.get(\"model_results\")\n    if model_results:\n        print(\"Model Results:\", json.dumps(model_results, indent=4))\n\n    xai_results = response.get(\"xai_results\")\n    if xai_results:\n        print(\"XAI Results Method:\", xai_results.get(\"xai_method\"))\n\n    # Handle binary image response\n    encoded_image = xai_results.get(\"image\")\n    if encoded_image:\n        image_bytes = base64.b64decode(encoded_image)\n\n        # Load the image into Pillow\n        image = Image.open(BytesIO(image_bytes))\n\n        # Save image to disk\n        image.save(\"xai_output.png\")\n\n        # Display the image using matplotlib\n        plt.imshow(image)\n        plt.axis(\"off\")\n        plt.show()\n\n\ndef option_profile_run_with_xai():\n    \"\"\"Run AI service with XAI and profile the run.\"\"\"\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n    print(\n        \"Note that currently only GradCAM methods on image-classification models are supported.\"\n    )\n\n    # ask for target class for explanation\n    target_category_indexes = input(\n        \"Please input target category indexes for explanation (comma-separated, e.g., 111, 32, 44, ...): \"\n    )\n    if not target_category_indexes or not target_category_indexes.strip():\n        print(\n            \"No target category indexes provided. Defaulting to explaining the top confident category.\"\n        )\n        target_category_indexes = []\n    else:\n        target_category_indexes = [\n            int(i.strip()) for i in target_category_indexes.split(\",\")\n        ]\n\n    num_requests = int(input(\"Enter the number of requests to send: \"))\n\n    try:\n        for gradcam_method_name in XAI_GRADCAM_METHODS:\n\n            data = {\n                **data,\n                \"ue_id\": UE_ID,\n                \"gradcam_method_name\": gradcam_method_name,\n                \"target_category_indexes\": target_category_indexes,\n            }\n            print(\"Data: \", data)\n\n            profile_result_processor = ProfileResultProcessor(SERVER_URL)\n\n            for _ in range(num_requests):\n                response, process_time, node_id, k8s_pod_name = send_post_request(\n                    f\"{SERVER_URL}/xai_model/profile_run\", data, files\n                )\n                print(\"Process Time: \", process_time)\n                print(\"Node ID: \", node_id)\n                print(\"K8S_POD_NAME: \", k8s_pod_name)\n                if not response:\n                    print(\"No profile response received.\")\n                    continue\n\n                # Handle JSON response\n                model_results = response.get(\"model_results\")\n                if model_results:\n                    print(\"Model Results:\", json.dumps(model_results, indent=4))\n\n                profile_result_processor.process_new_response(\n                    response,\n                    process_time=process_time,\n                    node_id=node_id,\n                    k8s_pod_name=k8s_pod_name,\n                    gradcam_method_name=gradcam_method_name,\n                )\n\n            # Print the final profile result and update the service_data.json\n            profile_result_processor.complete_profile()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\n\nOPTIONS = [\n    {\n        \"label\": \"Get help information\",\n        \"action\": option_help,\n    },\n    {\n        \"label\": \"Run AI service\",\n        \"action\": option_run,\n    },\n    {\n        \"label\": \"Profile AI service\",\n        \"action\": option_profile_run,\n    },\n    {\n        \"label\": \"Run AI service with XAI (only image-classification models)\",\n        \"action\": option_run_with_xai,\n    },\n    {\n        \"label\": \"Profile AI service with XAI (only image-classification models)\",\n        \"action\": option_profile_run_with_xai,\n    },\n]\n\n\nif __name__ == \"__main__\":\n    while True:\n        print(\"\\nOptions:\")\n        for i, option in enumerate(OPTIONS, start=1):\n            print(f\"{i}. {option['label']}\")\n        print(\"q. Quit\")\n        choice = input(\"Enter your choice: \")\n\n        if choice == \"q\":\n            print(\"Exiting the client. Goodbye!\")\n            break\n        else:\n            try:\n                choice = int(choice)\n                if 1 <= choice <= len(OPTIONS):\n                    OPTIONS[choice - 1][\"action\"]()\n                else:\n                    print(\"Invalid choice. Please try again.\")\n            except ValueError:\n                print(\"Invalid input. Please enter a number.\")\n",
            "ai_client_utils_script_content": "# import necessary libraries\nimport os\n\ndef prepare_ai_service_request_files():\n    \"\"\"Prepare the `files` part for the AI service request.\"\"\"\n    files = {}\n    image_file_path = input(\"Please input the image file path: \")\n    if not os.path.exists(image_file_path):\n        raise FileNotFoundError(f\"The image file {image_file_path} does not exist.\")\n    with open(image_file_path, \"rb\") as image_file:\n        files[\"file\"] = image_file.read()\n    return files\n\ndef prepare_ai_service_request_data():\n    \"\"\"Prepare the `data` part including `ue_id` for the AI service request.\"\"\"\n    data = {}\n    ue_id = input(\"Please input the unique execution ID (ue_id): \")\n    data[\"ue_id\"] = ue_id\n    return data",
            "model_script_content": "# import server utils\nfrom ai_server_utils import (\n    process_model_output_logits,\n    profile_activities,\n    prepare_profile_results,\n)\n\n# import profile utils\nfrom torch.profiler import profile, record_function\n\n# import necessary libs for AI model inference and request handling\nimport torch\nfrom fastapi import APIRouter, File, Form, UploadFile\nfrom fastapi.responses import JSONResponse\nfrom transformers import AutoImageProcessor, MobileViTForImageClassification\nfrom PIL import Image\n\n# --------------------------------\n# Device configuration\n# --------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# --------------------------------\n# Model-specific configuration\n# make sure the variables `MODEL_NAME` and `model` are defined here.\n# --------------------------------\nMODEL_NAME = \"apple/mobilevit-small\"\nprocessor = AutoImageProcessor.from_pretrained(MODEL_NAME)\nmodel = MobileViTForImageClassification.from_pretrained(MODEL_NAME).to(device)\nmodel.eval()\n\n# Initialize the FastAPI router\nrouter = APIRouter()\n\n\n@router.post(\"/run\")\nasync def run_model(file: UploadFile = File(...), ue_id: str = Form(...)):\n    try:\n        # Prepare the model input\n        image = Image.open(file.file).convert(\"RGB\")\n        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n\n        # Perform inference\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        # Process the model outputs\n        predictions = process_model_output_logits(model, outputs.logits)\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"model_results\": predictions,\n            }\n        )\n    except Exception as e:\n        print(f\"Error processing file: {e}\")\n        return JSONResponse(\n            content={\"error\": \"Failed to process the image. {e}\".format(e=str(e))},\n            status_code=500,\n        )\n\n\n@router.post(\"/profile_run\")\nasync def profile_run(file: UploadFile = File(...), ue_id: str = Form(...)):\n    \"\"\"\n    Endpoint to profile the AI model execution.\n    \"\"\"\n    try:\n        # Prepare the model input\n        image = Image.open(file.file).convert(\"RGB\")\n        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n\n        # perform profiling\n        with profile(\n            activities=profile_activities,\n            profile_memory=True,\n        ) as prof:\n            with record_function(\"model_run\"):\n                with torch.no_grad():\n                    model_outputs = model(**inputs)\n\n        profile_result = prepare_profile_results(prof)\n\n        # Process the model outputs\n        predictions = process_model_output_logits(model, model_outputs.logits)\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"profile_result\": profile_result,\n                \"model_results\": predictions,\n            }\n        )\n\n    except Exception as e:\n        print(f\"Error processing request: {e}\")\n        return JSONResponse(\n            content={\"error\": f\"Failed to process the request. {e}\"},\n            status_code=500,\n        )\n\n\n# Below are the model input and output specifications to be used by the `/help` endpoint\nMODEL_INPUT_FORM_SPEC = {\n    \"file\": {\n        \"type\": \"file upload\",\n        \"description\": \"The image file to be classified.\",\n        \"required\": True,\n        \"example\": \"puppy.png\",\n    }\n}\n\nMODEL_OUTPUT_JSON_SPEC = {\n    \"ue_id\": \"unique execution ID\",\n    \"model_results\": [\n        {\n            \"category_id\": \"category id\",\n            \"label\": \"category label\",\n            \"probability\": \"probability value\",\n        }\n    ],\n}\n",
            "healthcheck_script_content": "import requests\n\nprint(requests.get(\"http://localhost:8000/help\"))"
        }
    },
    {
        "model_name": "google/vit-base-patch16-224",
        "model_url": "https://huggingface.co/google/vit-base-patch16-224",
        "task": "image-classification",
        "task_detail": "The pre-trained AI model, Google ViT-Base-Patch16-224, is a Vision Transformer model designed primarily for image classification tasks. This transformer-based architecture processes images as sequences of fixed-size patches, each with a resolution of 16x16 pixels. The model is pre-trained on the extensive ImageNet-21k dataset, which consists of 14 million high-resolution images divided into 21,843 distinct classes. Following this, it undergoes fine-tuning on the ImageNet 2012 dataset, a more concise collection of 1 million images categorized into 1,000 classes. \n\nThe model expects input images to be in 224x224 resolution, with preprocessing involving resizing and normalizing across RGB channels using a mean of (0.5, 0.5, 0.5) and a standard deviation of (0.5, 0.5, 0.5). Within the model, images are segmented into patches for linear embedding, and a special [CLS] token is added to represent the entire image, assisting in classification tasks. Absolute positional embeddings are included to maintain the spatial information of the image patches as the sequence passes through the transformer encoder.\n\nThe output from this model is a set of logits that provide probabilities for each class it has been trained on. For classification, the index of the maximum logit value corresponds to the predicted class. This makes the model suitable for tasks requiring classification into the 1,000 ImageNet classes. For expanded capabilities, it can be fine-tuned towards specific datasets or tasks, making it versatile in various vision-based applications beyond the initial pre-training scope.\n\nIn practical usage scenarios, the model can be deployed through a simple integration with the Hugging Face Transformer library, which facilitates the processing, prediction, and retrieval of the class index labels. This AI model excels in environments needing accurate image classification compatible with datasets similar in complexity and size to ImageNet, serving well in fields such as automated visual inspection, object detection, and image organization across various sectors.",
        "accuracy_info": "No accuracy information found.",
        "image_repository_url": "",
        "service_disk_size_bytes": 3609149448.0,
        "profiles": [
            {
                "node_id": "ugurcan.celik",
                "device_type": "DeviceType.CPU",
                "device_name": "cuda",
                "initialization_time_ms": 6383.449554443359,
                "eviction_time_ms": 0.0,
                "initialization_cost": 0.0,
                "keep_alive_cost": 0.0,
                "energy_consumption_idle": 0.0,
                "idle_container_cpu_memory_usage": "2.9GB",
                "idle_container_device_memory_usage": "2.2GB",
                "inference": {
                    "cpu_time_ms": 143.60367000000002,
                    "device_time_ms": 10.283988999999915,
                    "cpu_memory_usage_MB": 0.0,
                    "self_cpu_memory_usage_MB": -1.52587890625e-05,
                    "device_memory_usage_MB": 8.12890625,
                    "self_device_memory_usage_MB": -154.546875,
                    "energy_consumption_execution": 0.0,
                    "disk_IO_MB": 0.0,
                    "input_data_MB": 0.0,
                    "output_data_MB": 0.0,
                    "execution_time_ms": 237.39131291707358,
                    "execution_cost": 0.0
                },
                "xai": [
                    {
                        "xai_method": "GradCAM",
                        "cpu_time_ms": 67.31051699999999,
                        "device_time_ms": 4.891092,
                        "cpu_memory_usage_MB": 1.14862060546875,
                        "self_cpu_memory_usage_MB": 0.570404052734375,
                        "device_memory_usage_MB": 452.8955078125,
                        "self_device_memory_usage_MB": 303.8310546875,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 637.1666590372721,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "HiResCAM",
                        "cpu_time_ms": 29.25127,
                        "device_time_ms": 4.887241000000005,
                        "cpu_memory_usage_MB": 1.14862060546875,
                        "self_cpu_memory_usage_MB": 0.570404052734375,
                        "device_memory_usage_MB": 113.283203125,
                        "self_device_memory_usage_MB": -35.83984375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 628.7709871927897,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "XGradCAM",
                        "cpu_time_ms": 30.086565,
                        "device_time_ms": 4.88574800000001,
                        "cpu_memory_usage_MB": 1.14862060546875,
                        "self_cpu_memory_usage_MB": 0.570404052734375,
                        "device_memory_usage_MB": 113.283203125,
                        "self_device_memory_usage_MB": -35.78125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 568.0562655131022,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "GradCAMPlusPlus",
                        "cpu_time_ms": 26.110899,
                        "device_time_ms": 4.885274999999998,
                        "cpu_memory_usage_MB": 1.14862060546875,
                        "self_cpu_memory_usage_MB": 0.570404052734375,
                        "device_memory_usage_MB": 113.283203125,
                        "self_device_memory_usage_MB": -35.78125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 619.652271270752,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "ScoreCAM",
                        "cpu_time_ms": 3171.524978,
                        "device_time_ms": 3053.2324899999735,
                        "cpu_memory_usage_MB": 441.57440185546875,
                        "self_cpu_memory_usage_MB": -0.0133819580078125,
                        "device_memory_usage_MB": 113.283203125,
                        "self_device_memory_usage_MB": -114414.63720703125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 13278.252442677816,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "LayerCAM",
                        "cpu_time_ms": 31.574861000000002,
                        "device_time_ms": 4.881366,
                        "cpu_memory_usage_MB": 1.14862060546875,
                        "self_cpu_memory_usage_MB": 0.570404052734375,
                        "device_memory_usage_MB": 113.283203125,
                        "self_device_memory_usage_MB": -35.78125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 533.7039629618326,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenCAM",
                        "cpu_time_ms": 64.908248,
                        "device_time_ms": 4.903231999999995,
                        "cpu_memory_usage_MB": 0.57440185546875,
                        "self_cpu_memory_usage_MB": -0.003814697265625,
                        "device_memory_usage_MB": 113.283203125,
                        "self_device_memory_usage_MB": -35.7802734375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 250.18962224324542,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenGradCAM",
                        "cpu_time_ms": 172.952173,
                        "device_time_ms": 4.92083299999999,
                        "cpu_memory_usage_MB": 1.14862060546875,
                        "self_cpu_memory_usage_MB": 0.570404052734375,
                        "device_memory_usage_MB": 113.283203125,
                        "self_device_memory_usage_MB": -35.78125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1040.7477219899495,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "KPCA_CAM",
                        "cpu_time_ms": 18.943887000000004,
                        "device_time_ms": 4.879354999999984,
                        "cpu_memory_usage_MB": 0.57440185546875,
                        "self_cpu_memory_usage_MB": -0.003814697265625,
                        "device_memory_usage_MB": 113.283203125,
                        "self_device_memory_usage_MB": -35.7802734375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 199.93233680725098,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "RandomCAM",
                        "cpu_time_ms": 29.954125,
                        "device_time_ms": 4.9150400000000145,
                        "cpu_memory_usage_MB": 1.14862060546875,
                        "self_cpu_memory_usage_MB": 0.570404052734375,
                        "device_memory_usage_MB": 113.283203125,
                        "self_device_memory_usage_MB": -35.78125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 670.1389948527018,
                        "execution_cost": 0.0
                    }
                ]
            },
            {
                "node_id": "LAP004262",
                "device_type": "DeviceType.CPU",
                "device_name": "None",
                "initialization_time_ms": 41775.08330345154,
                "eviction_time_ms": 0.0,
                "initialization_cost": 0.0,
                "keep_alive_cost": 0.0,
                "energy_consumption_idle": 0.0,
                "idle_container_cpu_memory_usage": "1.5GB",
                "idle_container_device_memory_usage": "0GB",
                "inference": {
                    "cpu_time_ms": 156.61362899999997,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 0.003814697265625,
                    "self_cpu_memory_usage_MB": -154.6728515625,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0.0,
                    "disk_IO_MB": 0.0,
                    "input_data_MB": 0.0,
                    "output_data_MB": 0.0,
                    "execution_time_ms": 248.48397572835287,
                    "execution_cost": 0.0
                },
                "xai": [
                    {
                        "xai_method": "GradCAM",
                        "cpu_time_ms": 526.2583740000001,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 450.9971694946289,
                        "self_cpu_memory_usage_MB": -366.58390045166016,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1053.3445676167805,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "HiResCAM",
                        "cpu_time_ms": 537.163626,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 120.19355010986328,
                        "self_cpu_memory_usage_MB": -366.58390045166016,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 976.9228299458822,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "XGradCAM",
                        "cpu_time_ms": 480.135967,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 120.19355010986328,
                        "self_cpu_memory_usage_MB": -366.58390045166016,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 856.2522729237875,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "GradCAMPlusPlus",
                        "cpu_time_ms": 496.322495,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 120.19355010986328,
                        "self_cpu_memory_usage_MB": -367.16104888916016,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 952.6763757069906,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "LayerCAM",
                        "cpu_time_ms": 462.599266,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 120.19355010986328,
                        "self_cpu_memory_usage_MB": -366.58390045166016,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 855.4818630218506,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenCAM",
                        "cpu_time_ms": 230.062443,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 119.61640167236328,
                        "self_cpu_memory_usage_MB": -35.7802734375,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 362.49597867329913,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenGradCAM",
                        "cpu_time_ms": 608.526745,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 120.19355010986328,
                        "self_cpu_memory_usage_MB": -366.58390045166016,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1102.5581359863281,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "KPCA_CAM",
                        "cpu_time_ms": 182.275092,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 119.61640167236328,
                        "self_cpu_memory_usage_MB": -35.7802734375,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 319.1238244374593,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "RandomCAM",
                        "cpu_time_ms": 474.440218,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 120.19355010986328,
                        "self_cpu_memory_usage_MB": -366.58390045166016,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 938.7407302856445,
                        "execution_cost": 0.0
                    }
                ]
            }
        ],
        "feedback": {
            "likes": [],
            "dislikes": [],
            "comments": []
        },
        "code": {
            "readme_content": "---\nlicense: apache-2.0\ntags:\n- vision\n- image-classification\ndatasets:\n- imagenet-1k\n- imagenet-21k\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\n  example_title: Tiger\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg\n  example_title: Teapot\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg\n  example_title: Palace\n---\n\n# Vision Transformer (base-sized model) \n\nVision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Dosovitskiy et al. and first released in [this repository](https://github.com/google-research/vision_transformer). However, the weights were converted from the [timm repository](https://github.com/rwightman/pytorch-image-models) by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him. \n\nDisclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. Next, the model was fine-tuned on ImageNet (also referred to as ILSVRC2012), a dataset comprising 1 million images and 1,000 classes, also at resolution 224x224.\n\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\n\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.\n\n## Intended uses & limitations\n\nYou can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=google/vit) to look for\nfine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\n\n```python\nfrom transformers import ViTImageProcessor, ViTForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n```\n\nFor more code examples, we refer to the [documentation](https://huggingface.co/transformers/model_doc/vit.html#).\n\n## Training data\n\nThe ViT model was pretrained on [ImageNet-21k](http://www.image-net.org/), a dataset consisting of 14 million images and 21k classes, and fine-tuned on [ImageNet](http://www.image-net.org/challenges/LSVRC/2012/), a dataset consisting of 1 million images and 1k classes. \n\n## Training procedure\n\n### Preprocessing\n\nThe exact details of preprocessing of images during training/validation can be found [here](https://github.com/google-research/vision_transformer/blob/master/vit_jax/input_pipeline.py). \n\nImages are resized/rescaled to the same resolution (224x224) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).\n\n### Pretraining\n\nThe model was trained on TPUv3 hardware (8 cores). All model variants are trained with a batch size of 4096 and learning rate warmup of 10k steps. For ImageNet, the authors found it beneficial to additionally apply gradient clipping at global norm 1. Training resolution is 224.\n\n## Evaluation results\n\nFor evaluation results on several image classification benchmarks, we refer to tables 2 and 5 of the original paper. Note that for fine-tuning, the best results are obtained with a higher resolution (384x384). Of course, increasing the model size will result in better performance.\n\n### BibTeX entry and citation info\n\n```bibtex\n@misc{wu2020visual,\n      title={Visual Transformers: Token-based Image Representation and Processing for Computer Vision}, \n      author={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},\n      year={2020},\n      eprint={2006.03677},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n```bibtex\n@inproceedings{deng2009imagenet,\n  title={Imagenet: A large-scale hierarchical image database},\n  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\n  booktitle={2009 IEEE conference on computer vision and pattern recognition},\n  pages={248--255},\n  year={2009},\n  organization={Ieee}\n}\n```",
            "dockerfile_content": "# Base image for AI service powered by HuggingFace pre-trained AI models.\n# the image has the following packages/libraries installed already:\n# - python3.12, pip, git, fastapi, uvicorn, torch, torchvision, opencv-python, transformers, python-multipart, Pillow, requests\nFROM python3.12_ai_service_base:latest\n\n# Set working directory\nWORKDIR /app\n\n# Copy application code\nCOPY . .\n\n# Install additional dependencies\nRUN pip install --no-cache-dir transformers\n\n# Expose port 8000\nEXPOSE 8000\n\n# Start the FastAPI server\nCMD [\"uvicorn\", \"ai_server:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--timeout-keep-alive\", \"600\"]",
            "ai_server_script_content": "import json\nimport os\nimport time\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import JSONResponse\nfrom contextlib import asynccontextmanager\nfrom ai_server_utils import (\n    PROFILE_OUTPUT_JSON_SPEC,\n    NODE_ID,\n    K8S_POD_NAME,\n)\n\n\n# -------------------------------------------\n# App Lifespan setup\n# -------------------------------------------\n# Record the script start time (when uvicorn starts the process)\nSCRIPT_START_TIME = time.time()\nINITIALIZATION_DURATION = 0.0\nservice_endpoint_specs = {\n    \"model_input_form_spec\": None,\n    \"model_output_json_spec\": None,\n    \"profile_output_json_spec\": None,\n    \"xai_model_input_form_spec\": None,\n    \"xai_model_output_json_spec\": None,\n    \"xai_profile_output_json_spec\": None,\n}\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n\n    global INITIALIZATION_DURATION\n    global SCRIPT_START_TIME\n    global service_endpoint_specs\n\n    # Load the AI model\n    print(\"Loading AI model...\")\n    from model import (\n        MODEL_INPUT_FORM_SPEC,\n        MODEL_OUTPUT_JSON_SPEC,\n        router as model_router,\n    )\n\n    service_endpoint_specs[\"model_input_form_spec\"] = MODEL_INPUT_FORM_SPEC\n    service_endpoint_specs[\"model_output_json_spec\"] = MODEL_OUTPUT_JSON_SPEC\n    service_endpoint_specs[\"profile_output_json_spec\"] = PROFILE_OUTPUT_JSON_SPEC\n\n    app.include_router(model_router, prefix=\"/model\", tags=[\"AI Model\"])\n\n    # Load the XAI model\n    if os.path.exists(os.path.dirname(__file__) + \"/xai_model.py\"):\n        print(\"Loading XAI model...\")\n        from xai_model import (\n            XAI_OUTPUT_JSON_SPEC,\n            router as xai_model_router,\n        )\n\n        # by default, the xai_model input form spec is the same as the model input form spec\n        service_endpoint_specs[\"xai_model_input_form_spec\"] = MODEL_INPUT_FORM_SPEC\n        service_endpoint_specs[\"xai_model_output_json_spec\"] = MODEL_OUTPUT_JSON_SPEC\n        service_endpoint_specs[\"xai_model_output_json_spec\"].update(\n            XAI_OUTPUT_JSON_SPEC\n        )\n        service_endpoint_specs[\"xai_profile_output_json_spec\"] = (\n            PROFILE_OUTPUT_JSON_SPEC\n        )\n        service_endpoint_specs[\"xai_profile_output_json_spec\"].update(\n            XAI_OUTPUT_JSON_SPEC\n        )\n\n        app.include_router(xai_model_router, prefix=\"/xai_model\", tags=[\"XAI Model\"])\n\n    # Record the initialization duration\n    INITIALIZATION_DURATION = time.time() - SCRIPT_START_TIME\n\n    print(f\"AI service loaded in {INITIALIZATION_DURATION:.2f} seconds.\")\n\n    yield\n\n    # Clean up the models and release the resources\n    service_endpoint_specs.clear()\n\n\n# -------------------------------------------\n# FastAPI application setup\n# -------------------------------------------\napp = FastAPI(lifespan=lifespan)\n\n\n# -------------------------------------------\n# Middlewares\n# -------------------------------------------\n@app.middleware(\"http\")\nasync def prepare_header_middleware(request: Request, call_next):\n    start_time = time.perf_counter()\n    response = await call_next(request)\n    process_time = time.perf_counter() - start_time\n    response.headers[\"X-Process-Time\"] = str(process_time)\n    response.headers[\"X-NODE-ID\"] = NODE_ID\n    response.headers[\"X-K8S-POD-NAME\"] = K8S_POD_NAME\n    return response\n\n\n# -------------------------------------------\n# General Endpoints\n# -------------------------------------------\n@app.get(\"/initialization_duration\")\ndef get_initialization_duration():\n    \"\"\"\n    Endpoint to retrieve the initialization duration of the AI model.\n    \"\"\"\n    global INITIALIZATION_DURATION\n\n    if INITIALIZATION_DURATION == 0.0:\n        return JSONResponse(\n            content={\"error\": \"Model not initialized.\"},\n            status_code=500,\n        )\n    return JSONResponse(\n        content={\n            \"initialization_duration\": INITIALIZATION_DURATION,\n            \"script_start_time\": SCRIPT_START_TIME,\n        }\n    )\n\n\n@app.get(\"/help\")\ndef get_help():\n    global service_endpoint_specs\n    help_info = {\n        \"endpoints\": {\n            \"/model/run\": {\n                \"method\": \"POST\",\n                \"description\": \"Executes the AI model with the provided input data.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_output_json_spec\"],\n                },\n            },\n            \"/model/profile_run\": {\n                \"method\": \"POST\",\n                \"description\": \"Profiles the AI model execution.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    \"profile_result\": \"Profiling results of the AI model execution.\",\n                    **service_endpoint_specs[\"profile_output_json_spec\"],\n                },\n            },\n            \"/initialization_duration\": {\n                \"method\": \"GET\",\n                \"description\": \"Retrieves the initialization duration of the AI model.\",\n                \"response\": {\n                    \"initialization_duration\": \"Time taken to initialize the model (in seconds).\",\n                    \"script_start_time\": \"Timestamp when the script started (in seconds since epoch).\",\n                },\n            },\n        }\n    }\n\n    if service_endpoint_specs[\"xai_model_input_form_spec\"] is not None:\n        help_info[\"endpoints\"][\"/xai_model/run\"] = {\n            \"method\": \"POST\",\n            \"description\": \"Executes the XAI model with the provided input data.\",\n            \"parameters\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_input_form_spec\"],\n            },\n            \"response\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_output_json_spec\"],\n            },\n        }\n\n        help_info[\"endpoints\"][\"/xai_model/profile_run\"] = {\n            \"method\": \"POST\",\n            \"description\": \"Profiles the XAI model execution.\",\n            \"parameters\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_input_form_spec\"],\n            },\n            \"response\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                \"profile_result\": \"Profiling results of the XAI model execution.\",\n                **service_endpoint_specs[\"xai_profile_output_json_spec\"],\n            },\n        }\n    \n    return JSONResponse(content=help_info)\n",
            "ai_client_script_content": "import base64\nimport json\nimport time\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nfrom ai_client_utils import (\n    prepare_ai_service_request_files,\n    prepare_ai_service_request_data,\n)\n\nXAI_GRADCAM_METHODS = [\n    \"GradCAM\",\n    \"HiResCAM\",\n    \"XGradCAM\",\n    \"GradCAMPlusPlus\",\n    \"ScoreCAM\",\n    \"LayerCAM\",\n    \"EigenCAM\",\n    \"EigenGradCAM\",\n    \"KPCA_CAM\",\n    \"RandomCAM\",\n]\n\n# -------------------------------------\n# prompt for necessary inputs\n# -------------------------------------\nSERVER_URL = input(\"Please input server URL (default to http://localhost:9000): \")\nif SERVER_URL.strip() == \"\":\n    SERVER_URL = \"http://localhost:9000\"\nUE_ID = input(\"Please input UE_ID (default to 123456): \")\nif UE_ID.strip() == \"\":\n    UE_ID = \"123456\"\n\n\ndef send_post_request(url, data, files):\n    \"\"\"Send request to run AI service and display AI service responses.\"\"\"\n    try:\n        response = requests.post(url, files=files, data=data)\n        # get the process time, node id and k8s pod name from the response headers\n        process_time = response.headers.get(\"X-Process-Time\")\n        node_id = response.headers.get(\"X-NODE-ID\")\n        k8s_pod_name = response.headers.get(\"X-K8S-POD-NAME\")\n        if response.status_code == 200:\n            return response.json(), process_time, node_id, k8s_pod_name\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n\ndef send_get_request(url, params=None):\n    \"\"\"Send GET request to the specified URL and return the response.\"\"\"\n    try:\n        response = requests.get(url, params=params)\n        process_time = response.headers.get(\"X-Process-Time\")\n        node_id = response.headers.get(\"X-NODE-ID\")\n        k8s_pod_name = response.headers.get(\"X-K8S-POD-NAME\")\n        if response.status_code == 200:\n            return response.json(), process_time, node_id, k8s_pod_name\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n            return None\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n\nclass ProfileResultProcessor:\n\n    def __init__(self, server_url):\n        self.server_url = server_url\n        self.start_time = time.time()\n        self.service_initialization_duration = 0\n        self.response_counter = 0\n        self.profile_name = None\n        self.device_type = None\n        self.device_name = None\n        self.node_id = None\n        self.k8s_pod_name = None\n        self.cpu_memory_usage_bytes = 0\n        self.self_cpu_memory_usage_bytes = 0\n        self.device_memory_usage_bytes = 0\n        self.self_device_memory_usage_bytes = 0\n        self.cpu_time_total_us = 0\n        self.self_cpu_time_total_us = 0\n        self.device_time_total_us = 0\n        self.self_device_time_total_us = 0\n\n        # xai related\n        self.gradcam_method_name = None\n\n        self.fetch_service_initialization_duration()\n\n    def fetch_service_initialization_duration(self):\n        \"\"\"Fetch the service initialization duration from the server.\"\"\"\n        response, process_time, node_id, k8s_pod_name = send_get_request(\n            f\"{self.server_url}/initialization_duration\"\n        )\n        if response:\n            self.service_initialization_duration = response.get(\n                \"initialization_duration\", 0\n            )\n        else:\n            print(\"Failed to fetch initialization duration.\")\n            self.service_initialization_duration = 0\n\n    def process_new_response(\n        self,\n        profile_response,\n        process_time=None,\n        node_id=None,\n        k8s_pod_name=None,\n        gradcam_method_name=None,\n    ):\n        if not profile_response:\n            return\n\n        profile_result = profile_response[\"profile_result\"]\n\n        if not profile_result:\n            return\n\n        if self.profile_name is None:\n            self.profile_name = profile_result[\"name\"]\n        if self.device_type is None:\n            self.device_type = profile_result[\"device_type\"]\n        if self.device_name is None:\n            self.device_name = profile_result[\"device_name\"]\n        if self.node_id is None:\n            self.node_id = node_id\n        if self.k8s_pod_name is None:\n            self.k8s_pod_name = k8s_pod_name\n\n        if self.gradcam_method_name is None:\n            self.gradcam_method_name = gradcam_method_name\n\n        # update the max profile result\n        self.cpu_memory_usage_bytes = max(\n            self.cpu_memory_usage_bytes, profile_result[\"cpu_memory_usage\"]\n        )\n        # self cpu memory usage could be negative. here we take the value that has the max absolute value\n        if abs(profile_result[\"self_cpu_memory_usage\"]) > abs( self.self_cpu_memory_usage_bytes):\n            self.self_cpu_memory_usage_bytes = profile_result[\"self_cpu_memory_usage\"]\n        self.device_memory_usage_bytes = max(\n            self.device_memory_usage_bytes, profile_result[\"device_memory_usage\"]\n        )\n        # same as self cpu memory usage\n        if abs(profile_result[\"self_device_memory_usage\"]) > abs(self.self_device_memory_usage_bytes):\n            self.self_device_memory_usage_bytes = profile_result[\"self_device_memory_usage\"]\n        self.cpu_time_total_us = max(\n            self.cpu_time_total_us, profile_result[\"cpu_time_total\"]\n        )\n        self.self_cpu_time_total_us = max(\n            self.self_cpu_time_total_us, profile_result[\"self_cpu_time_total\"]\n        )\n        self.device_time_total_us = max(\n            self.device_time_total_us, profile_result[\"device_time_total\"]\n        )\n        self.self_device_time_total_us = max(\n            self.self_device_time_total_us, profile_result[\"self_device_time_total\"]\n        )\n\n        self.response_counter += 1\n\n    def complete_profile(self):\n        print(\"\\n--------- PROFILE EVENT ---------\\n\")\n        print(f\"Name: {self.profile_name}\")\n        print(f\"Device Type: {self.device_type}\")\n        print(f\"Device Name: {self.device_name}\")\n        print(f\"Node ID: {self.node_id}\")\n        print(f\"K8S_POD_NAME: {self.k8s_pod_name}\")\n\n        if self.gradcam_method_name:\n            print(f\"GradCAM Method Name: {self.gradcam_method_name}\")\n\n        print(\"\\n--------- LATENCY RESULT ---------\\n\")\n        print(\"Service Initialization Duration: \", self.service_initialization_duration)\n        print(\"Total Requests: \", self.response_counter)\n        print(f\"Total Time Taken: {time.time() - self.start_time:.2f} seconds\")\n        print(\n            f\"Average Time Taken: {(time.time() - self.start_time) / self.response_counter:.2f} seconds\"\n        )\n\n        print(\"\\n--------- RESOURCE USAGE ---------\\n\")\n        print(f\"CPU Memory Usage: {self.cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\")\n        print(\n            f\"Self CPU Memory Usage: {self.self_cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Device Memory Usage: {self.device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Self Device Memory Usage: {self.self_device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(f\"CPU Time Total: {self.cpu_time_total_us / 1000:.2f} ms\")\n        print(f\"Self CPU Time Total: {self.self_cpu_time_total_us / 1000:.2f} ms\")\n        print(f\"Device Time Total: {self.device_time_total_us / 1000:.2f} ms\")\n        print(f\"Self Device Time Total: {self.self_device_time_total_us / 1000:.2f} ms\")\n\n        # update the service_data.json automatically\n        with open(\"service_data.json\", \"r\") as f:\n            service_data = json.load(f)\n\n        if not self.gradcam_method_name:\n            complete_profile_data_to_save = {\n                \"node_id\": self.node_id,\n                \"device_type\": self.device_type,\n                \"device_name\": self.device_name,\n                \"initialization_time_ms\": self.service_initialization_duration * 1000,\n                \"eviction_time_ms\": 0,\n                \"initialization_cost\": 0,\n                \"keep_alive_cost\": 0,\n                \"energy_consumption_idle\": 0,\n                \"inference\": {\n                    \"cpu_time_ms\": self.cpu_time_total_us / 1000,\n                    \"device_time_ms\": self.device_time_total_us / 1000,\n                    \"cpu_memory_usage_MB\": self.cpu_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"self_cpu_memory_usage_MB\": self.self_cpu_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"device_memory_usage_MB\": self.device_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"self_device_memory_usage_MB\": self.self_device_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"energy_consumption_execution\": 0,\n                    \"disk_IO_MB\": 0,\n                    \"input_data_MB\": 0,\n                    \"output_data_MB\": 0,\n                    \"execution_time_ms\": (time.time() - self.start_time)\n                    / self.response_counter\n                    * 1000,\n                    \"execution_cost\": 0,\n                },\n            }\n\n            # check if there is already a profile for this node id\n            profile_found = False\n            for profile in service_data[\"profiles\"]:\n                if profile[\"node_id\"] == self.node_id:\n                    profile[\"inference\"] = complete_profile_data_to_save[\"inference\"]\n                    profile_found = True\n                    break\n            if not profile_found:\n                service_data[\"profiles\"].append(complete_profile_data_to_save)\n\n            # save the updated service_data.json\n            with open(\"service_data.json\", \"w\") as f:\n                json.dump(service_data, f, indent=4)\n            print(\"\\n--------- SERVICE DATA UPDATED ---------\\n\")\n\n        else:\n            complete_xai_profile_data_to_save = {\n                \"node_id\": self.node_id,\n                \"device_type\": self.device_type,\n                \"device_name\": self.device_name,\n                \"initialization_time_ms\": self.service_initialization_duration * 1000,\n                \"eviction_time_ms\": 0,\n                \"initialization_cost\": 0,\n                \"keep_alive_cost\": 0,\n                \"energy_consumption_idle\": 0,\n                \"xai\": [\n                    {\n                        \"xai_method\": self.gradcam_method_name,\n                        \"cpu_time_ms\": self.cpu_time_total_us / 1000,\n                        \"device_time_ms\": self.device_time_total_us / 1000,\n                        \"cpu_memory_usage_MB\": self.cpu_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"self_cpu_memory_usage_MB\": self.self_cpu_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"device_memory_usage_MB\": self.device_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"self_device_memory_usage_MB\": self.self_device_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"energy_consumption_execution\": 0,\n                        \"disk_IO_MB\": 0,\n                        \"input_data_MB\": 0,\n                        \"output_data_MB\": 0,\n                        \"execution_time_ms\": (time.time() - self.start_time)\n                        / self.response_counter\n                        * 1000,\n                        \"execution_cost\": 0,\n                    }\n                ],\n            }\n\n            # check if there is already a profile for this node id\n            profile_found = False\n            for profile in service_data[\"profiles\"]:\n                if profile[\"node_id\"] == self.node_id:\n                    profile_found = True\n\n                    # check if there is already a profile for this xai method\n                    xai_method_found = False\n                    if not profile.get(\"xai\"):\n                        profile[\"xai\"] = []\n                    for xai_profile in profile[\"xai\"]:\n                        if xai_profile[\"xai_method\"] == self.gradcam_method_name:\n                            xai_profile.update(complete_xai_profile_data_to_save[\"xai\"][0]) \n                            xai_method_found = True\n                            break\n                    if not xai_method_found:\n                        profile[\"xai\"].append(complete_xai_profile_data_to_save[\"xai\"][0])\n                    break\n\n            if not profile_found:\n                service_data[\"profiles\"].append(complete_xai_profile_data_to_save)\n\n            # save the updated service_data.json\n            with open(\"service_data.json\", \"w\") as f:\n                json.dump(service_data, f, indent=4)\n            print(\"\\n--------- SERVICE DATA UPDATED ---------\\n\")\n\n\ndef option_run():\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n    data = {**data, \"ue_id\": UE_ID}\n    response, process_time, node_id, pod_name = send_post_request(\n        f\"{SERVER_URL}/model/run\", data, files\n    )\n    print(\"Process Time: \", process_time)\n    print(\"Node ID: \", node_id)\n    print(\"K8S_POD_NAME: \", pod_name)\n    print(\"Response\")\n    print(json.dumps(response, indent=4))\n\n\ndef option_profile_run():\n    data = prepare_ai_service_request_data()\n    data = {**data, \"ue_id\": UE_ID}\n    files = prepare_ai_service_request_files()\n    num_requests = int(input(\"Enter the number of requests to send: \"))\n\n    try:\n        profile_result_processor = ProfileResultProcessor(SERVER_URL)\n\n        for _ in range(num_requests):\n            profile_response, process_time, node_id, k8s_node_name = send_post_request(\n                f\"{SERVER_URL}/model/profile_run\", data, files\n            )\n            print(\"Process Time: \", process_time)\n            print(\"Node ID: \", node_id)\n            print(\"K8S_POD_NAME: \", k8s_node_name)\n            print(\"Response\")\n            print(json.dumps(profile_response, indent=4))\n            if not profile_response:\n                print(\"No profile response received.\")\n                continue\n\n            profile_result_processor.process_new_response(\n                profile_response,\n                process_time=process_time,\n                node_id=node_id,\n                k8s_pod_name=k8s_node_name,\n            )\n\n        # Print the final profile result and update the service_data.json\n        profile_result_processor.complete_profile()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\n\ndef option_help():\n    \"\"\"Get help information from the server.\"\"\"\n    try:\n        response, process_time, node_id, pod_name = send_get_request(\n            f\"{SERVER_URL}/help\"\n        )\n        print(\"Process Time: \", process_time)\n        print(\"Node ID: \", node_id)\n        print(\"K8S_POD_NAME: \", pod_name)\n        print(\"Response\")\n        print(json.dumps(response, indent=4))\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n\n\ndef option_run_with_xai():\n    \"\"\"Run AI service with XAI.\"\"\"\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n\n    print(\n        \"Note that currently only GradCAM methods on image-classification models are supported.\"\n    )\n    while True:\n        gradcam_method_name = input(\n            f\"Please select a GradCAM method (options: {XAI_GRADCAM_METHODS}): \"\n        )\n        if gradcam_method_name not in XAI_GRADCAM_METHODS:\n            print(f\"Invalid GradCAM method. Please select again.\")\n        else:\n            break\n\n    # ask for target class for explanation\n    target_category_indexes = input(\n        \"Please input target category indexes for explanation (comma-separated, e.g., 111, 32, 44, ...): \"\n    )\n    if not target_category_indexes or not target_category_indexes.strip():\n        print(\n            \"No target category indexes provided. Defaulting to explaining the top confident category.\"\n        )\n        target_category_indexes = []\n    else:\n        target_category_indexes = [\n            int(i.strip()) for i in target_category_indexes.split(\",\")\n        ]\n\n    data = {\n        **data,\n        \"ue_id\": UE_ID,\n        \"gradcam_method_name\": gradcam_method_name,\n        \"target_category_indexes\": target_category_indexes,\n    }\n    print(\"Data: \", data)\n    response, process_time, node_id, k8s_pod_name = send_post_request(\n        f\"{SERVER_URL}/xai_model/run\", data, files\n    )\n    print(\"Process Time: \", process_time)\n    print(\"Node ID: \", node_id)\n    print(\"K8S_POD_NAME: \", k8s_pod_name)\n\n    # Handle JSON response\n    model_results = response.get(\"model_results\")\n    if model_results:\n        print(\"Model Results:\", json.dumps(model_results, indent=4))\n\n    xai_results = response.get(\"xai_results\")\n    if xai_results:\n        print(\"XAI Results Method:\", xai_results.get(\"xai_method\"))\n\n    # Handle binary image response\n    encoded_image = xai_results.get(\"image\")\n    if encoded_image:\n        image_bytes = base64.b64decode(encoded_image)\n\n        # Load the image into Pillow\n        image = Image.open(BytesIO(image_bytes))\n\n        # Save image to disk\n        image.save(\"xai_output.png\")\n\n        # Display the image using matplotlib\n        plt.imshow(image)\n        plt.axis(\"off\")\n        plt.show()\n\n\ndef option_profile_run_with_xai():\n    \"\"\"Run AI service with XAI and profile the run.\"\"\"\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n    print(\n        \"Note that currently only GradCAM methods on image-classification models are supported.\"\n    )\n\n    # ask for target class for explanation\n    target_category_indexes = input(\n        \"Please input target category indexes for explanation (comma-separated, e.g., 111, 32, 44, ...): \"\n    )\n    if not target_category_indexes or not target_category_indexes.strip():\n        print(\n            \"No target category indexes provided. Defaulting to explaining the top confident category.\"\n        )\n        target_category_indexes = []\n    else:\n        target_category_indexes = [\n            int(i.strip()) for i in target_category_indexes.split(\",\")\n        ]\n\n    num_requests = int(input(\"Enter the number of requests to send: \"))\n\n    try:\n        for gradcam_method_name in XAI_GRADCAM_METHODS:\n\n            data = {\n                **data,\n                \"ue_id\": UE_ID,\n                \"gradcam_method_name\": gradcam_method_name,\n                \"target_category_indexes\": target_category_indexes,\n            }\n            print(\"Data: \", data)\n\n            profile_result_processor = ProfileResultProcessor(SERVER_URL)\n\n            for _ in range(num_requests):\n                response, process_time, node_id, k8s_pod_name = send_post_request(\n                    f\"{SERVER_URL}/xai_model/profile_run\", data, files\n                )\n                print(\"Process Time: \", process_time)\n                print(\"Node ID: \", node_id)\n                print(\"K8S_POD_NAME: \", k8s_pod_name)\n                if not response:\n                    print(\"No profile response received.\")\n                    continue\n\n                # Handle JSON response\n                model_results = response.get(\"model_results\")\n                if model_results:\n                    print(\"Model Results:\", json.dumps(model_results, indent=4))\n\n                profile_result_processor.process_new_response(\n                    response,\n                    process_time=process_time,\n                    node_id=node_id,\n                    k8s_pod_name=k8s_pod_name,\n                    gradcam_method_name=gradcam_method_name,\n                )\n\n            # Print the final profile result and update the service_data.json\n            profile_result_processor.complete_profile()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\n\nOPTIONS = [\n    {\n        \"label\": \"Get help information\",\n        \"action\": option_help,\n    },\n    {\n        \"label\": \"Run AI service\",\n        \"action\": option_run,\n    },\n    {\n        \"label\": \"Profile AI service\",\n        \"action\": option_profile_run,\n    },\n    {\n        \"label\": \"Run AI service with XAI (only image-classification models)\",\n        \"action\": option_run_with_xai,\n    },\n    {\n        \"label\": \"Profile AI service with XAI (only image-classification models)\",\n        \"action\": option_profile_run_with_xai,\n    },\n]\n\n\nif __name__ == \"__main__\":\n    while True:\n        print(\"\\nOptions:\")\n        for i, option in enumerate(OPTIONS, start=1):\n            print(f\"{i}. {option['label']}\")\n        print(\"q. Quit\")\n        choice = input(\"Enter your choice: \")\n\n        if choice == \"q\":\n            print(\"Exiting the client. Goodbye!\")\n            break\n        else:\n            try:\n                choice = int(choice)\n                if 1 <= choice <= len(OPTIONS):\n                    OPTIONS[choice - 1][\"action\"]()\n                else:\n                    print(\"Invalid choice. Please try again.\")\n            except ValueError:\n                print(\"Invalid input. Please enter a number.\")\n",
            "ai_client_utils_script_content": "from uuid import uuid4\n\ndef prepare_ai_service_request_files():\n    \"\"\"Prepare the `files` part for the AI service request.\"\"\"\n    files = {}\n    image_file_path = input(\"Please input the image file path: \")\n    with open(image_file_path, \"rb\") as image_file:\n        files[\"file\"] = image_file.read()\n    return files\n\ndef prepare_ai_service_request_data():\n    \"\"\"Prepare the `data` part for the AI service request.\"\"\"\n    data = {}\n    # Generate a unique execution ID\n    data[\"ue_id\"] = str(uuid4())\n    return data",
            "model_script_content": "# import server utils\nfrom ai_server_utils import (\n    process_model_output_logits,\n    profile_activities,\n    prepare_profile_results,\n)\n# import profile utils\nfrom torch.profiler import profile, record_function\n\n# import necessary libs for AI model inference and request handling\nimport torch\nfrom fastapi import APIRouter, File, Form, UploadFile\nfrom fastapi.responses import JSONResponse\nfrom transformers import ViTImageProcessor, ViTForImageClassification\nfrom PIL import Image\n\n# --------------------------------\n# Device configuration\n# --------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# --------------------------------\n# Model-specific configuration\n# make sure the variables `MODEL_NAME` and `model` are defined here.\n# --------------------------------\nMODEL_NAME = \"google/vit-base-patch16-224\"\nprocessor = ViTImageProcessor.from_pretrained(MODEL_NAME)\nmodel = ViTForImageClassification.from_pretrained(MODEL_NAME).to(device)\nmodel.eval()\n\n# Initialize the FastAPI router\nrouter = APIRouter()\n\n@router.post(\"/run\")\nasync def run_model(file: UploadFile = File(...), ue_id: str = Form(...)):\n    try:\n        # Prepare the model input\n        image = Image.open(file.file).convert(\"RGB\")\n        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n\n        # Perform inference\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        # Process the model outputs\n        predictions = process_model_output_logits(model, outputs.logits)\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"model_results\": predictions,\n            }\n        )\n    except Exception as e:\n        print(f\"Error processing file: {e}\")\n        return JSONResponse(\n            content={\"error\": \"Failed to process the image. {e}\".format(e=str(e))},\n            status_code=500,\n        )\n\n@router.post(\"/profile_run\")\nasync def profile_run(file: UploadFile = File(...), ue_id: str = Form(...)):\n    \"\"\"\n    Endpoint to profile the AI model execution.\n    \"\"\"\n    try:\n        # Prepare the model input\n        image = Image.open(file.file).convert(\"RGB\")\n        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n\n        # perform profiling\n        with profile(\n            activities=profile_activities,\n            profile_memory=True,\n        ) as prof:\n            with record_function(\"model_run\"):\n                with torch.no_grad():\n                    model_outputs = model(**inputs)\n\n        profile_result = prepare_profile_results(prof)\n\n        # Process the model outputs\n        predictions = process_model_output_logits(model, model_outputs.logits)\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"profile_result\": profile_result,\n                \"model_results\": predictions,\n            }\n        )\n\n    except Exception as e:\n        print(f\"Error processing request: {e}\")\n        return JSONResponse(\n            content={\"error\": f\"Failed to process the request. {e}\"},\n            status_code=500,\n        )\n\n# Below are the model input and output specifications to be used by the `/help` endpoint\nMODEL_INPUT_FORM_SPEC = {\n    \"file\": {\n        \"type\": \"file upload\",\n        \"description\": \"The image file to be classified.\",\n        \"required\": True,\n        \"example\": \"puppy.png\",\n    }\n}\n\nMODEL_OUTPUT_JSON_SPEC = {\n    \"ue_id\": \"unique execution ID\",\n    \"model_results\": [\n        {\n            \"category_id\": \"category id\",\n            \"label\": \"category label\",\n            \"probability\": \"probability value\",\n        }\n    ],\n}",
            "healthcheck_script_content": "import requests\n\nprint(requests.get(\"http://localhost:8000/help\"))"
        }
    },
    {
        "model_name": "microsoft/cvt-13",
        "model_url": "https://huggingface.co/microsoft/cvt-13",
        "task": "image-classification",
        "task_detail": "The Convolutional Vision Transformer (CvT-13) is a pre-trained AI model designed for image classification tasks, specifically focusing on classifying images into one of 1,000 categories defined by the ImageNet-1k dataset. The model leverages a hybrid architecture that integrates the strengths of Convolutional Neural Networks (CNNs) with Vision Transformers (ViTs) to enhance performance in visual recognition tasks.\n\n### Input Requirements:\nThe CvT-13 model expects input images to be pre-processed to a resolution of 224x224 pixels. The images should be in RGB format, which is standard for color imagery. During pre-processing, the images are typically normalized to match the distribution of the ImageNet dataset, ensuring consistency in the model's predictions.\n\n### Functionality and Types of Images:\nCvT-13 is adept at processing a wide variety of image types due to its training on the diverse ImageNet-1k dataset. This includes images of animals, objects, scenes, and people. Its convolutional components effectively capture local spatial hierarchies within images, while the transformer elements model the global context, enabling the model to understand complex visual patterns and categorize images accurately.\n\n### Output Format:\nThe output of the CvT-13 model is a set of logits corresponding to the 1,000 different ImageNet classes. These logits represent the raw, unnormalized scores for each class. To determine the predicted class, the class with the highest logit value is selected, indicating the model's confidence in its classification decision. The final output is an index that corresponds to a specific label or category in the model's configuration mapping (id2label), which includes class names such as 'tiger', 'teapot', and 'palace', among many others.\n\n### Use Case Scenarios:\nCvT-13 is suitable for various applications that require robust image classification capabilities. These scenarios include but are not limited to:\n\n1. **Content Tagging**: Automatically assigning tags or categories to images based on their content, which can be useful for sorting and organizing large image libraries.\n   \n2. **Visual Search Enhancements**: Improving image retrieval systems by providing accurate category predictions, enabling better filtering and searching.\n   \n3. **Automated Moderation**: Identifying and classifying inappropriate content by recognizing specific object categories within an image.\n\n4. **Augmented Reality**: Facilitating object recognition tasks within AR environments, allowing systems to overlay information based on recognized objects.\n\n5. **Robotic Vision**: Assisting robots in understanding their environment by classifying visual inputs, crucial for tasks like navigation and interaction with objects.\n\nOverall, CvT-13 offers a powerful, versatile solution for image classification tasks, capable of integrating into systems that require detailed visual understanding and categorization.",
        "accuracy_info": "No accuracy information found.",
        "image_repository_url": "",
        "service_disk_size_bytes": 3609178737.0,
        "profiles": [
            {
                "node_id": "ugurcan.celik",
                "device_type": "DeviceType.CPU",
                "device_name": "cuda",
                "initialization_time_ms": 4206.327676773071,
                "eviction_time_ms": 0.0,
                "initialization_cost": 0.0,
                "keep_alive_cost": 0.0,
                "energy_consumption_idle": 0.0,
                "idle_container_cpu_memory_usage": "1.7GB",
                "idle_container_device_memory_usage": "1.4GB",
                "inference": {
                    "cpu_time_ms": 187.67742800000002,
                    "device_time_ms": 4.507724000000145,
                    "cpu_memory_usage_MB": 0.0,
                    "self_cpu_memory_usage_MB": 0.0,
                    "device_memory_usage_MB": 8.75390625,
                    "self_device_memory_usage_MB": -160.39111328125,
                    "energy_consumption_execution": 0.0,
                    "disk_IO_MB": 0.0,
                    "input_data_MB": 0.0,
                    "output_data_MB": 0.0,
                    "execution_time_ms": 450.3493309020996,
                    "execution_cost": 0.0
                },
                "xai": [
                    {
                        "xai_method": "GradCAM",
                        "cpu_time_ms": 113.98083799999999,
                        "device_time_ms": 2.654031,
                        "cpu_memory_usage_MB": 0.5743179321289062,
                        "self_cpu_memory_usage_MB": 0.28339385986328125,
                        "device_memory_usage_MB": 188.98193359375,
                        "self_device_memory_usage_MB": -49.19970703125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1199.8968919118245,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "HiResCAM",
                        "cpu_time_ms": 56.823784999999994,
                        "device_time_ms": 2.641825999999977,
                        "cpu_memory_usage_MB": 0.5743179321289062,
                        "self_cpu_memory_usage_MB": 0.28339385986328125,
                        "device_memory_usage_MB": 106.3525390625,
                        "self_device_memory_usage_MB": -50.29833984375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1167.1055157979329,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "XGradCAM",
                        "cpu_time_ms": 57.32856699999999,
                        "device_time_ms": 2.6309829999999996,
                        "cpu_memory_usage_MB": 0.5743179321289062,
                        "self_cpu_memory_usage_MB": 0.28339385986328125,
                        "device_memory_usage_MB": 105.32421875,
                        "self_device_memory_usage_MB": -49.32568359375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1247.470219930013,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "GradCAMPlusPlus",
                        "cpu_time_ms": 55.233537,
                        "device_time_ms": 2.6401930000000244,
                        "cpu_memory_usage_MB": 0.5743179321289062,
                        "self_cpu_memory_usage_MB": 0.28339385986328125,
                        "device_memory_usage_MB": 105.32421875,
                        "self_device_memory_usage_MB": -49.32568359375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1208.2901000976562,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "ScoreCAM",
                        "cpu_time_ms": 688.739982,
                        "device_time_ms": 601.0481079999961,
                        "cpu_memory_usage_MB": 110.5372085571289,
                        "self_cpu_memory_usage_MB": -0.00811004638671875,
                        "device_memory_usage_MB": 105.32421875,
                        "self_device_memory_usage_MB": -59424.724609375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 11328.47809791565,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "LayerCAM",
                        "cpu_time_ms": 57.135424,
                        "device_time_ms": 2.681983000000018,
                        "cpu_memory_usage_MB": 0.5743179321289062,
                        "self_cpu_memory_usage_MB": 0.28339385986328125,
                        "device_memory_usage_MB": 105.32421875,
                        "self_device_memory_usage_MB": -49.32568359375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1470.7391262054443,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenCAM",
                        "cpu_time_ms": 40.878375,
                        "device_time_ms": 2.6708910000000374,
                        "cpu_memory_usage_MB": 0.28720855712890625,
                        "self_cpu_memory_usage_MB": -0.00371551513671875,
                        "device_memory_usage_MB": 105.32421875,
                        "self_device_memory_usage_MB": -49.32470703125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 373.94070625305176,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenGradCAM",
                        "cpu_time_ms": 74.246842,
                        "device_time_ms": 2.672365000000027,
                        "cpu_memory_usage_MB": 0.5743179321289062,
                        "self_cpu_memory_usage_MB": 0.28339385986328125,
                        "device_memory_usage_MB": 105.32421875,
                        "self_device_memory_usage_MB": -49.32568359375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1339.2651081085205,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "KPCA_CAM",
                        "cpu_time_ms": 27.958268,
                        "device_time_ms": 2.675367999999991,
                        "cpu_memory_usage_MB": 0.28720855712890625,
                        "self_cpu_memory_usage_MB": -0.00371551513671875,
                        "device_memory_usage_MB": 105.32421875,
                        "self_device_memory_usage_MB": -49.32470703125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 358.0079873402913,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "RandomCAM",
                        "cpu_time_ms": 56.520233999999995,
                        "device_time_ms": 2.6791470000000244,
                        "cpu_memory_usage_MB": 0.5743179321289062,
                        "self_cpu_memory_usage_MB": 0.28339385986328125,
                        "device_memory_usage_MB": 105.32421875,
                        "self_device_memory_usage_MB": -49.32568359375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1302.9780387878418,
                        "execution_cost": 0.0
                    }
                ]
            },
            {
                "node_id": "LAP004262",
                "device_type": "DeviceType.CPU",
                "device_name": "None",
                "initialization_time_ms": 14042.198419570923,
                "eviction_time_ms": 0.0,
                "initialization_cost": 0.0,
                "keep_alive_cost": 0.0,
                "energy_consumption_idle": 0.0,
                "idle_container_cpu_memory_usage": "1.1GB",
                "idle_container_device_memory_usage": "0GB",
                "inference": {
                    "cpu_time_ms": 107.620005,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 0.003814697265625,
                    "self_cpu_memory_usage_MB": -150.95584106445312,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0.0,
                    "disk_IO_MB": 0.0,
                    "input_data_MB": 0.0,
                    "output_data_MB": 0.0,
                    "execution_time_ms": 378.1147797902425,
                    "execution_cost": 0.0
                },
                "xai": [
                    {
                        "xai_method": "GradCAM",
                        "cpu_time_ms": 310.29550700000004,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 179.9972686767578,
                        "self_cpu_memory_usage_MB": -125.101318359375,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1388.8407548268635,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "HiResCAM",
                        "cpu_time_ms": 533.3051209999999,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 103.42582702636719,
                        "self_cpu_memory_usage_MB": -125.101318359375,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1500.4231135050456,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "XGradCAM",
                        "cpu_time_ms": 298.71446499999996,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 103.42582702636719,
                        "self_cpu_memory_usage_MB": -125.101318359375,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1272.3286151885986,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "GradCAMPlusPlus",
                        "cpu_time_ms": 297.55136100000004,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 103.42582702636719,
                        "self_cpu_memory_usage_MB": -125.101318359375,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1492.6209449768066,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "LayerCAM",
                        "cpu_time_ms": 301.05419,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 103.42582702636719,
                        "self_cpu_memory_usage_MB": -125.101318359375,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1290.7211780548096,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenCAM",
                        "cpu_time_ms": 792.635744,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 103.13725280761719,
                        "self_cpu_memory_usage_MB": -48.529869079589844,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1184.9650541941326,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenGradCAM",
                        "cpu_time_ms": 877.229958,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 103.42582702636719,
                        "self_cpu_memory_usage_MB": -125.101318359375,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1823.7172762552898,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "KPCA_CAM",
                        "cpu_time_ms": 119.652857,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 103.13725280761719,
                        "self_cpu_memory_usage_MB": -48.529869079589844,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 377.18860308329266,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "RandomCAM",
                        "cpu_time_ms": 289.278492,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 103.42582702636719,
                        "self_cpu_memory_usage_MB": -125.101318359375,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1445.1959133148193,
                        "execution_cost": 0.0
                    }
                ]
            }
        ],
        "feedback": {
            "likes": [],
            "dislikes": [],
            "comments": []
        },
        "code": {
            "readme_content": "---\nlicense: apache-2.0\ntags:\n- vision\n- image-classification\ndatasets:\n- imagenet-1k\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\n  example_title: Tiger\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg\n  example_title: Teapot\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg\n  example_title: Palace\n---\n\n# Convolutional Vision Transformer (CvT)\n\nCvT-13 model pre-trained on ImageNet-1k at resolution 224x224. It was introduced in the paper [CvT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2103.15808) by Wu et al. and first released in [this repository](https://github.com/microsoft/CvT). \n\nDisclaimer: The team releasing CvT did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Usage\n\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\n\n```python\nfrom transformers import AutoFeatureExtractor, CvtForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/cvt-13')\nmodel = CvtForImageClassification.from_pretrained('microsoft/cvt-13')\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n```",
            "dockerfile_content": "# Base image for AI service powered by HuggingFace pre-trained AI models.\n# the image has the following packages/libraries installed already:\n# - python3.12, pip, git, fastapi, uvicorn, torch, torchvision, opencv-python, transformers, python-multipart, Pillow, requests\nFROM python3.12_ai_service_base:latest\n\n# Set working directory\nWORKDIR /app\n\n# Copy application code\nCOPY . .\n\n# Install additional dependencies\nRUN pip install transformers\n\n# Expose port 8000\nEXPOSE 8000\n\n# Start the FastAPI server\nCMD [\"uvicorn\", \"ai_server:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--timeout-keep-alive\", \"600\"]",
            "ai_server_script_content": "import json\nimport os\nimport time\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import JSONResponse\nfrom contextlib import asynccontextmanager\nfrom ai_server_utils import (\n    PROFILE_OUTPUT_JSON_SPEC,\n    NODE_ID,\n    K8S_POD_NAME,\n)\n\n\n# -------------------------------------------\n# App Lifespan setup\n# -------------------------------------------\n# Record the script start time (when uvicorn starts the process)\nSCRIPT_START_TIME = time.time()\nINITIALIZATION_DURATION = 0.0\nservice_endpoint_specs = {\n    \"model_input_form_spec\": None,\n    \"model_output_json_spec\": None,\n    \"profile_output_json_spec\": None,\n    \"xai_model_input_form_spec\": None,\n    \"xai_model_output_json_spec\": None,\n    \"xai_profile_output_json_spec\": None,\n}\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n\n    global INITIALIZATION_DURATION\n    global SCRIPT_START_TIME\n    global service_endpoint_specs\n\n    # Load the AI model\n    print(\"Loading AI model...\")\n    from model import (\n        MODEL_INPUT_FORM_SPEC,\n        MODEL_OUTPUT_JSON_SPEC,\n        router as model_router,\n    )\n\n    service_endpoint_specs[\"model_input_form_spec\"] = MODEL_INPUT_FORM_SPEC\n    service_endpoint_specs[\"model_output_json_spec\"] = MODEL_OUTPUT_JSON_SPEC\n    service_endpoint_specs[\"profile_output_json_spec\"] = PROFILE_OUTPUT_JSON_SPEC\n\n    app.include_router(model_router, prefix=\"/model\", tags=[\"AI Model\"])\n\n    # Load the XAI model\n    if os.path.exists(os.path.dirname(__file__) + \"/xai_model.py\"):\n        print(\"Loading XAI model...\")\n        from xai_model import (\n            XAI_OUTPUT_JSON_SPEC,\n            router as xai_model_router,\n        )\n\n        # by default, the xai_model input form spec is the same as the model input form spec\n        service_endpoint_specs[\"xai_model_input_form_spec\"] = MODEL_INPUT_FORM_SPEC\n        service_endpoint_specs[\"xai_model_output_json_spec\"] = MODEL_OUTPUT_JSON_SPEC\n        service_endpoint_specs[\"xai_model_output_json_spec\"].update(\n            XAI_OUTPUT_JSON_SPEC\n        )\n        service_endpoint_specs[\"xai_profile_output_json_spec\"] = (\n            PROFILE_OUTPUT_JSON_SPEC\n        )\n        service_endpoint_specs[\"xai_profile_output_json_spec\"].update(\n            XAI_OUTPUT_JSON_SPEC\n        )\n\n        app.include_router(xai_model_router, prefix=\"/xai_model\", tags=[\"XAI Model\"])\n\n    # Record the initialization duration\n    INITIALIZATION_DURATION = time.time() - SCRIPT_START_TIME\n\n    print(f\"AI service loaded in {INITIALIZATION_DURATION:.2f} seconds.\")\n\n    yield\n\n    # Clean up the models and release the resources\n    service_endpoint_specs.clear()\n\n\n# -------------------------------------------\n# FastAPI application setup\n# -------------------------------------------\napp = FastAPI(lifespan=lifespan)\n\n\n# -------------------------------------------\n# Middlewares\n# -------------------------------------------\n@app.middleware(\"http\")\nasync def prepare_header_middleware(request: Request, call_next):\n    start_time = time.perf_counter()\n    response = await call_next(request)\n    process_time = time.perf_counter() - start_time\n    response.headers[\"X-Process-Time\"] = str(process_time)\n    response.headers[\"X-NODE-ID\"] = NODE_ID\n    response.headers[\"X-K8S-POD-NAME\"] = K8S_POD_NAME\n    return response\n\n\n# -------------------------------------------\n# General Endpoints\n# -------------------------------------------\n@app.get(\"/initialization_duration\")\ndef get_initialization_duration():\n    \"\"\"\n    Endpoint to retrieve the initialization duration of the AI model.\n    \"\"\"\n    global INITIALIZATION_DURATION\n\n    if INITIALIZATION_DURATION == 0.0:\n        return JSONResponse(\n            content={\"error\": \"Model not initialized.\"},\n            status_code=500,\n        )\n    return JSONResponse(\n        content={\n            \"initialization_duration\": INITIALIZATION_DURATION,\n            \"script_start_time\": SCRIPT_START_TIME,\n        }\n    )\n\n\n@app.get(\"/help\")\ndef get_help():\n    global service_endpoint_specs\n    help_info = {\n        \"endpoints\": {\n            \"/model/run\": {\n                \"method\": \"POST\",\n                \"description\": \"Executes the AI model with the provided input data.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_output_json_spec\"],\n                },\n            },\n            \"/model/profile_run\": {\n                \"method\": \"POST\",\n                \"description\": \"Profiles the AI model execution.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    \"profile_result\": \"Profiling results of the AI model execution.\",\n                    **service_endpoint_specs[\"profile_output_json_spec\"],\n                },\n            },\n            \"/initialization_duration\": {\n                \"method\": \"GET\",\n                \"description\": \"Retrieves the initialization duration of the AI model.\",\n                \"response\": {\n                    \"initialization_duration\": \"Time taken to initialize the model (in seconds).\",\n                    \"script_start_time\": \"Timestamp when the script started (in seconds since epoch).\",\n                },\n            },\n        }\n    }\n\n    if service_endpoint_specs[\"xai_model_input_form_spec\"] is not None:\n        help_info[\"endpoints\"][\"/xai_model/run\"] = {\n            \"method\": \"POST\",\n            \"description\": \"Executes the XAI model with the provided input data.\",\n            \"parameters\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_input_form_spec\"],\n            },\n            \"response\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_output_json_spec\"],\n            },\n        }\n\n        help_info[\"endpoints\"][\"/xai_model/profile_run\"] = {\n            \"method\": \"POST\",\n            \"description\": \"Profiles the XAI model execution.\",\n            \"parameters\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_input_form_spec\"],\n            },\n            \"response\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                \"profile_result\": \"Profiling results of the XAI model execution.\",\n                **service_endpoint_specs[\"xai_profile_output_json_spec\"],\n            },\n        }\n    \n    return JSONResponse(content=help_info)\n",
            "ai_client_script_content": "import base64\nimport json\nimport time\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nfrom ai_client_utils import (\n    prepare_ai_service_request_files,\n    prepare_ai_service_request_data,\n)\n\nXAI_GRADCAM_METHODS = [\n    \"GradCAM\",\n    \"HiResCAM\",\n    # \"AblationCAM\",\n    \"XGradCAM\",\n    \"GradCAMPlusPlus\",\n    \"ScoreCAM\",\n    \"LayerCAM\",\n    \"EigenCAM\",\n    \"EigenGradCAM\",\n    \"KPCA_CAM\",\n    \"RandomCAM\",\n]\n\n# -------------------------------------\n# prompt for necessary inputs\n# -------------------------------------\nSERVER_URL = input(\"Please input server URL (default to http://localhost:9000): \")\nif SERVER_URL.strip() == \"\":\n    SERVER_URL = \"http://localhost:9000\"\nUE_ID = input(\"Please input UE_ID (default to 123456): \")\nif UE_ID.strip() == \"\":\n    UE_ID = \"123456\"\n\n\ndef send_post_request(url, data, files):\n    \"\"\"Send request to run AI service and display AI service responses.\"\"\"\n    try:\n        response = requests.post(url, files=files, data=data)\n        # get the process time, node id and k8s pod name from the response headers\n        process_time = response.headers.get(\"X-Process-Time\")\n        node_id = response.headers.get(\"X-NODE-ID\")\n        k8s_pod_name = response.headers.get(\"X-K8S-POD-NAME\")\n        if response.status_code == 200:\n            return response.json(), process_time, node_id, k8s_pod_name\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n\ndef send_get_request(url, params=None):\n    \"\"\"Send GET request to the specified URL and return the response.\"\"\"\n    try:\n        response = requests.get(url, params=params)\n        process_time = response.headers.get(\"X-Process-Time\")\n        node_id = response.headers.get(\"X-NODE-ID\")\n        k8s_pod_name = response.headers.get(\"X-K8S-POD-NAME\")\n        if response.status_code == 200:\n            return response.json(), process_time, node_id, k8s_pod_name\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n            return None\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n\nclass ProfileResultProcessor:\n\n    def __init__(self, server_url):\n        self.server_url = server_url\n        self.start_time = time.time()\n        self.service_initialization_duration = 0\n        self.response_counter = 0\n        self.profile_name = None\n        self.device_type = None\n        self.device_name = None\n        self.node_id = None\n        self.k8s_pod_name = None\n        self.cpu_memory_usage_bytes = 0\n        self.self_cpu_memory_usage_bytes = 0\n        self.device_memory_usage_bytes = 0\n        self.self_device_memory_usage_bytes = 0\n        self.cpu_time_total_us = 0\n        self.self_cpu_time_total_us = 0\n        self.device_time_total_us = 0\n        self.self_device_time_total_us = 0\n\n        # xai related\n        self.gradcam_method_name = None\n\n        self.fetch_service_initialization_duration()\n\n    def fetch_service_initialization_duration(self):\n        \"\"\"Fetch the service initialization duration from the server.\"\"\"\n        response, process_time, node_id, k8s_pod_name = send_get_request(\n            f\"{self.server_url}/initialization_duration\"\n        )\n        if response:\n            self.service_initialization_duration = response.get(\n                \"initialization_duration\", 0\n            )\n        else:\n            print(\"Failed to fetch initialization duration.\")\n            self.service_initialization_duration = 0\n\n    def process_new_response(\n        self,\n        profile_response,\n        process_time=None,\n        node_id=None,\n        k8s_pod_name=None,\n        gradcam_method_name=None,\n    ):\n        if not profile_response:\n            return\n\n        profile_result = profile_response[\"profile_result\"]\n\n        if not profile_result:\n            return\n\n        if self.profile_name is None:\n            self.profile_name = profile_result[\"name\"]\n        if self.device_type is None:\n            self.device_type = profile_result[\"device_type\"]\n        if self.device_name is None:\n            self.device_name = profile_result[\"device_name\"]\n        if self.node_id is None:\n            self.node_id = node_id\n        if self.k8s_pod_name is None:\n            self.k8s_pod_name = k8s_pod_name\n\n        if self.gradcam_method_name is None:\n            self.gradcam_method_name = gradcam_method_name\n\n        # update the max profile result\n        self.cpu_memory_usage_bytes = max(\n            self.cpu_memory_usage_bytes, profile_result[\"cpu_memory_usage\"]\n        )\n        # self cpu memory usage could be negative. here we take the value that has the max absolute value\n        if abs(profile_result[\"self_cpu_memory_usage\"]) > abs( self.self_cpu_memory_usage_bytes):\n            self.self_cpu_memory_usage_bytes = profile_result[\"self_cpu_memory_usage\"]\n        self.device_memory_usage_bytes = max(\n            self.device_memory_usage_bytes, profile_result[\"device_memory_usage\"]\n        )\n        # same as self cpu memory usage\n        if abs(profile_result[\"self_device_memory_usage\"]) > abs(self.self_device_memory_usage_bytes):\n            self.self_device_memory_usage_bytes = profile_result[\"self_device_memory_usage\"]\n        self.cpu_time_total_us = max(\n            self.cpu_time_total_us, profile_result[\"cpu_time_total\"]\n        )\n        self.self_cpu_time_total_us = max(\n            self.self_cpu_time_total_us, profile_result[\"self_cpu_time_total\"]\n        )\n        self.device_time_total_us = max(\n            self.device_time_total_us, profile_result[\"device_time_total\"]\n        )\n        self.self_device_time_total_us = max(\n            self.self_device_time_total_us, profile_result[\"self_device_time_total\"]\n        )\n\n        self.response_counter += 1\n\n    def complete_profile(self):\n        print(\"\\n--------- PROFILE EVENT ---------\\n\")\n        print(f\"Name: {self.profile_name}\")\n        print(f\"Device Type: {self.device_type}\")\n        print(f\"Device Name: {self.device_name}\")\n        print(f\"Node ID: {self.node_id}\")\n        print(f\"K8S_POD_NAME: {self.k8s_pod_name}\")\n\n        if self.gradcam_method_name:\n            print(f\"GradCAM Method Name: {self.gradcam_method_name}\")\n\n        print(\"\\n--------- LATENCY RESULT ---------\\n\")\n        print(\"Service Initialization Duration: \", self.service_initialization_duration)\n        print(\"Total Requests: \", self.response_counter)\n        print(f\"Total Time Taken: {time.time() - self.start_time:.2f} seconds\")\n        print(\n            f\"Average Time Taken: {(time.time() - self.start_time) / self.response_counter:.2f} seconds\"\n        )\n\n        print(\"\\n--------- RESOURCE USAGE ---------\\n\")\n        print(f\"CPU Memory Usage: {self.cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\")\n        print(\n            f\"Self CPU Memory Usage: {self.self_cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Device Memory Usage: {self.device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Self Device Memory Usage: {self.self_device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(f\"CPU Time Total: {self.cpu_time_total_us / 1000:.2f} ms\")\n        print(f\"Self CPU Time Total: {self.self_cpu_time_total_us / 1000:.2f} ms\")\n        print(f\"Device Time Total: {self.device_time_total_us / 1000:.2f} ms\")\n        print(f\"Self Device Time Total: {self.self_device_time_total_us / 1000:.2f} ms\")\n\n        # update the service_data.json automatically\n        with open(\"service_data.json\", \"r\") as f:\n            service_data = json.load(f)\n\n        if not self.gradcam_method_name:\n            complete_profile_data_to_save = {\n                \"node_id\": self.node_id,\n                \"device_type\": self.device_type,\n                \"device_name\": self.device_name,\n                \"initialization_time_ms\": self.service_initialization_duration * 1000,\n                \"eviction_time_ms\": 0,\n                \"initialization_cost\": 0,\n                \"keep_alive_cost\": 0,\n                \"energy_consumption_idle\": 0,\n                \"inference\": {\n                    \"cpu_time_ms\": self.cpu_time_total_us / 1000,\n                    \"device_time_ms\": self.device_time_total_us / 1000,\n                    \"cpu_memory_usage_MB\": self.cpu_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"self_cpu_memory_usage_MB\": self.self_cpu_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"device_memory_usage_MB\": self.device_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"self_device_memory_usage_MB\": self.self_device_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"energy_consumption_execution\": 0,\n                    \"disk_IO_MB\": 0,\n                    \"input_data_MB\": 0,\n                    \"output_data_MB\": 0,\n                    \"execution_time_ms\": (time.time() - self.start_time)\n                    / self.response_counter\n                    * 1000,\n                    \"execution_cost\": 0,\n                },\n            }\n\n            # check if there is already a profile for this node id\n            profile_found = False\n            for profile in service_data[\"profiles\"]:\n                if profile[\"node_id\"] == self.node_id:\n                    profile[\"inference\"] = complete_profile_data_to_save[\"inference\"]\n                    profile_found = True\n                    break\n            if not profile_found:\n                service_data[\"profiles\"].append(complete_profile_data_to_save)\n\n            # save the updated service_data.json\n            with open(\"service_data.json\", \"w\") as f:\n                json.dump(service_data, f, indent=4)\n            print(\"\\n--------- SERVICE DATA UPDATED ---------\\n\")\n\n        else:\n            complete_xai_profile_data_to_save = {\n                \"node_id\": self.node_id,\n                \"device_type\": self.device_type,\n                \"device_name\": self.device_name,\n                \"initialization_time_ms\": self.service_initialization_duration * 1000,\n                \"eviction_time_ms\": 0,\n                \"initialization_cost\": 0,\n                \"keep_alive_cost\": 0,\n                \"energy_consumption_idle\": 0,\n                \"xai\": [\n                    {\n                        \"xai_method\": self.gradcam_method_name,\n                        \"cpu_time_ms\": self.cpu_time_total_us / 1000,\n                        \"device_time_ms\": self.device_time_total_us / 1000,\n                        \"cpu_memory_usage_MB\": self.cpu_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"self_cpu_memory_usage_MB\": self.self_cpu_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"device_memory_usage_MB\": self.device_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"self_device_memory_usage_MB\": self.self_device_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"energy_consumption_execution\": 0,\n                        \"disk_IO_MB\": 0,\n                        \"input_data_MB\": 0,\n                        \"output_data_MB\": 0,\n                        \"execution_time_ms\": (time.time() - self.start_time)\n                        / self.response_counter\n                        * 1000,\n                        \"execution_cost\": 0,\n                    }\n                ],\n            }\n\n            # check if there is already a profile for this node id\n            profile_found = False\n            for profile in service_data[\"profiles\"]:\n                if profile[\"node_id\"] == self.node_id:\n                    profile_found = True\n\n                    # check if there is already a profile for this xai method\n                    xai_method_found = False\n                    if not profile.get(\"xai\"):\n                        profile[\"xai\"] = []\n                    for xai_profile in profile[\"xai\"]:\n                        if xai_profile[\"xai_method\"] == self.gradcam_method_name:\n                            xai_profile.update(complete_xai_profile_data_to_save[\"xai\"][0]) \n                            xai_method_found = True\n                            break\n                    if not xai_method_found:\n                        profile[\"xai\"].append(complete_xai_profile_data_to_save[\"xai\"][0])\n                    break\n\n            if not profile_found:\n                service_data[\"profiles\"].append(complete_xai_profile_data_to_save)\n\n            # save the updated service_data.json\n            with open(\"service_data.json\", \"w\") as f:\n                json.dump(service_data, f, indent=4)\n            print(\"\\n--------- SERVICE DATA UPDATED ---------\\n\")\n\n\ndef option_run():\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n    data = {**data, \"ue_id\": UE_ID}\n    response, process_time, node_id, pod_name = send_post_request(\n        f\"{SERVER_URL}/model/run\", data, files\n    )\n    print(\"Process Time: \", process_time)\n    print(\"Node ID: \", node_id)\n    print(\"K8S_POD_NAME: \", pod_name)\n    print(\"Response\")\n    print(json.dumps(response, indent=4))\n\n\ndef option_profile_run():\n    data = prepare_ai_service_request_data()\n    data = {**data, \"ue_id\": UE_ID}\n    files = prepare_ai_service_request_files()\n    num_requests = int(input(\"Enter the number of requests to send: \"))\n\n    try:\n        profile_result_processor = ProfileResultProcessor(SERVER_URL)\n\n        for _ in range(num_requests):\n            profile_response, process_time, node_id, k8s_node_name = send_post_request(\n                f\"{SERVER_URL}/model/profile_run\", data, files\n            )\n            print(\"Process Time: \", process_time)\n            print(\"Node ID: \", node_id)\n            print(\"K8S_POD_NAME: \", k8s_node_name)\n            print(\"Response\")\n            print(json.dumps(profile_response, indent=4))\n            if not profile_response:\n                print(\"No profile response received.\")\n                continue\n\n            profile_result_processor.process_new_response(\n                profile_response,\n                process_time=process_time,\n                node_id=node_id,\n                k8s_pod_name=k8s_node_name,\n            )\n\n        # Print the final profile result and update the service_data.json\n        profile_result_processor.complete_profile()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\n\ndef option_help():\n    \"\"\"Get help information from the server.\"\"\"\n    try:\n        response, process_time, node_id, pod_name = send_get_request(\n            f\"{SERVER_URL}/help\"\n        )\n        print(\"Process Time: \", process_time)\n        print(\"Node ID: \", node_id)\n        print(\"K8S_POD_NAME: \", pod_name)\n        print(\"Response\")\n        print(json.dumps(response, indent=4))\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n\n\ndef option_run_with_xai():\n    \"\"\"Run AI service with XAI.\"\"\"\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n\n    print(\n        \"Note that currently only GradCAM methods on image-classification models are supported.\"\n    )\n    while True:\n        gradcam_method_name = input(\n            f\"Please select a GradCAM method (options: {XAI_GRADCAM_METHODS}): \"\n        )\n        if gradcam_method_name not in XAI_GRADCAM_METHODS:\n            print(f\"Invalid GradCAM method. Please select again.\")\n        else:\n            break\n\n    # ask for target class for explanation\n    target_category_indexes = input(\n        \"Please input target category indexes for explanation (comma-separated, e.g., 111, 32, 44, ...): \"\n    )\n    if not target_category_indexes or not target_category_indexes.strip():\n        print(\n            \"No target category indexes provided. Defaulting to explaining the top confident category.\"\n        )\n        target_category_indexes = []\n    else:\n        target_category_indexes = [\n            int(i.strip()) for i in target_category_indexes.split(\",\")\n        ]\n\n    data = {\n        **data,\n        \"ue_id\": UE_ID,\n        \"gradcam_method_name\": gradcam_method_name,\n        \"target_category_indexes\": target_category_indexes,\n    }\n    print(\"Data: \", data)\n    response, process_time, node_id, k8s_pod_name = send_post_request(\n        f\"{SERVER_URL}/xai_model/run\", data, files\n    )\n    print(\"Process Time: \", process_time)\n    print(\"Node ID: \", node_id)\n    print(\"K8S_POD_NAME: \", k8s_pod_name)\n\n    # Handle JSON response\n    model_results = response.get(\"model_results\")\n    if model_results:\n        print(\"Model Results:\", json.dumps(model_results, indent=4))\n\n    xai_results = response.get(\"xai_results\")\n    if xai_results:\n        print(\"XAI Results Method:\", xai_results.get(\"xai_method\"))\n\n    # Handle binary image response\n    encoded_image = xai_results.get(\"image\")\n    if encoded_image:\n        image_bytes = base64.b64decode(encoded_image)\n\n        # Load the image into Pillow\n        image = Image.open(BytesIO(image_bytes))\n\n        # Save image to disk\n        image.save(\"xai_output.png\")\n\n        # Display the image using matplotlib\n        plt.imshow(image)\n        plt.axis(\"off\")\n        plt.show()\n\n\ndef option_profile_run_with_xai():\n    \"\"\"Run AI service with XAI and profile the run.\"\"\"\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n    print(\n        \"Note that currently only GradCAM methods on image-classification models are supported.\"\n    )\n\n    # ask for target class for explanation\n    target_category_indexes = input(\n        \"Please input target category indexes for explanation (comma-separated, e.g., 111, 32, 44, ...): \"\n    )\n    if not target_category_indexes or not target_category_indexes.strip():\n        print(\n            \"No target category indexes provided. Defaulting to explaining the top confident category.\"\n        )\n        target_category_indexes = []\n    else:\n        target_category_indexes = [\n            int(i.strip()) for i in target_category_indexes.split(\",\")\n        ]\n\n    num_requests = int(input(\"Enter the number of requests to send: \"))\n\n    try:\n        for gradcam_method_name in XAI_GRADCAM_METHODS:\n\n            data = {\n                **data,\n                \"ue_id\": UE_ID,\n                \"gradcam_method_name\": gradcam_method_name,\n                \"target_category_indexes\": target_category_indexes,\n            }\n            print(\"Data: \", data)\n\n            profile_result_processor = ProfileResultProcessor(SERVER_URL)\n\n            for _ in range(num_requests):\n                response, process_time, node_id, k8s_pod_name = send_post_request(\n                    f\"{SERVER_URL}/xai_model/profile_run\", data, files\n                )\n                print(\"Process Time: \", process_time)\n                print(\"Node ID: \", node_id)\n                print(\"K8S_POD_NAME: \", k8s_pod_name)\n                if not response:\n                    print(\"No profile response received.\")\n                    continue\n\n                # Handle JSON response\n                model_results = response.get(\"model_results\")\n                if model_results:\n                    print(\"Model Results:\", json.dumps(model_results, indent=4))\n\n                profile_result_processor.process_new_response(\n                    response,\n                    process_time=process_time,\n                    node_id=node_id,\n                    k8s_pod_name=k8s_pod_name,\n                    gradcam_method_name=gradcam_method_name,\n                )\n\n            # Print the final profile result and update the service_data.json\n            profile_result_processor.complete_profile()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\n\nOPTIONS = [\n    {\n        \"label\": \"Get help information\",\n        \"action\": option_help,\n    },\n    {\n        \"label\": \"Run AI service\",\n        \"action\": option_run,\n    },\n    {\n        \"label\": \"Profile AI service\",\n        \"action\": option_profile_run,\n    },\n    {\n        \"label\": \"Run AI service with XAI (only image-classification models)\",\n        \"action\": option_run_with_xai,\n    },\n    {\n        \"label\": \"Profile AI service with XAI (only image-classification models)\",\n        \"action\": option_profile_run_with_xai,\n    },\n]\n\n\nif __name__ == \"__main__\":\n    while True:\n        print(\"\\nOptions:\")\n        for i, option in enumerate(OPTIONS, start=1):\n            print(f\"{i}. {option['label']}\")\n        print(\"q. Quit\")\n        choice = input(\"Enter your choice: \")\n\n        if choice == \"q\":\n            print(\"Exiting the client. Goodbye!\")\n            break\n        else:\n            try:\n                choice = int(choice)\n                if 1 <= choice <= len(OPTIONS):\n                    OPTIONS[choice - 1][\"action\"]()\n                else:\n                    print(\"Invalid choice. Please try again.\")\n            except ValueError:\n                print(\"Invalid input. Please enter a number.\")\n",
            "ai_client_utils_script_content": "def prepare_ai_service_request_files():\n    \"\"\"Prepare the `files` part for the AI service request.\"\"\"\n    files = {}\n    image_file_path = input(\"Please input the image file path: \")\n    with open(image_file_path, \"rb\") as image_file:\n        files[\"file\"] = image_file.read()\n    return files\n\n\ndef prepare_ai_service_request_data():\n    \"\"\"Prepare the `data` part other than `ue_id` for the AI service request.\"\"\"\n    data = {}\n    # Adding the ue_id to the data\n    ue_id = input(\"Please input the unique execution ID (ue_id): \")\n    data[\"ue_id\"] = ue_id\n    return data",
            "model_script_content": "# import server utils\nfrom ai_server_utils import (\n    process_model_output_logits,\n    profile_activities,\n    prepare_profile_results,\n)\n# import profile utils\nfrom torch.profiler import profile, record_function\n\n# import necessary libs for AI model inference and request handling\nimport torch\nfrom fastapi import APIRouter, File, Form, UploadFile\nfrom fastapi.responses import JSONResponse\nfrom transformers import AutoImageProcessor, CvtForImageClassification\nfrom PIL import Image\n\n# --------------------------------\n# Device configuration\n# --------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# --------------------------------\n# Model-specific configuration\n# make sure the variables `MODEL_NAME` and `model` are defined here.\n# --------------------------------\nMODEL_NAME = \"microsoft/cvt-13\"\nprocessor = AutoImageProcessor.from_pretrained(MODEL_NAME)\nmodel = CvtForImageClassification.from_pretrained(MODEL_NAME).to(device)\nmodel.eval()\n\n# Initialize the FastAPI router\nrouter = APIRouter()\n\n@router.post(\"/run\")\nasync def run_model(file: UploadFile = File(...), ue_id: str = Form(...)):\n    try:\n        # Prepare the model input\n        image = Image.open(file.file).convert(\"RGB\")\n        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n\n        # Perform inference\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        # Process the model outputs\n        predictions = process_model_output_logits(model, outputs.logits)\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"model_results\": predictions,\n            }\n        )\n    except Exception as e:\n        print(f\"Error processing file: {e}\")\n        return JSONResponse(\n            content={\"error\": \"Failed to process the image. {e}\".format(e=str(e))},\n            status_code=500,\n        )\n\n@router.post(\"/profile_run\")\nasync def profile_run(file: UploadFile = File(...), ue_id: str = Form(...)):\n    \"\"\"\n    Endpoint to profile the AI model execution.\n    \"\"\"\n    try:\n        # Prepare the model input\n        image = Image.open(file.file).convert(\"RGB\")\n        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n\n        # perform profiling\n        with profile(\n            activities=profile_activities,\n            profile_memory=True,\n        ) as prof:\n            with record_function(\"model_run\"):\n                with torch.no_grad():\n                    model_outputs = model(**inputs)\n\n        profile_result = prepare_profile_results(prof)\n\n        # Process the model outputs\n        predictions = process_model_output_logits(model, model_outputs.logits)\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"profile_result\": profile_result,\n                \"model_results\": predictions,\n            }\n        )\n\n    except Exception as e:\n        print(f\"Error processing request: {e}\")\n        return JSONResponse(\n            content={\"error\": f\"Failed to process the request. {e}\"},\n            status_code=500,\n        )\n\n# Below are the model input and output specifications to be used by the `/help` endpoint\nMODEL_INPUT_FORM_SPEC = {\n    \"file\": {\n        \"type\": \"file upload\",\n        \"description\": \"The image file to be classified.\",\n        \"required\": True,\n        \"example\": \"puppy.png\",\n    }\n}\n\nMODEL_OUTPUT_JSON_SPEC = {\n    \"ue_id\": \"unique execution ID\",\n    \"model_results\": [\n        {\n            \"category_id\": \"category id\",\n            \"label\": \"category label\",\n            \"probability\": \"probability value\",\n        }\n    ],\n}",
            "healthcheck_script_content": "import requests\n\nprint(requests.get(\"http://localhost:8000/help\"))"
        }
    },
    {
        "model_name": "microsoft/swin-large-patch4-window12-384-in22k",
        "model_url": "https://huggingface.co/microsoft/swin-large-patch4-window12-384-in22k",
        "task": "image-classification",
        "task_detail": "The \"microsoft/swin-large-patch4-window12-384-in22k\" model is a pre-trained Swin Transformer designed primarily for image classification tasks. This model has been trained on the expansive ImageNet-21k dataset, which comprises 14 million images distributed across 21,841 distinct classes, and it operates at a resolution of 384x384 pixels.\n\n**Functionality Overview:**\n\n1. **Classification Type:** The Swin Transformer model is utilized for image classification. It is excellent for determining labels from diverse, real-world image data due to its training on the extensive ImageNet-21k dataset. This permits the model to classify a broad spectrum of objects, animals, scenes, and inanimate items effectively, making it well-suited for numerous application scenarios across different domains such as industrial inspection, wildlife monitoring, and automated tagging.\n\n2. **Hierarchical Vision Transformations:** The model employs a hierarchical vision transformer architecture, which involves processing images with attention mechanisms confined to local shifted window operations rather than global attention. This leads to efficient hierarchical feature extraction from images by gradually merging image patches into higher-level features. The approach allows it to serve as a reliable backbone for both image classification and more complex dense prediction tasks.\n\n3. **Input Format:** This model expects images as input, formulated as 384x384 resolution JPEG files. The input images undergo a feature extraction process guided by an auto feature extractor that converts the raw image into a format suitable for the model, ensuring that the nuanced textures and shapes within an image are optimally represented for classification.\n\n4. **Output Format:** Upon processing an input image, the model outputs logits, which are essentially raw prediction values for each of the ImageNet classes. These logits are further interpreted to identify the class with the highest probability, corresponding to the most likely label for the image. The model can predict one class out of the 1,000 ImageNet classes when deployed, offering a highly granular classification result.\n\n5. **Scalability and Usability:** This model can be deployed as a standalone classifier for specific tasks or further fine-tuned for custom image classification challenges, wherever specialized or niche categories are involved beyond the ImageNet classes. The Swin Transformer's efficient computation and extensive training afford it the flexibility to integrate into larger, multifaceted AI systems requiring robust visual intelligence.\n\nIn conclusion, the microsoft/swin-large-patch4-window12-384-in22k model embodies a powerful tool for image classification, offering a comprehensive, adaptable solution thanks to its advanced transformer-based architecture and large-scale pre-training.",
        "accuracy_info": "No accuracy information found.",
        "image_repository_url": "",
        "service_disk_size_bytes": 3609260287.0,
        "profiles": [
            {
                "node_id": "ugurcan.celik",
                "device_type": "DeviceType.CPU",
                "device_name": "cuda",
                "initialization_time_ms": 12871.289253234863,
                "eviction_time_ms": 0.0,
                "initialization_cost": 0.0,
                "keep_alive_cost": 0.0,
                "energy_consumption_idle": 0.0,
                "idle_container_cpu_memory_usage": "7.1GB",
                "idle_container_device_memory_usage": "9.6GB",
                "inference": {
                    "cpu_time_ms": 264.75786100000005,
                    "device_time_ms": 35.688520999998715,
                    "cpu_memory_usage_MB": 0.0,
                    "self_cpu_memory_usage_MB": 0.0,
                    "device_memory_usage_MB": 8.20849609375,
                    "self_device_memory_usage_MB": -2512.17578125,
                    "energy_consumption_execution": 0.0,
                    "disk_IO_MB": 0.0,
                    "input_data_MB": 0.0,
                    "output_data_MB": 0.0,
                    "execution_time_ms": 803.0814329783121,
                    "execution_cost": 0.0
                },
                "xai": [
                    {
                        "xai_method": "GradCAM",
                        "cpu_time_ms": 178.561494,
                        "device_time_ms": 30.03042900000004,
                        "cpu_memory_usage_MB": 1.68768310546875,
                        "self_cpu_memory_usage_MB": 0.7606163024902344,
                        "device_memory_usage_MB": 2020.78515625,
                        "self_device_memory_usage_MB": -1534.873046875,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 2187.6978079477944,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "HiResCAM",
                        "cpu_time_ms": 93.539432,
                        "device_time_ms": 29.606142999999932,
                        "cpu_memory_usage_MB": 1.68768310546875,
                        "self_cpu_memory_usage_MB": 0.7606163024902344,
                        "device_memory_usage_MB": 1161.43994140625,
                        "self_device_memory_usage_MB": -1519.623046875,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 2245.9565003712974,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "XGradCAM",
                        "cpu_time_ms": 97.085808,
                        "device_time_ms": 29.863987000000023,
                        "cpu_memory_usage_MB": 1.68768310546875,
                        "self_cpu_memory_usage_MB": 0.7606163024902344,
                        "device_memory_usage_MB": 1156.57275390625,
                        "self_device_memory_usage_MB": -1513.130859375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 2214.8424784342446,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "GradCAMPlusPlus",
                        "cpu_time_ms": 94.978024,
                        "device_time_ms": 30.175570000000057,
                        "cpu_memory_usage_MB": 1.68768310546875,
                        "self_cpu_memory_usage_MB": 0.7606163024902344,
                        "device_memory_usage_MB": 1158.00244140625,
                        "self_device_memory_usage_MB": -1529.677734375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 2355.3999265034995,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "ScoreCAM",
                        "cpu_time_ms": 49832.035416,
                        "device_time_ms": 49674.01539900074,
                        "cpu_memory_usage_MB": 1296.8439331054688,
                        "self_cpu_memory_usage_MB": -0.10073089599609375,
                        "device_memory_usage_MB": 1158.74462890625,
                        "self_device_memory_usage_MB": -3549129.1142578125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 122872.23895390828,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "LayerCAM",
                        "cpu_time_ms": 92.585746,
                        "device_time_ms": 29.838417000000018,
                        "cpu_memory_usage_MB": 1.68768310546875,
                        "self_cpu_memory_usage_MB": 0.7606163024902344,
                        "device_memory_usage_MB": 1156.81494140625,
                        "self_device_memory_usage_MB": -1530.583984375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1998.7366994222004,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenCAM",
                        "cpu_time_ms": 256.798784,
                        "device_time_ms": 29.954633999999956,
                        "cpu_memory_usage_MB": 0.84393310546875,
                        "self_cpu_memory_usage_MB": -0.08313369750976562,
                        "device_memory_usage_MB": 1156.65087890625,
                        "self_device_memory_usage_MB": -1515.2705078125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 897.2539901733398,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenGradCAM",
                        "cpu_time_ms": 306.67354600000004,
                        "device_time_ms": 30.013614999999977,
                        "cpu_memory_usage_MB": 1.68768310546875,
                        "self_cpu_memory_usage_MB": 0.7606163024902344,
                        "device_memory_usage_MB": 1156.89306640625,
                        "self_device_memory_usage_MB": -1519.505859375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 4347.517013549805,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "KPCA_CAM",
                        "cpu_time_ms": 45.168487999999996,
                        "device_time_ms": 29.90163100000007,
                        "cpu_memory_usage_MB": 0.84393310546875,
                        "self_cpu_memory_usage_MB": -0.08313369750976562,
                        "device_memory_usage_MB": 1155.34619140625,
                        "self_device_memory_usage_MB": -1520.4892578125,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 871.532122294108,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "RandomCAM",
                        "cpu_time_ms": 93.01871299999999,
                        "device_time_ms": 30.022722000000023,
                        "cpu_memory_usage_MB": 1.68768310546875,
                        "self_cpu_memory_usage_MB": 0.7606163024902344,
                        "device_memory_usage_MB": 1155.79931640625,
                        "self_device_memory_usage_MB": -1520.794921875,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 2314.0013217926025,
                        "execution_cost": 0.0
                    }
                ]
            },
            {
                "node_id": "LAP004262",
                "device_type": "DeviceType.CPU",
                "device_name": "None",
                "initialization_time_ms": 110842.26989746094,
                "eviction_time_ms": 0.0,
                "initialization_cost": 0.0,
                "keep_alive_cost": 0.0,
                "energy_consumption_idle": 0.0,
                "idle_container_cpu_memory_usage": "5.9GB",
                "idle_container_device_memory_usage": "0GB",
                "inference": {
                    "cpu_time_ms": 1255.0337260000001,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 0.08331680297851562,
                    "self_cpu_memory_usage_MB": -2419.00634765625,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0.0,
                    "disk_IO_MB": 0.0,
                    "input_data_MB": 0.0,
                    "output_data_MB": 0.0,
                    "execution_time_ms": 1839.2037550608318,
                    "execution_cost": 0.0
                },
                "xai": [
                    {
                        "xai_method": "GradCAM",
                        "cpu_time_ms": 4302.723683,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 1980.3791732788086,
                        "self_cpu_memory_usage_MB": -2339.087734222412,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 6248.7367788950605,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "HiResCAM",
                        "cpu_time_ms": 3749.5933600000003,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 1107.6980018615723,
                        "self_cpu_memory_usage_MB": -2339.087734222412,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 5740.231990814209,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "XGradCAM",
                        "cpu_time_ms": 3797.214907,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 1107.6980018615723,
                        "self_cpu_memory_usage_MB": -2339.087734222412,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 5800.575335820516,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "GradCAMPlusPlus",
                        "cpu_time_ms": 4631.2502890000005,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 1107.6980018615723,
                        "self_cpu_memory_usage_MB": -2339.087734222412,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 6484.484275182088,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "LayerCAM",
                        "cpu_time_ms": 4007.405119,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 1107.6980018615723,
                        "self_cpu_memory_usage_MB": -2339.087734222412,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 5690.09788831075,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenCAM",
                        "cpu_time_ms": 1895.4330569999997,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 1106.8542518615723,
                        "self_cpu_memory_usage_MB": -1466.4065551757812,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 2095.7268873850503,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenGradCAM",
                        "cpu_time_ms": 3927.2000379999995,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 1107.6980018615723,
                        "self_cpu_memory_usage_MB": -2339.087734222412,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 5393.364508946736,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "KPCA_CAM",
                        "cpu_time_ms": 1144.642305,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 1106.8542518615723,
                        "self_cpu_memory_usage_MB": -1466.4065551757812,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1860.0891431172688,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "RandomCAM",
                        "cpu_time_ms": 3086.605591,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 1107.6980018615723,
                        "self_cpu_memory_usage_MB": -2339.087734222412,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 4680.584748586019,
                        "execution_cost": 0.0
                    }
                ]
            }
        ],
        "feedback": {
            "likes": [],
            "dislikes": [],
            "comments": []
        },
        "code": {
            "readme_content": "---\nlicense: apache-2.0\ntags:\n- vision\n- image-classification\ndatasets:\n- imagenet-21k\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\n  example_title: Tiger\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg\n  example_title: Teapot\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg\n  example_title: Palace\n---\n\n# Swin Transformer (large-sized model) \n\nSwin Transformer model pre-trained on ImageNet-21k (14 million images, 21,841 classes) at resolution 384x384. It was introduced in the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Liu et al. and first released in [this repository](https://github.com/microsoft/Swin-Transformer). \n\nDisclaimer: The team releasing Swin Transformer did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThe Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks. In contrast, previous vision Transformers produce feature maps of a single low resolution and have quadratic computation complexity to input image size due to computation of self-attention globally.\n\n![model image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/swin_transformer_architecture.png)\n\n[Source](https://paperswithcode.com/method/swin-transformer)\n\n## Intended uses & limitations\n\nYou can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=swin) to look for\nfine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\n\n```python\nfrom transformers import AutoFeatureExtractor, SwinForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/swin-large-patch4-window12-384-in22k\")\nmodel = SwinForImageClassification.from_pretrained(\"microsoft/swin-large-patch4-window12-384-in22k\")\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n```\n\nFor more code examples, we refer to the [documentation](https://huggingface.co/transformers/model_doc/swin.html#).\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-2103-14030,\n  author    = {Ze Liu and\n               Yutong Lin and\n               Yue Cao and\n               Han Hu and\n               Yixuan Wei and\n               Zheng Zhang and\n               Stephen Lin and\n               Baining Guo},\n  title     = {Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},\n  journal   = {CoRR},\n  volume    = {abs/2103.14030},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2103.14030},\n  eprinttype = {arXiv},\n  eprint    = {2103.14030},\n  timestamp = {Thu, 08 Apr 2021 07:53:26 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-14030.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```",
            "dockerfile_content": "# Base image for AI service powered by HuggingFace pre-trained AI models.\n# the image has the following packages/libraries installed already:\n# - python3.12, pip, git, fastapi, uvicorn, torch, torchvision, opencv-python, transformers, python-multipart, Pillow, requests\nFROM python3.12_ai_service_base:latest\n\n# Set working directory\nWORKDIR /app\n\n# Copy application code\nCOPY . .\n\n# Expose port 8000\nEXPOSE 8000\n\n# Start the FastAPI server\nCMD [\"uvicorn\", \"ai_server:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--timeout-keep-alive\", \"600\"]",
            "ai_server_script_content": "import json\nimport os\nimport time\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import JSONResponse\nfrom contextlib import asynccontextmanager\nfrom ai_server_utils import (\n    PROFILE_OUTPUT_JSON_SPEC,\n    NODE_ID,\n    K8S_POD_NAME,\n)\n\n\n# -------------------------------------------\n# App Lifespan setup\n# -------------------------------------------\n# Record the script start time (when uvicorn starts the process)\nSCRIPT_START_TIME = time.time()\nINITIALIZATION_DURATION = 0.0\nservice_endpoint_specs = {\n    \"model_input_form_spec\": None,\n    \"model_output_json_spec\": None,\n    \"profile_output_json_spec\": None,\n    \"xai_model_input_form_spec\": None,\n    \"xai_model_output_json_spec\": None,\n    \"xai_profile_output_json_spec\": None,\n}\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n\n    global INITIALIZATION_DURATION\n    global SCRIPT_START_TIME\n    global service_endpoint_specs\n\n    # Load the AI model\n    print(\"Loading AI model...\")\n    from model import (\n        MODEL_INPUT_FORM_SPEC,\n        MODEL_OUTPUT_JSON_SPEC,\n        router as model_router,\n    )\n\n    service_endpoint_specs[\"model_input_form_spec\"] = MODEL_INPUT_FORM_SPEC\n    service_endpoint_specs[\"model_output_json_spec\"] = MODEL_OUTPUT_JSON_SPEC\n    service_endpoint_specs[\"profile_output_json_spec\"] = PROFILE_OUTPUT_JSON_SPEC\n\n    app.include_router(model_router, prefix=\"/model\", tags=[\"AI Model\"])\n\n    # Load the XAI model\n    if os.path.exists(os.path.dirname(__file__) + \"/xai_model.py\"):\n        print(\"Loading XAI model...\")\n        from xai_model import (\n            XAI_OUTPUT_JSON_SPEC,\n            router as xai_model_router,\n        )\n\n        # by default, the xai_model input form spec is the same as the model input form spec\n        service_endpoint_specs[\"xai_model_input_form_spec\"] = MODEL_INPUT_FORM_SPEC\n        service_endpoint_specs[\"xai_model_output_json_spec\"] = MODEL_OUTPUT_JSON_SPEC\n        service_endpoint_specs[\"xai_model_output_json_spec\"].update(\n            XAI_OUTPUT_JSON_SPEC\n        )\n        service_endpoint_specs[\"xai_profile_output_json_spec\"] = (\n            PROFILE_OUTPUT_JSON_SPEC\n        )\n        service_endpoint_specs[\"xai_profile_output_json_spec\"].update(\n            XAI_OUTPUT_JSON_SPEC\n        )\n\n        app.include_router(xai_model_router, prefix=\"/xai_model\", tags=[\"XAI Model\"])\n\n    # Record the initialization duration\n    INITIALIZATION_DURATION = time.time() - SCRIPT_START_TIME\n\n    print(f\"AI service loaded in {INITIALIZATION_DURATION:.2f} seconds.\")\n\n    yield\n\n    # Clean up the models and release the resources\n    service_endpoint_specs.clear()\n\n\n# -------------------------------------------\n# FastAPI application setup\n# -------------------------------------------\napp = FastAPI(lifespan=lifespan)\n\n\n# -------------------------------------------\n# Middlewares\n# -------------------------------------------\n@app.middleware(\"http\")\nasync def prepare_header_middleware(request: Request, call_next):\n    start_time = time.perf_counter()\n    response = await call_next(request)\n    process_time = time.perf_counter() - start_time\n    response.headers[\"X-Process-Time\"] = str(process_time)\n    response.headers[\"X-NODE-ID\"] = NODE_ID\n    response.headers[\"X-K8S-POD-NAME\"] = K8S_POD_NAME\n    return response\n\n\n# -------------------------------------------\n# General Endpoints\n# -------------------------------------------\n@app.get(\"/initialization_duration\")\ndef get_initialization_duration():\n    \"\"\"\n    Endpoint to retrieve the initialization duration of the AI model.\n    \"\"\"\n    global INITIALIZATION_DURATION\n\n    if INITIALIZATION_DURATION == 0.0:\n        return JSONResponse(\n            content={\"error\": \"Model not initialized.\"},\n            status_code=500,\n        )\n    return JSONResponse(\n        content={\n            \"initialization_duration\": INITIALIZATION_DURATION,\n            \"script_start_time\": SCRIPT_START_TIME,\n        }\n    )\n\n\n@app.get(\"/help\")\ndef get_help():\n    global service_endpoint_specs\n    help_info = {\n        \"endpoints\": {\n            \"/model/run\": {\n                \"method\": \"POST\",\n                \"description\": \"Executes the AI model with the provided input data.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_output_json_spec\"],\n                },\n            },\n            \"/model/profile_run\": {\n                \"method\": \"POST\",\n                \"description\": \"Profiles the AI model execution.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    \"profile_result\": \"Profiling results of the AI model execution.\",\n                    **service_endpoint_specs[\"profile_output_json_spec\"],\n                },\n            },\n            \"/initialization_duration\": {\n                \"method\": \"GET\",\n                \"description\": \"Retrieves the initialization duration of the AI model.\",\n                \"response\": {\n                    \"initialization_duration\": \"Time taken to initialize the model (in seconds).\",\n                    \"script_start_time\": \"Timestamp when the script started (in seconds since epoch).\",\n                },\n            },\n        }\n    }\n\n    if service_endpoint_specs[\"xai_model_input_form_spec\"] is not None:\n        help_info[\"endpoints\"][\"/xai_model/run\"] = {\n            \"method\": \"POST\",\n            \"description\": \"Executes the XAI model with the provided input data.\",\n            \"parameters\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_input_form_spec\"],\n            },\n            \"response\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_output_json_spec\"],\n            },\n        }\n\n        help_info[\"endpoints\"][\"/xai_model/profile_run\"] = {\n            \"method\": \"POST\",\n            \"description\": \"Profiles the XAI model execution.\",\n            \"parameters\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_input_form_spec\"],\n            },\n            \"response\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                \"profile_result\": \"Profiling results of the XAI model execution.\",\n                **service_endpoint_specs[\"xai_profile_output_json_spec\"],\n            },\n        }\n    \n    return JSONResponse(content=help_info)\n",
            "ai_client_script_content": "import base64\nimport json\nimport time\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nfrom ai_client_utils import (\n    prepare_ai_service_request_files,\n    prepare_ai_service_request_data,\n)\n\nXAI_GRADCAM_METHODS = [\n    \"GradCAM\",\n    \"HiResCAM\",\n    # \"AblationCAM\",\n    \"XGradCAM\",\n    \"GradCAMPlusPlus\",\n    \"ScoreCAM\",\n    \"LayerCAM\",\n    \"EigenCAM\",\n    \"EigenGradCAM\",\n    \"KPCA_CAM\",\n    \"RandomCAM\",\n]\n\n# -------------------------------------\n# prompt for necessary inputs\n# -------------------------------------\nSERVER_URL = input(\"Please input server URL (default to http://localhost:9000): \")\nif SERVER_URL.strip() == \"\":\n    SERVER_URL = \"http://localhost:9000\"\nUE_ID = input(\"Please input UE_ID (default to 123456): \")\nif UE_ID.strip() == \"\":\n    UE_ID = \"123456\"\n\n\ndef send_post_request(url, data, files):\n    \"\"\"Send request to run AI service and display AI service responses.\"\"\"\n    try:\n        response = requests.post(url, files=files, data=data)\n        # get the process time, node id and k8s pod name from the response headers\n        process_time = response.headers.get(\"X-Process-Time\")\n        node_id = response.headers.get(\"X-NODE-ID\")\n        k8s_pod_name = response.headers.get(\"X-K8S-POD-NAME\")\n        if response.status_code == 200:\n            return response.json(), process_time, node_id, k8s_pod_name\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n\ndef send_get_request(url, params=None):\n    \"\"\"Send GET request to the specified URL and return the response.\"\"\"\n    try:\n        response = requests.get(url, params=params)\n        process_time = response.headers.get(\"X-Process-Time\")\n        node_id = response.headers.get(\"X-NODE-ID\")\n        k8s_pod_name = response.headers.get(\"X-K8S-POD-NAME\")\n        if response.status_code == 200:\n            return response.json(), process_time, node_id, k8s_pod_name\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n            return None\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n\nclass ProfileResultProcessor:\n\n    def __init__(self, server_url):\n        self.server_url = server_url\n        self.start_time = time.time()\n        self.service_initialization_duration = 0\n        self.response_counter = 0\n        self.profile_name = None\n        self.device_type = None\n        self.device_name = None\n        self.node_id = None\n        self.k8s_pod_name = None\n        self.cpu_memory_usage_bytes = 0\n        self.self_cpu_memory_usage_bytes = 0\n        self.device_memory_usage_bytes = 0\n        self.self_device_memory_usage_bytes = 0\n        self.cpu_time_total_us = 0\n        self.self_cpu_time_total_us = 0\n        self.device_time_total_us = 0\n        self.self_device_time_total_us = 0\n\n        # xai related\n        self.gradcam_method_name = None\n\n        self.fetch_service_initialization_duration()\n\n    def fetch_service_initialization_duration(self):\n        \"\"\"Fetch the service initialization duration from the server.\"\"\"\n        response, process_time, node_id, k8s_pod_name = send_get_request(\n            f\"{self.server_url}/initialization_duration\"\n        )\n        if response:\n            self.service_initialization_duration = response.get(\n                \"initialization_duration\", 0\n            )\n        else:\n            print(\"Failed to fetch initialization duration.\")\n            self.service_initialization_duration = 0\n\n    def process_new_response(\n        self,\n        profile_response,\n        process_time=None,\n        node_id=None,\n        k8s_pod_name=None,\n        gradcam_method_name=None,\n    ):\n        if not profile_response:\n            return\n\n        profile_result = profile_response[\"profile_result\"]\n\n        if not profile_result:\n            return\n\n        if self.profile_name is None:\n            self.profile_name = profile_result[\"name\"]\n        if self.device_type is None:\n            self.device_type = profile_result[\"device_type\"]\n        if self.device_name is None:\n            self.device_name = profile_result[\"device_name\"]\n        if self.node_id is None:\n            self.node_id = node_id\n        if self.k8s_pod_name is None:\n            self.k8s_pod_name = k8s_pod_name\n\n        if self.gradcam_method_name is None:\n            self.gradcam_method_name = gradcam_method_name\n\n        # update the max profile result\n        self.cpu_memory_usage_bytes = max(\n            self.cpu_memory_usage_bytes, profile_result[\"cpu_memory_usage\"]\n        )\n        # self cpu memory usage could be negative. here we take the value that has the max absolute value\n        if abs(profile_result[\"self_cpu_memory_usage\"]) > abs( self.self_cpu_memory_usage_bytes):\n            self.self_cpu_memory_usage_bytes = profile_result[\"self_cpu_memory_usage\"]\n        self.device_memory_usage_bytes = max(\n            self.device_memory_usage_bytes, profile_result[\"device_memory_usage\"]\n        )\n        # same as self cpu memory usage\n        if abs(profile_result[\"self_device_memory_usage\"]) > abs(self.self_device_memory_usage_bytes):\n            self.self_device_memory_usage_bytes = profile_result[\"self_device_memory_usage\"]\n        self.cpu_time_total_us = max(\n            self.cpu_time_total_us, profile_result[\"cpu_time_total\"]\n        )\n        self.self_cpu_time_total_us = max(\n            self.self_cpu_time_total_us, profile_result[\"self_cpu_time_total\"]\n        )\n        self.device_time_total_us = max(\n            self.device_time_total_us, profile_result[\"device_time_total\"]\n        )\n        self.self_device_time_total_us = max(\n            self.self_device_time_total_us, profile_result[\"self_device_time_total\"]\n        )\n\n        self.response_counter += 1\n\n    def complete_profile(self):\n        print(\"\\n--------- PROFILE EVENT ---------\\n\")\n        print(f\"Name: {self.profile_name}\")\n        print(f\"Device Type: {self.device_type}\")\n        print(f\"Device Name: {self.device_name}\")\n        print(f\"Node ID: {self.node_id}\")\n        print(f\"K8S_POD_NAME: {self.k8s_pod_name}\")\n\n        if self.gradcam_method_name:\n            print(f\"GradCAM Method Name: {self.gradcam_method_name}\")\n\n        print(\"\\n--------- LATENCY RESULT ---------\\n\")\n        print(\"Service Initialization Duration: \", self.service_initialization_duration)\n        print(\"Total Requests: \", self.response_counter)\n        print(f\"Total Time Taken: {time.time() - self.start_time:.2f} seconds\")\n        print(\n            f\"Average Time Taken: {(time.time() - self.start_time) / self.response_counter:.2f} seconds\"\n        )\n\n        print(\"\\n--------- RESOURCE USAGE ---------\\n\")\n        print(f\"CPU Memory Usage: {self.cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\")\n        print(\n            f\"Self CPU Memory Usage: {self.self_cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Device Memory Usage: {self.device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Self Device Memory Usage: {self.self_device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(f\"CPU Time Total: {self.cpu_time_total_us / 1000:.2f} ms\")\n        print(f\"Self CPU Time Total: {self.self_cpu_time_total_us / 1000:.2f} ms\")\n        print(f\"Device Time Total: {self.device_time_total_us / 1000:.2f} ms\")\n        print(f\"Self Device Time Total: {self.self_device_time_total_us / 1000:.2f} ms\")\n\n        # update the service_data.json automatically\n        with open(\"service_data.json\", \"r\") as f:\n            service_data = json.load(f)\n\n        if not self.gradcam_method_name:\n            complete_profile_data_to_save = {\n                \"node_id\": self.node_id,\n                \"device_type\": self.device_type,\n                \"device_name\": self.device_name,\n                \"initialization_time_ms\": self.service_initialization_duration * 1000,\n                \"eviction_time_ms\": 0,\n                \"initialization_cost\": 0,\n                \"keep_alive_cost\": 0,\n                \"energy_consumption_idle\": 0,\n                \"inference\": {\n                    \"cpu_time_ms\": self.cpu_time_total_us / 1000,\n                    \"device_time_ms\": self.device_time_total_us / 1000,\n                    \"cpu_memory_usage_MB\": self.cpu_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"self_cpu_memory_usage_MB\": self.self_cpu_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"device_memory_usage_MB\": self.device_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"self_device_memory_usage_MB\": self.self_device_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"energy_consumption_execution\": 0,\n                    \"disk_IO_MB\": 0,\n                    \"input_data_MB\": 0,\n                    \"output_data_MB\": 0,\n                    \"execution_time_ms\": (time.time() - self.start_time)\n                    / self.response_counter\n                    * 1000,\n                    \"execution_cost\": 0,\n                },\n            }\n\n            # check if there is already a profile for this node id\n            profile_found = False\n            for profile in service_data[\"profiles\"]:\n                if profile[\"node_id\"] == self.node_id:\n                    profile[\"inference\"] = complete_profile_data_to_save[\"inference\"]\n                    profile_found = True\n                    break\n            if not profile_found:\n                service_data[\"profiles\"].append(complete_profile_data_to_save)\n\n            # save the updated service_data.json\n            with open(\"service_data.json\", \"w\") as f:\n                json.dump(service_data, f, indent=4)\n            print(\"\\n--------- SERVICE DATA UPDATED ---------\\n\")\n\n        else:\n            complete_xai_profile_data_to_save = {\n                \"node_id\": self.node_id,\n                \"device_type\": self.device_type,\n                \"device_name\": self.device_name,\n                \"initialization_time_ms\": self.service_initialization_duration * 1000,\n                \"eviction_time_ms\": 0,\n                \"initialization_cost\": 0,\n                \"keep_alive_cost\": 0,\n                \"energy_consumption_idle\": 0,\n                \"xai\": [\n                    {\n                        \"xai_method\": self.gradcam_method_name,\n                        \"cpu_time_ms\": self.cpu_time_total_us / 1000,\n                        \"device_time_ms\": self.device_time_total_us / 1000,\n                        \"cpu_memory_usage_MB\": self.cpu_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"self_cpu_memory_usage_MB\": self.self_cpu_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"device_memory_usage_MB\": self.device_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"self_device_memory_usage_MB\": self.self_device_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"energy_consumption_execution\": 0,\n                        \"disk_IO_MB\": 0,\n                        \"input_data_MB\": 0,\n                        \"output_data_MB\": 0,\n                        \"execution_time_ms\": (time.time() - self.start_time)\n                        / self.response_counter\n                        * 1000,\n                        \"execution_cost\": 0,\n                    }\n                ],\n            }\n\n            # check if there is already a profile for this node id\n            profile_found = False\n            for profile in service_data[\"profiles\"]:\n                if profile[\"node_id\"] == self.node_id:\n                    profile_found = True\n\n                    # check if there is already a profile for this xai method\n                    xai_method_found = False\n                    if not profile.get(\"xai\"):\n                        profile[\"xai\"] = []\n                    for xai_profile in profile[\"xai\"]:\n                        if xai_profile[\"xai_method\"] == self.gradcam_method_name:\n                            xai_profile.update(complete_xai_profile_data_to_save[\"xai\"][0]) \n                            xai_method_found = True\n                            break\n                    if not xai_method_found:\n                        profile[\"xai\"].append(complete_xai_profile_data_to_save[\"xai\"][0])\n                    break\n\n            if not profile_found:\n                service_data[\"profiles\"].append(complete_xai_profile_data_to_save)\n\n            # save the updated service_data.json\n            with open(\"service_data.json\", \"w\") as f:\n                json.dump(service_data, f, indent=4)\n            print(\"\\n--------- SERVICE DATA UPDATED ---------\\n\")\n\n\ndef option_run():\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n    data = {**data, \"ue_id\": UE_ID}\n    response, process_time, node_id, pod_name = send_post_request(\n        f\"{SERVER_URL}/model/run\", data, files\n    )\n    print(\"Process Time: \", process_time)\n    print(\"Node ID: \", node_id)\n    print(\"K8S_POD_NAME: \", pod_name)\n    print(\"Response\")\n    print(json.dumps(response, indent=4))\n\n\ndef option_profile_run():\n    data = prepare_ai_service_request_data()\n    data = {**data, \"ue_id\": UE_ID}\n    files = prepare_ai_service_request_files()\n    num_requests = int(input(\"Enter the number of requests to send: \"))\n\n    try:\n        profile_result_processor = ProfileResultProcessor(SERVER_URL)\n\n        for _ in range(num_requests):\n            profile_response, process_time, node_id, k8s_node_name = send_post_request(\n                f\"{SERVER_URL}/model/profile_run\", data, files\n            )\n            print(\"Process Time: \", process_time)\n            print(\"Node ID: \", node_id)\n            print(\"K8S_POD_NAME: \", k8s_node_name)\n            print(\"Response\")\n            print(json.dumps(profile_response, indent=4))\n            if not profile_response:\n                print(\"No profile response received.\")\n                continue\n\n            profile_result_processor.process_new_response(\n                profile_response,\n                process_time=process_time,\n                node_id=node_id,\n                k8s_pod_name=k8s_node_name,\n            )\n\n        # Print the final profile result and update the service_data.json\n        profile_result_processor.complete_profile()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\n\ndef option_help():\n    \"\"\"Get help information from the server.\"\"\"\n    try:\n        response, process_time, node_id, pod_name = send_get_request(\n            f\"{SERVER_URL}/help\"\n        )\n        print(\"Process Time: \", process_time)\n        print(\"Node ID: \", node_id)\n        print(\"K8S_POD_NAME: \", pod_name)\n        print(\"Response\")\n        print(json.dumps(response, indent=4))\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n\n\ndef option_run_with_xai():\n    \"\"\"Run AI service with XAI.\"\"\"\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n\n    print(\n        \"Note that currently only GradCAM methods on image-classification models are supported.\"\n    )\n    while True:\n        gradcam_method_name = input(\n            f\"Please select a GradCAM method (options: {XAI_GRADCAM_METHODS}): \"\n        )\n        if gradcam_method_name not in XAI_GRADCAM_METHODS:\n            print(f\"Invalid GradCAM method. Please select again.\")\n        else:\n            break\n\n    # ask for target class for explanation\n    target_category_indexes = input(\n        \"Please input target category indexes for explanation (comma-separated, e.g., 111, 32, 44, ...): \"\n    )\n    if not target_category_indexes or not target_category_indexes.strip():\n        print(\n            \"No target category indexes provided. Defaulting to explaining the top confident category.\"\n        )\n        target_category_indexes = []\n    else:\n        target_category_indexes = [\n            int(i.strip()) for i in target_category_indexes.split(\",\")\n        ]\n\n    data = {\n        **data,\n        \"ue_id\": UE_ID,\n        \"gradcam_method_name\": gradcam_method_name,\n        \"target_category_indexes\": target_category_indexes,\n    }\n    print(\"Data: \", data)\n    response, process_time, node_id, k8s_pod_name = send_post_request(\n        f\"{SERVER_URL}/xai_model/run\", data, files\n    )\n    print(\"Process Time: \", process_time)\n    print(\"Node ID: \", node_id)\n    print(\"K8S_POD_NAME: \", k8s_pod_name)\n\n    # Handle JSON response\n    model_results = response.get(\"model_results\")\n    if model_results:\n        print(\"Model Results:\", json.dumps(model_results, indent=4))\n\n    xai_results = response.get(\"xai_results\")\n    if xai_results:\n        print(\"XAI Results Method:\", xai_results.get(\"xai_method\"))\n\n    # Handle binary image response\n    encoded_image = xai_results.get(\"image\")\n    if encoded_image:\n        image_bytes = base64.b64decode(encoded_image)\n\n        # Load the image into Pillow\n        image = Image.open(BytesIO(image_bytes))\n\n        # Save image to disk\n        image.save(\"xai_output.png\")\n\n        # Display the image using matplotlib\n        plt.imshow(image)\n        plt.axis(\"off\")\n        plt.show()\n\n\ndef option_profile_run_with_xai():\n    \"\"\"Run AI service with XAI and profile the run.\"\"\"\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n    print(\n        \"Note that currently only GradCAM methods on image-classification models are supported.\"\n    )\n\n    # ask for target class for explanation\n    target_category_indexes = input(\n        \"Please input target category indexes for explanation (comma-separated, e.g., 111, 32, 44, ...): \"\n    )\n    if not target_category_indexes or not target_category_indexes.strip():\n        print(\n            \"No target category indexes provided. Defaulting to explaining the top confident category.\"\n        )\n        target_category_indexes = []\n    else:\n        target_category_indexes = [\n            int(i.strip()) for i in target_category_indexes.split(\",\")\n        ]\n\n    num_requests = int(input(\"Enter the number of requests to send: \"))\n\n    try:\n        for gradcam_method_name in XAI_GRADCAM_METHODS:\n\n            data = {\n                **data,\n                \"ue_id\": UE_ID,\n                \"gradcam_method_name\": gradcam_method_name,\n                \"target_category_indexes\": target_category_indexes,\n            }\n            print(\"Data: \", data)\n\n            profile_result_processor = ProfileResultProcessor(SERVER_URL)\n\n            for _ in range(num_requests):\n                response, process_time, node_id, k8s_pod_name = send_post_request(\n                    f\"{SERVER_URL}/xai_model/profile_run\", data, files\n                )\n                print(\"Process Time: \", process_time)\n                print(\"Node ID: \", node_id)\n                print(\"K8S_POD_NAME: \", k8s_pod_name)\n                if not response:\n                    print(\"No profile response received.\")\n                    continue\n\n                # Handle JSON response\n                model_results = response.get(\"model_results\")\n                if model_results:\n                    print(\"Model Results:\", json.dumps(model_results, indent=4))\n\n                profile_result_processor.process_new_response(\n                    response,\n                    process_time=process_time,\n                    node_id=node_id,\n                    k8s_pod_name=k8s_pod_name,\n                    gradcam_method_name=gradcam_method_name,\n                )\n\n            # Print the final profile result and update the service_data.json\n            profile_result_processor.complete_profile()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\n\nOPTIONS = [\n    {\n        \"label\": \"Get help information\",\n        \"action\": option_help,\n    },\n    {\n        \"label\": \"Run AI service\",\n        \"action\": option_run,\n    },\n    {\n        \"label\": \"Profile AI service\",\n        \"action\": option_profile_run,\n    },\n    {\n        \"label\": \"Run AI service with XAI (only image-classification models)\",\n        \"action\": option_run_with_xai,\n    },\n    {\n        \"label\": \"Profile AI service with XAI (only image-classification models)\",\n        \"action\": option_profile_run_with_xai,\n    },\n]\n\n\nif __name__ == \"__main__\":\n    while True:\n        print(\"\\nOptions:\")\n        for i, option in enumerate(OPTIONS, start=1):\n            print(f\"{i}. {option['label']}\")\n        print(\"q. Quit\")\n        choice = input(\"Enter your choice: \")\n\n        if choice == \"q\":\n            print(\"Exiting the client. Goodbye!\")\n            break\n        else:\n            try:\n                choice = int(choice)\n                if 1 <= choice <= len(OPTIONS):\n                    OPTIONS[choice - 1][\"action\"]()\n                else:\n                    print(\"Invalid choice. Please try again.\")\n            except ValueError:\n                print(\"Invalid input. Please enter a number.\")\n",
            "ai_client_utils_script_content": "def prepare_ai_service_request_files():\n    \"\"\"Prepare the `files` part for the AI service request.\"\"\"\n    files = {}\n    image_file_path = input(\"Please input the image file path: \")\n    with open(image_file_path, \"rb\") as image_file:\n        files[\"file\"] = image_file.read()\n    return files\n\ndef prepare_ai_service_request_data():\n    \"\"\"Prepare the `data` part other than `ue_id` for the AI service request.\"\"\"\n    data = {}\n    ue_id = input(\"Please input the unique execution ID (ue_id): \")\n    data[\"ue_id\"] = ue_id\n    return data",
            "model_script_content": "# import server utils\nfrom ai_server_utils import (\n    process_model_output_logits,\n    profile_activities,\n    prepare_profile_results,\n)\n# import profile utils\nfrom torch.profiler import profile, record_function\n\n# import necessary libs for AI model inference and request handling\nimport torch\nfrom fastapi import APIRouter, File, Form, UploadFile\nfrom fastapi.responses import JSONResponse\nfrom transformers import AutoImageProcessor, SwinForImageClassification\nfrom PIL import Image\n\n# --------------------------------\n# Device configuration\n# --------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# --------------------------------\n# Model-specific configuration\n# make sure the variables `MODEL_NAME` and `model` are defined here.\n# --------------------------------\nMODEL_NAME = \"microsoft/swin-large-patch4-window12-384-in22k\"\nprocessor = AutoImageProcessor.from_pretrained(MODEL_NAME)\nmodel = SwinForImageClassification.from_pretrained(MODEL_NAME).to(device)\nmodel.eval()\n\n# Initialize the FastAPI router\nrouter = APIRouter()\n\n@router.post(\"/run\")\nasync def run_model(file: UploadFile = File(...), ue_id: str = Form(...)):\n    try:\n        # Prepare the model input\n        image = Image.open(file.file).convert(\"RGB\")\n        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n\n        # Perform inference\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        # Process the model outputs\n        predictions = process_model_output_logits(model, outputs.logits)\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"model_results\": predictions,\n            }\n        )\n    except Exception as e:\n        print(f\"Error processing file: {e}\")\n        return JSONResponse(\n            content={\"error\": \"Failed to process the image. {e}\".format(e=str(e))},\n            status_code=500,\n        )\n\n@router.post(\"/profile_run\")\nasync def profile_run(file: UploadFile = File(...), ue_id: str = Form(...)):\n    \"\"\"\n    Endpoint to profile the AI model execution.\n    \"\"\"\n    try:\n        # Prepare the model input\n        image = Image.open(file.file).convert(\"RGB\")\n        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n\n        # perform profiling\n        with profile(\n            activities=profile_activities,\n            profile_memory=True,\n        ) as prof:\n            with record_function(\"model_run\"):\n                with torch.no_grad():\n                    model_outputs = model(**inputs)\n\n        profile_result = prepare_profile_results(prof)\n\n        # Process the model outputs\n        predictions = process_model_output_logits(model, model_outputs.logits)\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"profile_result\": profile_result,\n                \"model_results\": predictions,\n            }\n        )\n\n    except Exception as e:\n        print(f\"Error processing request: {e}\")\n        return JSONResponse(\n            content={\"error\": f\"Failed to process the request. {e}\"},\n            status_code=500,\n        )\n\n# Below are the model input and output specifications to be used by the `/help` endpoint\nMODEL_INPUT_FORM_SPEC = {\n    \"file\": {\n        \"type\": \"file upload\",\n        \"description\": \"The image file to be classified.\",\n        \"required\": True,\n        \"example\": \"puppy.png\",\n    }\n}\n\nMODEL_OUTPUT_JSON_SPEC = {\n    \"ue_id\": \"unique execution ID\",\n    \"model_results\": [\n        {\n            \"category_id\": \"category id\",\n            \"label\": \"category label\",\n            \"probability\": \"probability value\",\n        }\n    ],\n}",
            "healthcheck_script_content": "import requests\n\nprint(requests.get(\"http://localhost:8000/help\"))"
        }
    },
    {
        "model_name": "nvidia/mit-b0",
        "model_url": "https://huggingface.co/nvidia/mit-b0",
        "task": "image-classification",
        "task_detail": "The NVIDIA/mit-b0 pre-trained AI model is a hierarchical Transformer-based encoder specifically designed for applications in computer vision, particularly revolving around semantic segmentation tasks. It represents one of the variants in the SegFormer model suite, known for delivering optimal performance in semantic segmentation by leveraging both the power of Transformers and Multi-Layer Perceptrons (MLPs).\n\n### Functionality Description\n\n#### Core Capabilities\n1. **Hierarchical Transformer Encoder**: The model employs a hierarchical approach, utilizing Transformer architectures that intricately capture various levels of spatial hierarchies present in visual data. This enhances its capacity to understand and represent complex image features at different granularities.\n\n2. **Pre-training and Fine-tuning Paradigm**: Initially pre-trained on the extensive ImageNet-1k dataset, this model processes a broad spectrum of visual concepts, offering a robust foundation for subsequent fine-tuning. Fine-tuning adapts the model to specific downstream tasks, particularly efficient for semantic segmentation.\n\n3. **Semantic Segmentation**: While the repository provides the pre-trained encoder without the lightweight MLP decode head, the model can be effectively adapted for fine-tuning to achieve semantic segmentation across varied visual datasets. It has demonstrated high performance in benchmark tasks such as ADE20K and Cityscapes.\n\n#### Intended Use Cases\n- **Semantic Segmentation**: Practical for scenarios requiring precise delineation of objects and structures within images, such as autonomous driving (road and obstacle identification), medical imaging (structure delineation), and urban planning (land use classification).\n- **Transfer Learning**: Users can harness the model for transfer learning, adapting its pre-trained knowledge for diverse image classification and segmentation tasks tailored to custom datasets beyond ImageNet-1k.\n\n#### Input and Output Format\n- **Input Format**: The model processes images formatted typically as PIL Image objects, oftentimes sourced from URLs or local storage. It expects inputs that adhere to ImageNet-1k standards for optimum performance.\n- **Output Format**: After processing, the model generates logits representing the probability across the 1,000 ImageNet classes, from which the predicted class can be identified by determining the class index with the highest score.\n\nWith its foundation on Transformer architectures, this model is an embodiment of modern advancements in computer vision, striking a balance between simplicity and efficiency. As part of the Hugging Face repository, it also offers seamless integration with their ecosystem, providing expansive resources and documentation for users aiming to leverage its capabilities for advanced image processing tasks. \n\nThis detailed functionality description is crafted to assist in semantic linking against use case scenarios, optimizing both comprehension and applicability for varied industry needs.",
        "accuracy_info": "No accuracy information found.",
        "image_repository_url": "",
        "service_disk_size_bytes": 3609359837.0,
        "profiles": [
            {
                "node_id": "ugurcan.celik",
                "device_type": "DeviceType.CPU",
                "device_name": "cuda",
                "initialization_time_ms": 3737.701177597046,
                "eviction_time_ms": 0.0,
                "initialization_cost": 0.0,
                "keep_alive_cost": 0.0,
                "energy_consumption_idle": 0.0,
                "idle_container_cpu_memory_usage": "1.3GB",
                "idle_container_device_memory_usage": "2.3GB",
                "inference": {
                    "cpu_time_ms": 185.22334999999998,
                    "device_time_ms": 6.892565999999687,
                    "cpu_memory_usage_MB": 0.0,
                    "self_cpu_memory_usage_MB": 0.0,
                    "device_memory_usage_MB": 8.12890625,
                    "self_device_memory_usage_MB": -367.2666015625,
                    "energy_consumption_execution": 0.0,
                    "disk_IO_MB": 0.0,
                    "input_data_MB": 0.0,
                    "output_data_MB": 0.0,
                    "execution_time_ms": 290.22804896036786,
                    "execution_cost": 0.0
                },
                "xai": [
                    {
                        "xai_method": "GradCAM",
                        "cpu_time_ms": 217.371792,
                        "device_time_ms": 3.2873749999999893,
                        "cpu_memory_usage_MB": 0.50006103515625,
                        "self_cpu_memory_usage_MB": 0.246246337890625,
                        "device_memory_usage_MB": 242.74658203125,
                        "self_device_memory_usage_MB": -147.3759765625,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 784.5080693562826,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "HiResCAM",
                        "cpu_time_ms": 38.375471000000005,
                        "device_time_ms": 3.261615999999993,
                        "cpu_memory_usage_MB": 0.50006103515625,
                        "self_cpu_memory_usage_MB": 0.246246337890625,
                        "device_memory_usage_MB": 221.8876953125,
                        "self_device_memory_usage_MB": -147.3759765625,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 704.667886098226,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "XGradCAM",
                        "cpu_time_ms": 35.735322999999994,
                        "device_time_ms": 3.263443000000007,
                        "cpu_memory_usage_MB": 0.50006103515625,
                        "self_cpu_memory_usage_MB": 0.246246337890625,
                        "device_memory_usage_MB": 221.8876953125,
                        "self_device_memory_usage_MB": -147.3759765625,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 705.5991490681967,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "GradCAMPlusPlus",
                        "cpu_time_ms": 38.253601,
                        "device_time_ms": 3.269622999999996,
                        "cpu_memory_usage_MB": 0.50006103515625,
                        "self_cpu_memory_usage_MB": 0.246246337890625,
                        "device_memory_usage_MB": 221.8876953125,
                        "self_device_memory_usage_MB": -147.3759765625,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 718.1201775868734,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "ScoreCAM",
                        "cpu_time_ms": 1043.635375,
                        "device_time_ms": 1001.0616359999989,
                        "cpu_memory_usage_MB": 64.25006103515625,
                        "self_cpu_memory_usage_MB": -0.006683349609375,
                        "device_memory_usage_MB": 221.8876953125,
                        "self_device_memory_usage_MB": -94840.15625,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 4572.356144587199,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "LayerCAM",
                        "cpu_time_ms": 35.811116999999996,
                        "device_time_ms": 3.2891309999999905,
                        "cpu_memory_usage_MB": 0.50006103515625,
                        "self_cpu_memory_usage_MB": 0.246246337890625,
                        "device_memory_usage_MB": 221.8876953125,
                        "self_device_memory_usage_MB": -147.3759765625,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 803.4454981486002,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenCAM",
                        "cpu_time_ms": 29.557176,
                        "device_time_ms": 3.284845000000002,
                        "cpu_memory_usage_MB": 0.25006103515625,
                        "self_cpu_memory_usage_MB": -0.003753662109375,
                        "device_memory_usage_MB": 221.8876953125,
                        "self_device_memory_usage_MB": -147.375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 280.8982531229655,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenGradCAM",
                        "cpu_time_ms": 170.629817,
                        "device_time_ms": 3.2961749999999954,
                        "cpu_memory_usage_MB": 0.50006103515625,
                        "self_cpu_memory_usage_MB": 0.246246337890625,
                        "device_memory_usage_MB": 221.8876953125,
                        "self_device_memory_usage_MB": -147.3759765625,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 816.7763551076254,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "KPCA_CAM",
                        "cpu_time_ms": 21.954662,
                        "device_time_ms": 3.2845579999999943,
                        "cpu_memory_usage_MB": 0.25006103515625,
                        "self_cpu_memory_usage_MB": -0.003753662109375,
                        "device_memory_usage_MB": 221.8876953125,
                        "self_device_memory_usage_MB": -147.375,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 319.51133410135907,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "RandomCAM",
                        "cpu_time_ms": 34.616454000000004,
                        "device_time_ms": 3.2927590000000047,
                        "cpu_memory_usage_MB": 0.50006103515625,
                        "self_cpu_memory_usage_MB": 0.246246337890625,
                        "device_memory_usage_MB": 221.8876953125,
                        "self_device_memory_usage_MB": -147.3759765625,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 733.8398297627767,
                        "execution_cost": 0.0
                    }
                ]
            },
            {
                "node_id": "LAP004262",
                "device_type": "DeviceType.CPU",
                "device_name": "None",
                "initialization_time_ms": 6111.06014251709,
                "eviction_time_ms": 0.0,
                "initialization_cost": 0.0,
                "keep_alive_cost": 0.0,
                "energy_consumption_idle": 0.0,
                "idle_container_cpu_memory_usage": "1.7GB",
                "idle_container_device_memory_usage": "0GB",
                "inference": {
                    "cpu_time_ms": 108.19508499999999,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 0.003814697265625,
                    "self_cpu_memory_usage_MB": -363.267578125,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0.0,
                    "disk_IO_MB": 0.0,
                    "input_data_MB": 0.0,
                    "output_data_MB": 0.0,
                    "execution_time_ms": 203.20367813110352,
                    "execution_cost": 0.0
                },
                "xai": [
                    {
                        "xai_method": "GradCAM",
                        "cpu_time_ms": 427.19369,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 234.030517578125,
                        "self_cpu_memory_usage_MB": -160.7990493774414,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 947.9987621307373,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "HiResCAM",
                        "cpu_time_ms": 505.29799099999997,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 220.38766479492188,
                        "self_cpu_memory_usage_MB": -160.7990493774414,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 975.2175013224283,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "XGradCAM",
                        "cpu_time_ms": 447.40744200000006,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 220.38766479492188,
                        "self_cpu_memory_usage_MB": -160.7990493774414,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1084.2050711313884,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "GradCAMPlusPlus",
                        "cpu_time_ms": 469.73238,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 220.38766479492188,
                        "self_cpu_memory_usage_MB": -160.7677993774414,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 923.3861764272054,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "LayerCAM",
                        "cpu_time_ms": 461.69106,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 220.38766479492188,
                        "self_cpu_memory_usage_MB": -160.7677993774414,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1095.572868982951,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenCAM",
                        "cpu_time_ms": 639.372952,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 220.13766479492188,
                        "self_cpu_memory_usage_MB": -147.12493896484375,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 543.7111854553223,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "EigenGradCAM",
                        "cpu_time_ms": 524.837486,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 220.38766479492188,
                        "self_cpu_memory_usage_MB": -160.7677993774414,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1155.2560329437256,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "KPCA_CAM",
                        "cpu_time_ms": 138.432713,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 220.13766479492188,
                        "self_cpu_memory_usage_MB": -147.12493896484375,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 331.7454655965169,
                        "execution_cost": 0.0
                    },
                    {
                        "xai_method": "RandomCAM",
                        "cpu_time_ms": 454.69828,
                        "device_time_ms": 0.0,
                        "cpu_memory_usage_MB": 220.38766479492188,
                        "self_cpu_memory_usage_MB": -160.7677993774414,
                        "device_memory_usage_MB": 0.0,
                        "self_device_memory_usage_MB": 0.0,
                        "energy_consumption_execution": 0.0,
                        "disk_IO_MB": 0.0,
                        "input_data_MB": 0.0,
                        "output_data_MB": 0.0,
                        "execution_time_ms": 1093.5981273651123,
                        "execution_cost": 0.0
                    }
                ]
            }
        ],
        "feedback": {
            "likes": [],
            "dislikes": [],
            "comments": []
        },
        "code": {
            "readme_content": "---\nlicense: other\ntags:\n- vision\ndatasets:\n- imagenet_1k\nwidget:\n- src: https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\n  example_title: House\n- src: https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000002.jpg\n  example_title: Castle\n---\n\n# SegFormer (b0-sized) encoder pre-trained-only\n\nSegFormer encoder fine-tuned on Imagenet-1k. It was introduced in the paper [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203) by Xie et al. and first released in [this repository](https://github.com/NVlabs/SegFormer). \n\nDisclaimer: The team releasing SegFormer did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nSegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes. The hierarchical Transformer is first pre-trained on ImageNet-1k, after which a decode head is added and fine-tuned altogether on a downstream dataset.\n\nThis repository only contains the pre-trained hierarchical Transformer, hence it can be used for fine-tuning purposes.\n\n## Intended uses & limitations\n\nYou can use the model for fine-tuning of semantic segmentation. See the [model hub](https://huggingface.co/models?other=segformer) to look for fine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\n\n```python\nfrom transformers import SegformerImageProcessor, SegformerForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nimage_processor = SegformerImageProcessor.from_pretrained(\"nvidia/mit-b0\")\nmodel = SegformerForImageClassification.from_pretrained(\"nvidia/mit-b0\")\n\ninputs = image_processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n```\n\nFor more code examples, we refer to the [documentation](https://huggingface.co/transformers/model_doc/segformer.html#).\n\n### License\n\nThe license for this model can be found [here](https://github.com/NVlabs/SegFormer/blob/master/LICENSE).\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-2105-15203,\n  author    = {Enze Xie and\n               Wenhai Wang and\n               Zhiding Yu and\n               Anima Anandkumar and\n               Jose M. Alvarez and\n               Ping Luo},\n  title     = {SegFormer: Simple and Efficient Design for Semantic Segmentation with\n               Transformers},\n  journal   = {CoRR},\n  volume    = {abs/2105.15203},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2105.15203},\n  eprinttype = {arXiv},\n  eprint    = {2105.15203},\n  timestamp = {Wed, 02 Jun 2021 11:46:42 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-15203.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n",
            "dockerfile_content": "FROM python3.12_ai_service_base:latest\n\nWORKDIR /app\n\nCOPY . .\n\nRUN pip install --no-cache-dir fastapi uvicorn transformers Pillow requests\n\nEXPOSE 8000\n\nCMD [\"uvicorn\", \"ai_server:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--timeout-keep-alive\", \"600\"]",
            "ai_server_script_content": "import json\nimport os\nimport time\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import JSONResponse\nfrom contextlib import asynccontextmanager\nfrom ai_server_utils import (\n    PROFILE_OUTPUT_JSON_SPEC,\n    NODE_ID,\n    K8S_POD_NAME,\n)\n\n\n# -------------------------------------------\n# App Lifespan setup\n# -------------------------------------------\n# Record the script start time (when uvicorn starts the process)\nSCRIPT_START_TIME = time.time()\nINITIALIZATION_DURATION = 0.0\nservice_endpoint_specs = {\n    \"model_input_form_spec\": None,\n    \"model_output_json_spec\": None,\n    \"profile_output_json_spec\": None,\n    \"xai_model_input_form_spec\": None,\n    \"xai_model_output_json_spec\": None,\n    \"xai_profile_output_json_spec\": None,\n}\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n\n    global INITIALIZATION_DURATION\n    global SCRIPT_START_TIME\n    global service_endpoint_specs\n\n    # Load the AI model\n    print(\"Loading AI model...\")\n    from model import (\n        MODEL_INPUT_FORM_SPEC,\n        MODEL_OUTPUT_JSON_SPEC,\n        router as model_router,\n    )\n\n    service_endpoint_specs[\"model_input_form_spec\"] = MODEL_INPUT_FORM_SPEC\n    service_endpoint_specs[\"model_output_json_spec\"] = MODEL_OUTPUT_JSON_SPEC\n    service_endpoint_specs[\"profile_output_json_spec\"] = PROFILE_OUTPUT_JSON_SPEC\n\n    app.include_router(model_router, prefix=\"/model\", tags=[\"AI Model\"])\n\n    # Load the XAI model\n    if os.path.exists(os.path.dirname(__file__) + \"/xai_model.py\"):\n        print(\"Loading XAI model...\")\n        from xai_model import (\n            XAI_OUTPUT_JSON_SPEC,\n            router as xai_model_router,\n        )\n\n        # by default, the xai_model input form spec is the same as the model input form spec\n        service_endpoint_specs[\"xai_model_input_form_spec\"] = MODEL_INPUT_FORM_SPEC\n        service_endpoint_specs[\"xai_model_output_json_spec\"] = MODEL_OUTPUT_JSON_SPEC\n        service_endpoint_specs[\"xai_model_output_json_spec\"].update(\n            XAI_OUTPUT_JSON_SPEC\n        )\n        service_endpoint_specs[\"xai_profile_output_json_spec\"] = (\n            PROFILE_OUTPUT_JSON_SPEC\n        )\n        service_endpoint_specs[\"xai_profile_output_json_spec\"].update(\n            XAI_OUTPUT_JSON_SPEC\n        )\n\n        app.include_router(xai_model_router, prefix=\"/xai_model\", tags=[\"XAI Model\"])\n\n    # Record the initialization duration\n    INITIALIZATION_DURATION = time.time() - SCRIPT_START_TIME\n\n    print(f\"AI service loaded in {INITIALIZATION_DURATION:.2f} seconds.\")\n\n    yield\n\n    # Clean up the models and release the resources\n    service_endpoint_specs.clear()\n\n\n# -------------------------------------------\n# FastAPI application setup\n# -------------------------------------------\napp = FastAPI(lifespan=lifespan)\n\n\n# -------------------------------------------\n# Middlewares\n# -------------------------------------------\n@app.middleware(\"http\")\nasync def prepare_header_middleware(request: Request, call_next):\n    start_time = time.perf_counter()\n    response = await call_next(request)\n    process_time = time.perf_counter() - start_time\n    response.headers[\"X-Process-Time\"] = str(process_time)\n    response.headers[\"X-NODE-ID\"] = NODE_ID\n    response.headers[\"X-K8S-POD-NAME\"] = K8S_POD_NAME\n    return response\n\n\n# -------------------------------------------\n# General Endpoints\n# -------------------------------------------\n@app.get(\"/initialization_duration\")\ndef get_initialization_duration():\n    \"\"\"\n    Endpoint to retrieve the initialization duration of the AI model.\n    \"\"\"\n    global INITIALIZATION_DURATION\n\n    if INITIALIZATION_DURATION == 0.0:\n        return JSONResponse(\n            content={\"error\": \"Model not initialized.\"},\n            status_code=500,\n        )\n    return JSONResponse(\n        content={\n            \"initialization_duration\": INITIALIZATION_DURATION,\n            \"script_start_time\": SCRIPT_START_TIME,\n        }\n    )\n\n\n@app.get(\"/help\")\ndef get_help():\n    global service_endpoint_specs\n    help_info = {\n        \"endpoints\": {\n            \"/model/run\": {\n                \"method\": \"POST\",\n                \"description\": \"Executes the AI model with the provided input data.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_output_json_spec\"],\n                },\n            },\n            \"/model/profile_run\": {\n                \"method\": \"POST\",\n                \"description\": \"Profiles the AI model execution.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    \"profile_result\": \"Profiling results of the AI model execution.\",\n                    **service_endpoint_specs[\"profile_output_json_spec\"],\n                },\n            },\n            \"/initialization_duration\": {\n                \"method\": \"GET\",\n                \"description\": \"Retrieves the initialization duration of the AI model.\",\n                \"response\": {\n                    \"initialization_duration\": \"Time taken to initialize the model (in seconds).\",\n                    \"script_start_time\": \"Timestamp when the script started (in seconds since epoch).\",\n                },\n            },\n        }\n    }\n\n    if service_endpoint_specs[\"xai_model_input_form_spec\"] is not None:\n        help_info[\"endpoints\"][\"/xai_model/run\"] = {\n            \"method\": \"POST\",\n            \"description\": \"Executes the XAI model with the provided input data.\",\n            \"parameters\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_input_form_spec\"],\n            },\n            \"response\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_output_json_spec\"],\n            },\n        }\n\n        help_info[\"endpoints\"][\"/xai_model/profile_run\"] = {\n            \"method\": \"POST\",\n            \"description\": \"Profiles the XAI model execution.\",\n            \"parameters\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_input_form_spec\"],\n            },\n            \"response\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                \"profile_result\": \"Profiling results of the XAI model execution.\",\n                **service_endpoint_specs[\"xai_profile_output_json_spec\"],\n            },\n        }\n    \n    return JSONResponse(content=help_info)\n",
            "ai_client_script_content": "import base64\nimport json\nimport time\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nfrom ai_client_utils import (\n    prepare_ai_service_request_files,\n    prepare_ai_service_request_data,\n)\n\nXAI_GRADCAM_METHODS = [\n    \"GradCAM\",\n    \"HiResCAM\",\n    # \"AblationCAM\",\n    \"XGradCAM\",\n    \"GradCAMPlusPlus\",\n    \"ScoreCAM\",\n    \"LayerCAM\",\n    \"EigenCAM\",\n    \"EigenGradCAM\",\n    \"KPCA_CAM\",\n    \"RandomCAM\",\n]\n\n# -------------------------------------\n# prompt for necessary inputs\n# -------------------------------------\nSERVER_URL = input(\"Please input server URL (default to http://localhost:9000): \")\nif SERVER_URL.strip() == \"\":\n    SERVER_URL = \"http://localhost:9000\"\nUE_ID = input(\"Please input UE_ID (default to 123456): \")\nif UE_ID.strip() == \"\":\n    UE_ID = \"123456\"\n\n\ndef send_post_request(url, data, files):\n    \"\"\"Send request to run AI service and display AI service responses.\"\"\"\n    try:\n        response = requests.post(url, files=files, data=data)\n        # get the process time, node id and k8s pod name from the response headers\n        process_time = response.headers.get(\"X-Process-Time\")\n        node_id = response.headers.get(\"X-NODE-ID\")\n        k8s_pod_name = response.headers.get(\"X-K8S-POD-NAME\")\n        if response.status_code == 200:\n            return response.json(), process_time, node_id, k8s_pod_name\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n\ndef send_get_request(url, params=None):\n    \"\"\"Send GET request to the specified URL and return the response.\"\"\"\n    try:\n        response = requests.get(url, params=params)\n        process_time = response.headers.get(\"X-Process-Time\")\n        node_id = response.headers.get(\"X-NODE-ID\")\n        k8s_pod_name = response.headers.get(\"X-K8S-POD-NAME\")\n        if response.status_code == 200:\n            return response.json(), process_time, node_id, k8s_pod_name\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n            return None\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n\nclass ProfileResultProcessor:\n\n    def __init__(self, server_url):\n        self.server_url = server_url\n        self.start_time = time.time()\n        self.service_initialization_duration = 0\n        self.response_counter = 0\n        self.profile_name = None\n        self.device_type = None\n        self.device_name = None\n        self.node_id = None\n        self.k8s_pod_name = None\n        self.cpu_memory_usage_bytes = 0\n        self.self_cpu_memory_usage_bytes = 0\n        self.device_memory_usage_bytes = 0\n        self.self_device_memory_usage_bytes = 0\n        self.cpu_time_total_us = 0\n        self.self_cpu_time_total_us = 0\n        self.device_time_total_us = 0\n        self.self_device_time_total_us = 0\n\n        # xai related\n        self.gradcam_method_name = None\n\n        self.fetch_service_initialization_duration()\n\n    def fetch_service_initialization_duration(self):\n        \"\"\"Fetch the service initialization duration from the server.\"\"\"\n        response, process_time, node_id, k8s_pod_name = send_get_request(\n            f\"{self.server_url}/initialization_duration\"\n        )\n        if response:\n            self.service_initialization_duration = response.get(\n                \"initialization_duration\", 0\n            )\n        else:\n            print(\"Failed to fetch initialization duration.\")\n            self.service_initialization_duration = 0\n\n    def process_new_response(\n        self,\n        profile_response,\n        process_time=None,\n        node_id=None,\n        k8s_pod_name=None,\n        gradcam_method_name=None,\n    ):\n        if not profile_response:\n            return\n\n        profile_result = profile_response[\"profile_result\"]\n\n        if not profile_result:\n            return\n\n        if self.profile_name is None:\n            self.profile_name = profile_result[\"name\"]\n        if self.device_type is None:\n            self.device_type = profile_result[\"device_type\"]\n        if self.device_name is None:\n            self.device_name = profile_result[\"device_name\"]\n        if self.node_id is None:\n            self.node_id = node_id\n        if self.k8s_pod_name is None:\n            self.k8s_pod_name = k8s_pod_name\n\n        if self.gradcam_method_name is None:\n            self.gradcam_method_name = gradcam_method_name\n\n        # update the max profile result\n        self.cpu_memory_usage_bytes = max(\n            self.cpu_memory_usage_bytes, profile_result[\"cpu_memory_usage\"]\n        )\n        # self cpu memory usage could be negative. here we take the value that has the max absolute value\n        if abs(profile_result[\"self_cpu_memory_usage\"]) > abs( self.self_cpu_memory_usage_bytes):\n            self.self_cpu_memory_usage_bytes = profile_result[\"self_cpu_memory_usage\"]\n        self.device_memory_usage_bytes = max(\n            self.device_memory_usage_bytes, profile_result[\"device_memory_usage\"]\n        )\n        # same as self cpu memory usage\n        if abs(profile_result[\"self_device_memory_usage\"]) > abs(self.self_device_memory_usage_bytes):\n            self.self_device_memory_usage_bytes = profile_result[\"self_device_memory_usage\"]\n        self.cpu_time_total_us = max(\n            self.cpu_time_total_us, profile_result[\"cpu_time_total\"]\n        )\n        self.self_cpu_time_total_us = max(\n            self.self_cpu_time_total_us, profile_result[\"self_cpu_time_total\"]\n        )\n        self.device_time_total_us = max(\n            self.device_time_total_us, profile_result[\"device_time_total\"]\n        )\n        self.self_device_time_total_us = max(\n            self.self_device_time_total_us, profile_result[\"self_device_time_total\"]\n        )\n\n        self.response_counter += 1\n\n    def complete_profile(self):\n        print(\"\\n--------- PROFILE EVENT ---------\\n\")\n        print(f\"Name: {self.profile_name}\")\n        print(f\"Device Type: {self.device_type}\")\n        print(f\"Device Name: {self.device_name}\")\n        print(f\"Node ID: {self.node_id}\")\n        print(f\"K8S_POD_NAME: {self.k8s_pod_name}\")\n\n        if self.gradcam_method_name:\n            print(f\"GradCAM Method Name: {self.gradcam_method_name}\")\n\n        print(\"\\n--------- LATENCY RESULT ---------\\n\")\n        print(\"Service Initialization Duration: \", self.service_initialization_duration)\n        print(\"Total Requests: \", self.response_counter)\n        print(f\"Total Time Taken: {time.time() - self.start_time:.2f} seconds\")\n        print(\n            f\"Average Time Taken: {(time.time() - self.start_time) / self.response_counter:.2f} seconds\"\n        )\n\n        print(\"\\n--------- RESOURCE USAGE ---------\\n\")\n        print(f\"CPU Memory Usage: {self.cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\")\n        print(\n            f\"Self CPU Memory Usage: {self.self_cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Device Memory Usage: {self.device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Self Device Memory Usage: {self.self_device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(f\"CPU Time Total: {self.cpu_time_total_us / 1000:.2f} ms\")\n        print(f\"Self CPU Time Total: {self.self_cpu_time_total_us / 1000:.2f} ms\")\n        print(f\"Device Time Total: {self.device_time_total_us / 1000:.2f} ms\")\n        print(f\"Self Device Time Total: {self.self_device_time_total_us / 1000:.2f} ms\")\n\n        # update the service_data.json automatically\n        with open(\"service_data.json\", \"r\") as f:\n            service_data = json.load(f)\n\n        if not self.gradcam_method_name:\n            complete_profile_data_to_save = {\n                \"node_id\": self.node_id,\n                \"device_type\": self.device_type,\n                \"device_name\": self.device_name,\n                \"initialization_time_ms\": self.service_initialization_duration * 1000,\n                \"eviction_time_ms\": 0,\n                \"initialization_cost\": 0,\n                \"keep_alive_cost\": 0,\n                \"energy_consumption_idle\": 0,\n                \"inference\": {\n                    \"cpu_time_ms\": self.cpu_time_total_us / 1000,\n                    \"device_time_ms\": self.device_time_total_us / 1000,\n                    \"cpu_memory_usage_MB\": self.cpu_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"self_cpu_memory_usage_MB\": self.self_cpu_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"device_memory_usage_MB\": self.device_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"self_device_memory_usage_MB\": self.self_device_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"energy_consumption_execution\": 0,\n                    \"disk_IO_MB\": 0,\n                    \"input_data_MB\": 0,\n                    \"output_data_MB\": 0,\n                    \"execution_time_ms\": (time.time() - self.start_time)\n                    / self.response_counter\n                    * 1000,\n                    \"execution_cost\": 0,\n                },\n            }\n\n            # check if there is already a profile for this node id\n            profile_found = False\n            for profile in service_data[\"profiles\"]:\n                if profile[\"node_id\"] == self.node_id:\n                    profile[\"inference\"] = complete_profile_data_to_save[\"inference\"]\n                    profile_found = True\n                    break\n            if not profile_found:\n                service_data[\"profiles\"].append(complete_profile_data_to_save)\n\n            # save the updated service_data.json\n            with open(\"service_data.json\", \"w\") as f:\n                json.dump(service_data, f, indent=4)\n            print(\"\\n--------- SERVICE DATA UPDATED ---------\\n\")\n\n        else:\n            complete_xai_profile_data_to_save = {\n                \"node_id\": self.node_id,\n                \"device_type\": self.device_type,\n                \"device_name\": self.device_name,\n                \"initialization_time_ms\": self.service_initialization_duration * 1000,\n                \"eviction_time_ms\": 0,\n                \"initialization_cost\": 0,\n                \"keep_alive_cost\": 0,\n                \"energy_consumption_idle\": 0,\n                \"xai\": [\n                    {\n                        \"xai_method\": self.gradcam_method_name,\n                        \"cpu_time_ms\": self.cpu_time_total_us / 1000,\n                        \"device_time_ms\": self.device_time_total_us / 1000,\n                        \"cpu_memory_usage_MB\": self.cpu_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"self_cpu_memory_usage_MB\": self.self_cpu_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"device_memory_usage_MB\": self.device_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"self_device_memory_usage_MB\": self.self_device_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"energy_consumption_execution\": 0,\n                        \"disk_IO_MB\": 0,\n                        \"input_data_MB\": 0,\n                        \"output_data_MB\": 0,\n                        \"execution_time_ms\": (time.time() - self.start_time)\n                        / self.response_counter\n                        * 1000,\n                        \"execution_cost\": 0,\n                    }\n                ],\n            }\n\n            # check if there is already a profile for this node id\n            profile_found = False\n            for profile in service_data[\"profiles\"]:\n                if profile[\"node_id\"] == self.node_id:\n                    profile_found = True\n\n                    # check if there is already a profile for this xai method\n                    xai_method_found = False\n                    if not profile.get(\"xai\"):\n                        profile[\"xai\"] = []\n                    for xai_profile in profile[\"xai\"]:\n                        if xai_profile[\"xai_method\"] == self.gradcam_method_name:\n                            xai_profile.update(complete_xai_profile_data_to_save[\"xai\"][0]) \n                            xai_method_found = True\n                            break\n                    if not xai_method_found:\n                        profile[\"xai\"].append(complete_xai_profile_data_to_save[\"xai\"][0])\n                    break\n\n            if not profile_found:\n                service_data[\"profiles\"].append(complete_xai_profile_data_to_save)\n\n            # save the updated service_data.json\n            with open(\"service_data.json\", \"w\") as f:\n                json.dump(service_data, f, indent=4)\n            print(\"\\n--------- SERVICE DATA UPDATED ---------\\n\")\n\n\ndef option_run():\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n    data = {**data, \"ue_id\": UE_ID}\n    response, process_time, node_id, pod_name = send_post_request(\n        f\"{SERVER_URL}/model/run\", data, files\n    )\n    print(\"Process Time: \", process_time)\n    print(\"Node ID: \", node_id)\n    print(\"K8S_POD_NAME: \", pod_name)\n    print(\"Response\")\n    print(json.dumps(response, indent=4))\n\n\ndef option_profile_run():\n    data = prepare_ai_service_request_data()\n    data = {**data, \"ue_id\": UE_ID}\n    files = prepare_ai_service_request_files()\n    num_requests = int(input(\"Enter the number of requests to send: \"))\n\n    try:\n        profile_result_processor = ProfileResultProcessor(SERVER_URL)\n\n        for _ in range(num_requests):\n            profile_response, process_time, node_id, k8s_node_name = send_post_request(\n                f\"{SERVER_URL}/model/profile_run\", data, files\n            )\n            print(\"Process Time: \", process_time)\n            print(\"Node ID: \", node_id)\n            print(\"K8S_POD_NAME: \", k8s_node_name)\n            print(\"Response\")\n            print(json.dumps(profile_response, indent=4))\n            if not profile_response:\n                print(\"No profile response received.\")\n                continue\n\n            profile_result_processor.process_new_response(\n                profile_response,\n                process_time=process_time,\n                node_id=node_id,\n                k8s_pod_name=k8s_node_name,\n            )\n\n        # Print the final profile result and update the service_data.json\n        profile_result_processor.complete_profile()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\n\ndef option_help():\n    \"\"\"Get help information from the server.\"\"\"\n    try:\n        response, process_time, node_id, pod_name = send_get_request(\n            f\"{SERVER_URL}/help\"\n        )\n        print(\"Process Time: \", process_time)\n        print(\"Node ID: \", node_id)\n        print(\"K8S_POD_NAME: \", pod_name)\n        print(\"Response\")\n        print(json.dumps(response, indent=4))\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n\n\ndef option_run_with_xai():\n    \"\"\"Run AI service with XAI.\"\"\"\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n\n    print(\n        \"Note that currently only GradCAM methods on image-classification models are supported.\"\n    )\n    while True:\n        gradcam_method_name = input(\n            f\"Please select a GradCAM method (options: {XAI_GRADCAM_METHODS}): \"\n        )\n        if gradcam_method_name not in XAI_GRADCAM_METHODS:\n            print(f\"Invalid GradCAM method. Please select again.\")\n        else:\n            break\n\n    # ask for target class for explanation\n    target_category_indexes = input(\n        \"Please input target category indexes for explanation (comma-separated, e.g., 111, 32, 44, ...): \"\n    )\n    if not target_category_indexes or not target_category_indexes.strip():\n        print(\n            \"No target category indexes provided. Defaulting to explaining the top confident category.\"\n        )\n        target_category_indexes = []\n    else:\n        target_category_indexes = [\n            int(i.strip()) for i in target_category_indexes.split(\",\")\n        ]\n\n    data = {\n        **data,\n        \"ue_id\": UE_ID,\n        \"gradcam_method_name\": gradcam_method_name,\n        \"target_category_indexes\": target_category_indexes,\n    }\n    print(\"Data: \", data)\n    response, process_time, node_id, k8s_pod_name = send_post_request(\n        f\"{SERVER_URL}/xai_model/run\", data, files\n    )\n    print(\"Process Time: \", process_time)\n    print(\"Node ID: \", node_id)\n    print(\"K8S_POD_NAME: \", k8s_pod_name)\n\n    # Handle JSON response\n    model_results = response.get(\"model_results\")\n    if model_results:\n        print(\"Model Results:\", json.dumps(model_results, indent=4))\n\n    xai_results = response.get(\"xai_results\")\n    if xai_results:\n        print(\"XAI Results Method:\", xai_results.get(\"xai_method\"))\n\n    # Handle binary image response\n    encoded_image = xai_results.get(\"image\")\n    if encoded_image:\n        image_bytes = base64.b64decode(encoded_image)\n\n        # Load the image into Pillow\n        image = Image.open(BytesIO(image_bytes))\n\n        # Save image to disk\n        image.save(\"xai_output.png\")\n\n        # Display the image using matplotlib\n        plt.imshow(image)\n        plt.axis(\"off\")\n        plt.show()\n\n\ndef option_profile_run_with_xai():\n    \"\"\"Run AI service with XAI and profile the run.\"\"\"\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n    print(\n        \"Note that currently only GradCAM methods on image-classification models are supported.\"\n    )\n\n    # ask for target class for explanation\n    target_category_indexes = input(\n        \"Please input target category indexes for explanation (comma-separated, e.g., 111, 32, 44, ...): \"\n    )\n    if not target_category_indexes or not target_category_indexes.strip():\n        print(\n            \"No target category indexes provided. Defaulting to explaining the top confident category.\"\n        )\n        target_category_indexes = []\n    else:\n        target_category_indexes = [\n            int(i.strip()) for i in target_category_indexes.split(\",\")\n        ]\n\n    num_requests = int(input(\"Enter the number of requests to send: \"))\n\n    try:\n        for gradcam_method_name in XAI_GRADCAM_METHODS:\n\n            data = {\n                **data,\n                \"ue_id\": UE_ID,\n                \"gradcam_method_name\": gradcam_method_name,\n                \"target_category_indexes\": target_category_indexes,\n            }\n            print(\"Data: \", data)\n\n            profile_result_processor = ProfileResultProcessor(SERVER_URL)\n\n            for _ in range(num_requests):\n                response, process_time, node_id, k8s_pod_name = send_post_request(\n                    f\"{SERVER_URL}/xai_model/profile_run\", data, files\n                )\n                print(\"Process Time: \", process_time)\n                print(\"Node ID: \", node_id)\n                print(\"K8S_POD_NAME: \", k8s_pod_name)\n                if not response:\n                    print(\"No profile response received.\")\n                    continue\n\n                # Handle JSON response\n                model_results = response.get(\"model_results\")\n                if model_results:\n                    print(\"Model Results:\", json.dumps(model_results, indent=4))\n\n                profile_result_processor.process_new_response(\n                    response,\n                    process_time=process_time,\n                    node_id=node_id,\n                    k8s_pod_name=k8s_pod_name,\n                    gradcam_method_name=gradcam_method_name,\n                )\n\n            # Print the final profile result and update the service_data.json\n            profile_result_processor.complete_profile()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\n\nOPTIONS = [\n    {\n        \"label\": \"Get help information\",\n        \"action\": option_help,\n    },\n    {\n        \"label\": \"Run AI service\",\n        \"action\": option_run,\n    },\n    {\n        \"label\": \"Profile AI service\",\n        \"action\": option_profile_run,\n    },\n    {\n        \"label\": \"Run AI service with XAI (only image-classification models)\",\n        \"action\": option_run_with_xai,\n    },\n    {\n        \"label\": \"Profile AI service with XAI (only image-classification models)\",\n        \"action\": option_profile_run_with_xai,\n    },\n]\n\n\nif __name__ == \"__main__\":\n    while True:\n        print(\"\\nOptions:\")\n        for i, option in enumerate(OPTIONS, start=1):\n            print(f\"{i}. {option['label']}\")\n        print(\"q. Quit\")\n        choice = input(\"Enter your choice: \")\n\n        if choice == \"q\":\n            print(\"Exiting the client. Goodbye!\")\n            break\n        else:\n            try:\n                choice = int(choice)\n                if 1 <= choice <= len(OPTIONS):\n                    OPTIONS[choice - 1][\"action\"]()\n                else:\n                    print(\"Invalid choice. Please try again.\")\n            except ValueError:\n                print(\"Invalid input. Please enter a number.\")\n",
            "ai_client_utils_script_content": "# import any necessary libraries.\n\ndef prepare_ai_service_request_files():\n    \"\"\"Prepare the `files` part for the AI service request.\"\"\"\n    files = {}\n    image_file_path = input(\"Please input the image file path: \")\n    with open(image_file_path, \"rb\") as image_file:\n        files[\"file\"] = image_file.read()\n    return files\n\ndef prepare_ai_service_request_data():\n    \"\"\"Prepare the `data` part including `ue_id` for the AI service request.\"\"\"\n    data = {}\n    ue_id = input(\"Please input the unique execution ID (ue_id): \")\n    data[\"ue_id\"] = ue_id\n    return data",
            "model_script_content": "# import server utils\nfrom ai_server_utils import (\n    process_model_output_logits,\n    profile_activities,\n    prepare_profile_results,\n)\n# import profile utils\nfrom torch.profiler import profile, record_function\n\n# import necessary libs for AI model inference and request handling\nimport torch\nfrom fastapi import APIRouter, File, Form, UploadFile\nfrom fastapi.responses import JSONResponse\nfrom transformers import SegformerImageProcessor, SegformerForImageClassification\nfrom PIL import Image\n\n# --------------------------------\n# Device configuration\n# --------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# --------------------------------\n# Model-specific configuration\n# make sure the variables `MODEL_NAME` and `model` are defined here.\n# --------------------------------\nMODEL_NAME = \"nvidia/mit-b0\"\nprocessor = SegformerImageProcessor.from_pretrained(MODEL_NAME)\nmodel = SegformerForImageClassification.from_pretrained(MODEL_NAME).to(device)\nmodel.eval()\n\n\n# Initialize the FastAPI router\nrouter = APIRouter()\n\n\n@router.post(\"/run\")\nasync def run_model(file: UploadFile = File(...), ue_id: str = Form(...)):\n    try:\n        # Prepare the model input\n        image = Image.open(file.file).convert(\"RGB\")\n        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n\n        # Perform inference\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        # Process the model outputs\n        predictions = process_model_output_logits(model, outputs.logits)\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"model_results\": predictions,\n            }\n        )\n    except Exception as e:\n        print(f\"Error processing file: {e}\")\n        return JSONResponse(\n            content={\"error\": \"Failed to process the image. {e}\".format(e=str(e))},\n            status_code=500,\n        )\n\n\n@router.post(\"/profile_run\")\nasync def profile_run(file: UploadFile = File(...), ue_id: str = Form(...)):\n    \"\"\"\n    Endpoint to profile the AI model execution.\n    \"\"\"\n    try:\n        # Prepare the model input\n        image = Image.open(file.file).convert(\"RGB\")\n        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n\n        # perform profiling\n        with profile(\n            activities=profile_activities,\n            profile_memory=True,\n        ) as prof:\n            with record_function(\"model_run\"):\n                with torch.no_grad():\n                    model_outputs = model(**inputs)\n\n        profile_result = prepare_profile_results(prof)\n\n        # Process the model outputs\n        predictions = process_model_output_logits(model, model_outputs.logits)\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"profile_result\": profile_result,\n                \"model_results\": predictions,\n            }\n        )\n\n    except Exception as e:\n        print(f\"Error processing request: {e}\")\n        return JSONResponse(\n            content={\"error\": f\"Failed to process the request. {e}\"},\n            status_code=500,\n        )\n\n\n# Below are the model input and output specifications to be used by the `/help` endpoint\nMODEL_INPUT_FORM_SPEC = {\n    \"file\": {\n        \"type\": \"file upload\",\n        \"description\": \"The image file to be classified.\",\n        \"required\": True,\n        \"example\": \"puppy.png\",\n    }\n}\n\nMODEL_OUTPUT_JSON_SPEC = {\n    \"ue_id\": \"unique execution ID\",\n    \"model_results\": [\n        {\n            \"category_id\": \"category id\",\n            \"label\": \"category label\",\n            \"probability\": \"probability value\",\n        }\n    ],\n}",
            "healthcheck_script_content": "import requests\n\nprint(requests.get(\"http://localhost:8000/help\"))"
        }
    }
]