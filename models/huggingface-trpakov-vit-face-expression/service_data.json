{
    "model_name": "trpakov/vit-face-expression",
    "model_url": "https://huggingface.co/trpakov/vit-face-expression",
    "task": "image-classification",
    "task_detail": "The pre-trained AI model \"trpakov/vit-face-expression\" specializes in facial expression recognition, leveraging the Vision Transformer (ViT) architecture for this purpose. This model has been fine-tuned from the vit-base-patch16-224-in21k model using the FER2013 dataset, which contains a diverse range of facial images annotated across seven distinct emotional categories: Angry, Disgust, Fear, Happy, Sad, Surprise, and Neutral.\n\n**Functionality and Use Case Scenarios:**\n\n1. **Image Classification Capabilities:**\n   - The model processes facial images for the classification of human emotions.\n   - It categorizes images into one of the seven predefined emotion categories based on the visible expressions.\n\n2. **Expected Input Format:**\n   - Input images must be single-channel or multi-channel (grayscale or RGB) facial images.\n   - Images should be preprocessed to meet the model's requirements, including being resized to a standard input size compatible with Vision Transformer models.\n\n3. **Preprocessing Steps:**\n   - **Resizing:** Input images are resized to the required dimensions to ensure uniformity and compatibility with the Vision Transformer architecture.\n   - **Normalization:** Pixel values are adjusted to fit a specific range to aid in the model's learning process.\n   - **Data Augmentation:** Techniques such as random rotations, flips, and zooms are applied to enhance the variability and robustness of the training dataset, potentially improving the model's generalization to new images.\n\n4. **Output Format:**\n   - The model outputs a predicted emotion category for each input image.\n   - Typically, this is in the form of a label corresponding to one of the seven emotions.\n\n5. **Performance Metrics:**\n   - The model demonstrates a validation set accuracy of 71.13% and a test set accuracy of 71.16%, providing a solid baseline for emotion recognition tasks.\n\n6. **Limitations:**\n   - The model's effectiveness can be impacted by biases within the training data, which may affect its performance across different demographic groups.\n   - Its generalizability to unseen data is largely dependent on the diversity within the FER2013 dataset.\n\n7. **Potential Application Areas:**\n   - **Human-Computer Interaction:** Enhancing user interfaces with emotion-based feedback systems.\n   - **Mental Health Monitoring:** Assisting in the detection of emotional states that may indicate mental health conditions.\n   - **Market Research:** Analyzing consumer emotions in response to products or advertisements.\n   - **Security and Surveillance:** Monitoring expressions to assess suspicious or unusual emotional behaviors.\n\nOverall, the \"trpakov/vit-face-expression\" model is designed for developers and researchers who need an effective tool for recognizing human emotions through facial expressions, offering a competitive solution with practical applications across diverse fields.",
    "accuracy_info": "The model's performance on facial emotion recognition, based on the README, shows that it achieves a validation set accuracy of 71.13% and a test set accuracy of 71.16%.",
    "image_repository_url": "docker.io/cranfield6g/cranfield-edge-trpakov-vit-face-expression",
    "service_disk_size_bytes": 3664003171,
    "profiles": [
        {
            "node_id": "LAP004262",
            "device_type": "DeviceType.CPU",
            "device_name": "None",
            "initialization_time_ms": 470468.30344200134,
            "eviction_time_ms": 0,
            "initialization_cost": 0,
            "keep_alive_cost": 0,
            "energy_consumption_idle": 0,
            "inference": {
                "cpu_time_ms": 183.963112,
                "device_time_ms": 0.0,
                "cpu_memory_usage_MB": 2.6702880859375e-05,
                "self_cpu_memory_usage_MB": -154.67360305786133,
                "device_memory_usage_MB": 0.0,
                "self_device_memory_usage_MB": 0.0,
                "energy_consumption_execution": 0,
                "disk_IO_MB": 0,
                "input_data_MB": 0,
                "output_data_MB": 0,
                "execution_time_ms": 258.3962281545003,
                "execution_cost": 0
            },
            "xai": [
                {
                    "xai_method": "GradCAM",
                    "cpu_time_ms": 518.223513,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 448.0804138183594,
                    "self_cpu_memory_usage_MB": -363.6709327697754,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 928.8991292317709,
                    "execution_cost": 0
                },
                {
                    "xai_method": "HiResCAM",
                    "cpu_time_ms": 542.1168489999999,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 120.18976211547852,
                    "self_cpu_memory_usage_MB": -363.6709327697754,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 892.3839728037516,
                    "execution_cost": 0
                },
                {
                    "xai_method": "XGradCAM",
                    "cpu_time_ms": 532.2828820000001,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 120.18976211547852,
                    "self_cpu_memory_usage_MB": -363.6709327697754,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 998.2210000356039,
                    "execution_cost": 0
                },
                {
                    "xai_method": "GradCAMPlusPlus",
                    "cpu_time_ms": 519.070933,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 120.18976211547852,
                    "self_cpu_memory_usage_MB": -365.4023780822754,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 980.0560474395752,
                    "execution_cost": 0
                },
                {
                    "xai_method": "LayerCAM",
                    "cpu_time_ms": 497.32752500000004,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 120.18976211547852,
                    "self_cpu_memory_usage_MB": -364.8252296447754,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 900.0178972880045,
                    "execution_cost": 0
                },
                {
                    "xai_method": "EigenCAM",
                    "cpu_time_ms": 226.323253,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 119.61261367797852,
                    "self_cpu_memory_usage_MB": -35.7802734375,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 361.84438069661456,
                    "execution_cost": 0
                },
                {
                    "xai_method": "EigenGradCAM",
                    "cpu_time_ms": 804.453076,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 120.18976211547852,
                    "self_cpu_memory_usage_MB": -363.6709327697754,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 1136.0421975453694,
                    "execution_cost": 0
                },
                {
                    "xai_method": "KPCA_CAM",
                    "cpu_time_ms": 198.095589,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 119.61261367797852,
                    "self_cpu_memory_usage_MB": -35.7802734375,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 342.9727554321289,
                    "execution_cost": 0
                },
                {
                    "xai_method": "RandomCAM",
                    "cpu_time_ms": 568.139331,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 120.18976211547852,
                    "self_cpu_memory_usage_MB": -364.2480812072754,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 1132.5628757476807,
                    "execution_cost": 0
                }
            ],
            "idle_container_cpu_memory_usage": "1.5GB",
            "idle_container_device_memory_usage": "0GB"
        }
    ],
    "feedback": {
        "likes": [],
        "dislikes": [],
        "comments": []
    },
    "code": {
        "readme_content": "---\r\nlicense: apache-2.0\r\n---\r\n# Vision Transformer (ViT) for Facial Expression Recognition Model Card\n\n## Model Overview\n\n- **Model Name:** [trpakov/vit-face-expression](https://huggingface.co/trpakov/vit-face-expression)\n\n- **Task:** Facial Expression/Emotion Recognition\n\n- **Dataset:** [FER2013](https://www.kaggle.com/datasets/msambare/fer2013)\n\n- **Model Architecture:** [Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)\n\n- **Finetuned from model:** [vit-base-patch16-224-in21k](https://huggingface.co/google/vit-base-patch16-224-in21k)\n\n## Model Description\n\nThe vit-face-expression model is a Vision Transformer fine-tuned for the task of facial emotion recognition. \n\nIt is trained on the FER2013 dataset, which consists of facial images categorized into seven different emotions:\n- Angry\n- Disgust\n- Fear\n- Happy\n- Sad\n- Surprise\n- Neutral\n\n## Data Preprocessing\n\nThe input images are preprocessed before being fed into the model. The preprocessing steps include:\n- **Resizing:** Images are resized to the specified input size.\n- **Normalization:** Pixel values are normalized to a specific range.\n- **Data Augmentation:** Random transformations such as rotations, flips, and zooms are applied to augment the training dataset.\n\n## Evaluation Metrics\n\n- **Validation set accuracy:** 0.7113\n- **Test set accuracy:** 0.7116\n\n## Limitations\n\n- **Data Bias:** The model's performance may be influenced by biases present in the training data.\n- **Generalization:** The model's ability to generalize to unseen data is subject to the diversity of the training dataset.",
        "dockerfile_content": "# Base image for AI service powered by HuggingFace pre-trained AI models.\n# the image has the following packages/libraries installed already:\n# - python3.12, pip, git, fastapi, uvicorn, torch, torchvision, opencv-python, transformers, python-multipart, Pillow, requests\nFROM python3.12_ai_service_base:latest\n\n# Set working directory\nWORKDIR /app\n\n# Copy application code\nCOPY . .\n\n# Install additional dependencies\nRUN pip install transformers\n\n# Expose port 8000\nEXPOSE 8000\n\n# Start the FastAPI server\nCMD [\"uvicorn\", \"ai_server:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--timeout-keep-alive\", \"600\"]",
        "ai_server_script_content": "import json\nimport os\nimport time\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import JSONResponse\nfrom contextlib import asynccontextmanager\nfrom ai_server_utils import (\n    PROFILE_OUTPUT_JSON_SPEC,\n    NODE_ID,\n    K8S_POD_NAME,\n)\n\n\n# -------------------------------------------\n# App Lifespan setup\n# -------------------------------------------\n# Record the script start time (when uvicorn starts the process)\nSCRIPT_START_TIME = time.time()\nINITIALIZATION_DURATION = 0.0\nservice_endpoint_specs = {\n    \"model_input_form_spec\": None,\n    \"model_output_json_spec\": None,\n    \"profile_output_json_spec\": None,\n    \"xai_model_input_form_spec\": None,\n    \"xai_model_output_json_spec\": None,\n    \"xai_profile_output_json_spec\": None,\n}\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n\n    global INITIALIZATION_DURATION\n    global SCRIPT_START_TIME\n    global service_endpoint_specs\n\n    # Load the AI model\n    print(\"Loading AI model...\")\n    from model import (\n        MODEL_INPUT_FORM_SPEC,\n        MODEL_OUTPUT_JSON_SPEC,\n        router as model_router,\n    )\n\n    service_endpoint_specs[\"model_input_form_spec\"] = MODEL_INPUT_FORM_SPEC\n    service_endpoint_specs[\"model_output_json_spec\"] = MODEL_OUTPUT_JSON_SPEC\n    service_endpoint_specs[\"profile_output_json_spec\"] = PROFILE_OUTPUT_JSON_SPEC\n\n    app.include_router(model_router, prefix=\"/model\", tags=[\"AI Model\"])\n\n    # Load the XAI model\n    if os.path.exists(os.path.dirname(__file__) + \"/xai_model.py\"):\n        print(\"Loading XAI model...\")\n        from xai_model import (\n            XAI_OUTPUT_JSON_SPEC,\n            router as xai_model_router,\n        )\n\n        # by default, the xai_model input form spec is the same as the model input form spec\n        service_endpoint_specs[\"xai_model_input_form_spec\"] = MODEL_INPUT_FORM_SPEC\n        service_endpoint_specs[\"xai_model_output_json_spec\"] = MODEL_OUTPUT_JSON_SPEC\n        service_endpoint_specs[\"xai_model_output_json_spec\"].update(\n            XAI_OUTPUT_JSON_SPEC\n        )\n        service_endpoint_specs[\"xai_profile_output_json_spec\"] = (\n            PROFILE_OUTPUT_JSON_SPEC\n        )\n        service_endpoint_specs[\"xai_profile_output_json_spec\"].update(\n            XAI_OUTPUT_JSON_SPEC\n        )\n\n        app.include_router(xai_model_router, prefix=\"/xai_model\", tags=[\"XAI Model\"])\n\n    # Record the initialization duration\n    INITIALIZATION_DURATION = time.time() - SCRIPT_START_TIME\n\n    print(f\"AI service loaded in {INITIALIZATION_DURATION:.2f} seconds.\")\n\n    yield\n\n    # Clean up the models and release the resources\n    service_endpoint_specs.clear()\n\n\n# -------------------------------------------\n# FastAPI application setup\n# -------------------------------------------\napp = FastAPI(lifespan=lifespan)\n\n\n# -------------------------------------------\n# Middlewares\n# -------------------------------------------\n@app.middleware(\"http\")\nasync def prepare_header_middleware(request: Request, call_next):\n    start_time = time.perf_counter()\n    response = await call_next(request)\n    process_time = time.perf_counter() - start_time\n    response.headers[\"X-Process-Time\"] = str(process_time)\n    response.headers[\"X-NODE-ID\"] = NODE_ID\n    response.headers[\"X-K8S-POD-NAME\"] = K8S_POD_NAME\n    return response\n\n\n# -------------------------------------------\n# General Endpoints\n# -------------------------------------------\n@app.get(\"/initialization_duration\")\ndef get_initialization_duration():\n    \"\"\"\n    Endpoint to retrieve the initialization duration of the AI model.\n    \"\"\"\n    global INITIALIZATION_DURATION\n\n    if INITIALIZATION_DURATION == 0.0:\n        return JSONResponse(\n            content={\"error\": \"Model not initialized.\"},\n            status_code=500,\n        )\n    return JSONResponse(\n        content={\n            \"initialization_duration\": INITIALIZATION_DURATION,\n            \"script_start_time\": SCRIPT_START_TIME,\n        }\n    )\n\n\n@app.get(\"/help\")\ndef get_help():\n    global service_endpoint_specs\n    help_info = {\n        \"endpoints\": {\n            \"/model/run\": {\n                \"method\": \"POST\",\n                \"description\": \"Executes the AI model with the provided input data.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_output_json_spec\"],\n                },\n            },\n            \"/model/profile_run\": {\n                \"method\": \"POST\",\n                \"description\": \"Profiles the AI model execution.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    \"profile_result\": \"Profiling results of the AI model execution.\",\n                    **service_endpoint_specs[\"profile_output_json_spec\"],\n                },\n            },\n            \"/initialization_duration\": {\n                \"method\": \"GET\",\n                \"description\": \"Retrieves the initialization duration of the AI model.\",\n                \"response\": {\n                    \"initialization_duration\": \"Time taken to initialize the model (in seconds).\",\n                    \"script_start_time\": \"Timestamp when the script started (in seconds since epoch).\",\n                },\n            },\n        }\n    }\n\n    if service_endpoint_specs[\"xai_model_input_form_spec\"] is not None:\n        help_info[\"endpoints\"][\"/xai_model/run\"] = {\n            \"method\": \"POST\",\n            \"description\": \"Executes the XAI model with the provided input data.\",\n            \"parameters\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_input_form_spec\"],\n            },\n            \"response\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_output_json_spec\"],\n            },\n        }\n\n        help_info[\"endpoints\"][\"/xai_model/profile_run\"] = {\n            \"method\": \"POST\",\n            \"description\": \"Profiles the XAI model execution.\",\n            \"parameters\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_input_form_spec\"],\n            },\n            \"response\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                \"profile_result\": \"Profiling results of the XAI model execution.\",\n                **service_endpoint_specs[\"xai_profile_output_json_spec\"],\n            },\n        }\n    \n    return JSONResponse(content=help_info)\n",
        "ai_server_utils_script_content": "import os\nimport socket\nimport torch\nfrom io import BytesIO\nimport base64\n\nfrom torch.profiler import profile, ProfilerActivity, record_function\n\n\n# -------------------------------------------\n# ENV Variables\n# -------------------------------------------\nNODE_ID = os.getenv(\"NODE_ID\", socket.gethostname())\nK8S_POD_NAME = os.getenv(\"K8S_POD_NAME\", \"UNKNOWN\")\n\n\n# -------------------------------------------\n# Profile Utils\n# -------------------------------------------\nprofile_activities = [\n    ProfilerActivity.CPU,\n    ProfilerActivity.CUDA,\n    ProfilerActivity.MTIA,\n    ProfilerActivity.XPU,\n]\n\ndef get_image_classification_results_from_model_output_logits(model, model_output_logits):\n    \"\"\"\n    Process the model outputs to prepare for the response.\n    \"\"\"\n    probabilities = torch.nn.functional.softmax(model_output_logits[0], dim=0)\n\n    # Return the top 5 predictions with labels\n    top5_prob, top5_catid = torch.topk(probabilities, 5)\n    predictions = []\n    for i in range(top5_prob.size(0)):\n        category_id = top5_catid[i].item()\n        predictions.append(\n            {\n                \"category_id\": category_id,\n                \"label\": model.config.id2label[category_id],\n                \"probability\": top5_prob[i].item(),\n            }\n        )\n    return predictions\n\ndef prepare_profile_results(prof):\n    \"\"\"\n    Prepare the profile results for the model inputs and outputs.\n    \"\"\"\n    print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n\n    profile_event = prof.key_averages()[0]\n\n    profile_result = {\n        \"name\": profile_event.key,\n        \"device_type\": str(profile_event.device_type),\n        \"device_name\": str(profile_event.use_device),\n        \"cpu_memory_usage\": profile_event.cpu_memory_usage,\n        \"self_cpu_memory_usage\": profile_event.self_cpu_memory_usage,\n        \"device_memory_usage\": profile_event.device_memory_usage,\n        \"self_device_memory_usage\": profile_event.self_device_memory_usage,\n        \"cpu_time_total\": profile_event.cpu_time_total,\n        \"self_cpu_time_total\": profile_event.self_cpu_time_total,\n        \"device_time_total\": profile_event.device_time_total,\n        \"self_device_time_total\": profile_event.self_device_time_total,\n    }\n    return profile_result\n\n\ndef encode_image(image):\n    \"\"\"\n    Encode the image to bytes\n    \"\"\"\n    buffered = BytesIO()\n    image.save(buffered, format=\"PNG\")\n    encoded_image = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n    return encoded_image\n\n\nPROFILE_OUTPUT_JSON_SPEC = {\n    \"ue_id\": \"unique execution ID\",\n    \"profile_result\": {\n        \"name\": \"name of the profile event\",\n        \"device_type\": \"type of device used (e.g., CPU, GPU, ...)\",\n        \"device_name\": \"name of the device used\",\n        \"cpu_memory_usage\": \"CPU memory usage in bytes\",\n        \"self_cpu_memory_usage\": \"self CPU memory usage in bytes\",\n        \"device_memory_usage\": \"device memory usage in bytes\",\n        \"self_device_memory_usage\": \"self device memory usage in bytes\",\n        \"cpu_time_total\": \"total CPU time in microseconds\",\n        \"self_cpu_time_total\": \"self total CPU time in microseconds\",\n        \"device_time_total\": \"total device time in microseconds\",\n        \"self_device_time_total\": \"self total device time in microseconds\",\n    },\n    \"model_results\": \"the AI service model results\",\n}\n",
        "ai_client_script_content": "import base64\nimport json\nimport time\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nfrom ai_client_utils import (\n    prepare_ai_service_request_files,\n    prepare_ai_service_request_data,\n)\n\nXAI_GRADCAM_METHODS = [\n    \"GradCAM\",\n    \"HiResCAM\",\n    # \"AblationCAM\",\n    \"XGradCAM\",\n    \"GradCAMPlusPlus\",\n    \"ScoreCAM\",\n    \"LayerCAM\",\n    \"EigenCAM\",\n    \"EigenGradCAM\",\n    \"KPCA_CAM\",\n    \"RandomCAM\",\n]\n\n# -------------------------------------\n# prompt for necessary inputs\n# -------------------------------------\nSERVER_URL = input(\"Please input server URL (default to http://localhost:9000): \")\nif SERVER_URL.strip() == \"\":\n    SERVER_URL = \"http://localhost:9000\"\nUE_ID = input(\"Please input UE_ID (default to 123456): \")\nif UE_ID.strip() == \"\":\n    UE_ID = \"123456\"\n\n\ndef send_post_request(url, data, files):\n    \"\"\"Send request to run AI service and display AI service responses.\"\"\"\n    try:\n        response = requests.post(url, files=files, data=data)\n        # get the process time, node id and k8s pod name from the response headers\n        process_time = response.headers.get(\"X-Process-Time\")\n        node_id = response.headers.get(\"X-NODE-ID\")\n        k8s_pod_name = response.headers.get(\"X-K8S-POD-NAME\")\n        if response.status_code == 200:\n            return response.json(), process_time, node_id, k8s_pod_name\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n\ndef send_get_request(url, params=None):\n    \"\"\"Send GET request to the specified URL and return the response.\"\"\"\n    try:\n        response = requests.get(url, params=params)\n        process_time = response.headers.get(\"X-Process-Time\")\n        node_id = response.headers.get(\"X-NODE-ID\")\n        k8s_pod_name = response.headers.get(\"X-K8S-POD-NAME\")\n        if response.status_code == 200:\n            return response.json(), process_time, node_id, k8s_pod_name\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n            return None\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n\nclass ProfileResultProcessor:\n\n    def __init__(self, server_url):\n        self.server_url = server_url\n        self.start_time = time.time()\n        self.service_initialization_duration = 0\n        self.response_counter = 0\n        self.profile_name = None\n        self.device_type = None\n        self.device_name = None\n        self.node_id = None\n        self.k8s_pod_name = None\n        self.cpu_memory_usage_bytes = 0\n        self.self_cpu_memory_usage_bytes = 0\n        self.device_memory_usage_bytes = 0\n        self.self_device_memory_usage_bytes = 0\n        self.cpu_time_total_us = 0\n        self.self_cpu_time_total_us = 0\n        self.device_time_total_us = 0\n        self.self_device_time_total_us = 0\n\n        # xai related\n        self.gradcam_method_name = None\n\n        self.fetch_service_initialization_duration()\n\n    def fetch_service_initialization_duration(self):\n        \"\"\"Fetch the service initialization duration from the server.\"\"\"\n        response, process_time, node_id, k8s_pod_name = send_get_request(\n            f\"{self.server_url}/initialization_duration\"\n        )\n        if response:\n            self.service_initialization_duration = response.get(\n                \"initialization_duration\", 0\n            )\n        else:\n            print(\"Failed to fetch initialization duration.\")\n            self.service_initialization_duration = 0\n\n    def process_new_response(\n        self,\n        profile_response,\n        process_time=None,\n        node_id=None,\n        k8s_pod_name=None,\n        gradcam_method_name=None,\n    ):\n        if not profile_response:\n            return\n\n        profile_result = profile_response[\"profile_result\"]\n\n        if not profile_result:\n            return\n\n        if self.profile_name is None:\n            self.profile_name = profile_result[\"name\"]\n        if self.device_type is None:\n            self.device_type = profile_result[\"device_type\"]\n        if self.device_name is None:\n            self.device_name = profile_result[\"device_name\"]\n        if self.node_id is None:\n            self.node_id = node_id\n        if self.k8s_pod_name is None:\n            self.k8s_pod_name = k8s_pod_name\n\n        if self.gradcam_method_name is None:\n            self.gradcam_method_name = gradcam_method_name\n\n        # update the max profile result\n        self.cpu_memory_usage_bytes = max(\n            self.cpu_memory_usage_bytes, profile_result[\"cpu_memory_usage\"]\n        )\n        # self cpu memory usage could be negative. here we take the value that has the max absolute value\n        if abs(profile_result[\"self_cpu_memory_usage\"]) > abs( self.self_cpu_memory_usage_bytes):\n            self.self_cpu_memory_usage_bytes = profile_result[\"self_cpu_memory_usage\"]\n        self.device_memory_usage_bytes = max(\n            self.device_memory_usage_bytes, profile_result[\"device_memory_usage\"]\n        )\n        # same as self cpu memory usage\n        if abs(profile_result[\"self_device_memory_usage\"]) > abs(self.self_device_memory_usage_bytes):\n            self.self_device_memory_usage_bytes = profile_result[\"self_device_memory_usage\"]\n        self.cpu_time_total_us = max(\n            self.cpu_time_total_us, profile_result[\"cpu_time_total\"]\n        )\n        self.self_cpu_time_total_us = max(\n            self.self_cpu_time_total_us, profile_result[\"self_cpu_time_total\"]\n        )\n        self.device_time_total_us = max(\n            self.device_time_total_us, profile_result[\"device_time_total\"]\n        )\n        self.self_device_time_total_us = max(\n            self.self_device_time_total_us, profile_result[\"self_device_time_total\"]\n        )\n\n        self.response_counter += 1\n\n    def complete_profile(self):\n        print(\"\\n--------- PROFILE EVENT ---------\\n\")\n        print(f\"Name: {self.profile_name}\")\n        print(f\"Device Type: {self.device_type}\")\n        print(f\"Device Name: {self.device_name}\")\n        print(f\"Node ID: {self.node_id}\")\n        print(f\"K8S_POD_NAME: {self.k8s_pod_name}\")\n\n        if self.gradcam_method_name:\n            print(f\"GradCAM Method Name: {self.gradcam_method_name}\")\n\n        print(\"\\n--------- LATENCY RESULT ---------\\n\")\n        print(\"Service Initialization Duration: \", self.service_initialization_duration)\n        print(\"Total Requests: \", self.response_counter)\n        print(f\"Total Time Taken: {time.time() - self.start_time:.2f} seconds\")\n        print(\n            f\"Average Time Taken: {(time.time() - self.start_time) / self.response_counter:.2f} seconds\"\n        )\n\n        print(\"\\n--------- RESOURCE USAGE ---------\\n\")\n        print(f\"CPU Memory Usage: {self.cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\")\n        print(\n            f\"Self CPU Memory Usage: {self.self_cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Device Memory Usage: {self.device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Self Device Memory Usage: {self.self_device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(f\"CPU Time Total: {self.cpu_time_total_us / 1000:.2f} ms\")\n        print(f\"Self CPU Time Total: {self.self_cpu_time_total_us / 1000:.2f} ms\")\n        print(f\"Device Time Total: {self.device_time_total_us / 1000:.2f} ms\")\n        print(f\"Self Device Time Total: {self.self_device_time_total_us / 1000:.2f} ms\")\n\n        # update the service_data.json automatically\n        with open(\"service_data.json\", \"r\") as f:\n            service_data = json.load(f)\n\n        if not self.gradcam_method_name:\n            complete_profile_data_to_save = {\n                \"node_id\": self.node_id,\n                \"device_type\": self.device_type,\n                \"device_name\": self.device_name,\n                \"initialization_time_ms\": self.service_initialization_duration * 1000,\n                \"eviction_time_ms\": 0,\n                \"initialization_cost\": 0,\n                \"keep_alive_cost\": 0,\n                \"energy_consumption_idle\": 0,\n                \"inference\": {\n                    \"cpu_time_ms\": self.cpu_time_total_us / 1000,\n                    \"device_time_ms\": self.device_time_total_us / 1000,\n                    \"cpu_memory_usage_MB\": self.cpu_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"self_cpu_memory_usage_MB\": self.self_cpu_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"device_memory_usage_MB\": self.device_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"self_device_memory_usage_MB\": self.self_device_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"energy_consumption_execution\": 0,\n                    \"disk_IO_MB\": 0,\n                    \"input_data_MB\": 0,\n                    \"output_data_MB\": 0,\n                    \"execution_time_ms\": (time.time() - self.start_time)\n                    / self.response_counter\n                    * 1000,\n                    \"execution_cost\": 0,\n                },\n            }\n\n            # check if there is already a profile for this node id\n            profile_found = False\n            for profile in service_data[\"profiles\"]:\n                if profile[\"node_id\"] == self.node_id:\n                    profile[\"inference\"] = complete_profile_data_to_save[\"inference\"]\n                    profile_found = True\n                    break\n            if not profile_found:\n                service_data[\"profiles\"].append(complete_profile_data_to_save)\n\n            # save the updated service_data.json\n            with open(\"service_data.json\", \"w\") as f:\n                json.dump(service_data, f, indent=4)\n            print(\"\\n--------- SERVICE DATA UPDATED ---------\\n\")\n\n        else:\n            complete_xai_profile_data_to_save = {\n                \"node_id\": self.node_id,\n                \"device_type\": self.device_type,\n                \"device_name\": self.device_name,\n                \"initialization_time_ms\": self.service_initialization_duration * 1000,\n                \"eviction_time_ms\": 0,\n                \"initialization_cost\": 0,\n                \"keep_alive_cost\": 0,\n                \"energy_consumption_idle\": 0,\n                \"xai\": [\n                    {\n                        \"xai_method\": self.gradcam_method_name,\n                        \"cpu_time_ms\": self.cpu_time_total_us / 1000,\n                        \"device_time_ms\": self.device_time_total_us / 1000,\n                        \"cpu_memory_usage_MB\": self.cpu_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"self_cpu_memory_usage_MB\": self.self_cpu_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"device_memory_usage_MB\": self.device_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"self_device_memory_usage_MB\": self.self_device_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"energy_consumption_execution\": 0,\n                        \"disk_IO_MB\": 0,\n                        \"input_data_MB\": 0,\n                        \"output_data_MB\": 0,\n                        \"execution_time_ms\": (time.time() - self.start_time)\n                        / self.response_counter\n                        * 1000,\n                        \"execution_cost\": 0,\n                    }\n                ],\n            }\n\n            # check if there is already a profile for this node id\n            profile_found = False\n            for profile in service_data[\"profiles\"]:\n                if profile[\"node_id\"] == self.node_id:\n                    profile_found = True\n\n                    # check if there is already a profile for this xai method\n                    xai_method_found = False\n                    if not profile.get(\"xai\"):\n                        profile[\"xai\"] = []\n                    for xai_profile in profile[\"xai\"]:\n                        if xai_profile[\"xai_method\"] == self.gradcam_method_name:\n                            xai_profile.update(complete_xai_profile_data_to_save[\"xai\"][0]) \n                            xai_method_found = True\n                            break\n                    if not xai_method_found:\n                        profile[\"xai\"].append(complete_xai_profile_data_to_save[\"xai\"][0])\n                    break\n\n            if not profile_found:\n                service_data[\"profiles\"].append(complete_xai_profile_data_to_save)\n\n            # save the updated service_data.json\n            with open(\"service_data.json\", \"w\") as f:\n                json.dump(service_data, f, indent=4)\n            print(\"\\n--------- SERVICE DATA UPDATED ---------\\n\")\n\n\ndef option_run():\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n    data = {**data, \"ue_id\": UE_ID}\n    response, process_time, node_id, pod_name = send_post_request(\n        f\"{SERVER_URL}/model/run\", data, files\n    )\n    print(\"Process Time: \", process_time)\n    print(\"Node ID: \", node_id)\n    print(\"K8S_POD_NAME: \", pod_name)\n    print(\"Response\")\n    print(json.dumps(response, indent=4))\n\n\ndef option_profile_run():\n    data = prepare_ai_service_request_data()\n    data = {**data, \"ue_id\": UE_ID}\n    files = prepare_ai_service_request_files()\n    num_requests = int(input(\"Enter the number of requests to send: \"))\n\n    try:\n        profile_result_processor = ProfileResultProcessor(SERVER_URL)\n\n        for _ in range(num_requests):\n            profile_response, process_time, node_id, k8s_node_name = send_post_request(\n                f\"{SERVER_URL}/model/profile_run\", data, files\n            )\n            print(\"Process Time: \", process_time)\n            print(\"Node ID: \", node_id)\n            print(\"K8S_POD_NAME: \", k8s_node_name)\n            print(\"Response\")\n            print(json.dumps(profile_response, indent=4))\n            if not profile_response:\n                print(\"No profile response received.\")\n                continue\n\n            profile_result_processor.process_new_response(\n                profile_response,\n                process_time=process_time,\n                node_id=node_id,\n                k8s_pod_name=k8s_node_name,\n            )\n\n        # Print the final profile result and update the service_data.json\n        profile_result_processor.complete_profile()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\n\ndef option_help():\n    \"\"\"Get help information from the server.\"\"\"\n    try:\n        response, process_time, node_id, pod_name = send_get_request(\n            f\"{SERVER_URL}/help\"\n        )\n        print(\"Process Time: \", process_time)\n        print(\"Node ID: \", node_id)\n        print(\"K8S_POD_NAME: \", pod_name)\n        print(\"Response\")\n        print(json.dumps(response, indent=4))\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n\n\ndef option_run_with_xai():\n    \"\"\"Run AI service with XAI.\"\"\"\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n\n    print(\n        \"Note that currently only GradCAM methods on image-classification models are supported.\"\n    )\n    while True:\n        gradcam_method_name = input(\n            f\"Please select a GradCAM method (options: {XAI_GRADCAM_METHODS}): \"\n        )\n        if gradcam_method_name not in XAI_GRADCAM_METHODS:\n            print(f\"Invalid GradCAM method. Please select again.\")\n        else:\n            break\n\n    # ask for target class for explanation\n    target_category_indexes = input(\n        \"Please input target category indexes for explanation (comma-separated, e.g., 111, 32, 44, ...): \"\n    )\n    if not target_category_indexes or not target_category_indexes.strip():\n        print(\n            \"No target category indexes provided. Defaulting to explaining the top confident category.\"\n        )\n        target_category_indexes = []\n    else:\n        target_category_indexes = [\n            int(i.strip()) for i in target_category_indexes.split(\",\")\n        ]\n\n    data = {\n        **data,\n        \"ue_id\": UE_ID,\n        \"gradcam_method_name\": gradcam_method_name,\n        \"target_category_indexes\": target_category_indexes,\n    }\n    print(\"Data: \", data)\n    response, process_time, node_id, k8s_pod_name = send_post_request(\n        f\"{SERVER_URL}/xai_model/run\", data, files\n    )\n    print(\"Process Time: \", process_time)\n    print(\"Node ID: \", node_id)\n    print(\"K8S_POD_NAME: \", k8s_pod_name)\n\n    # Handle JSON response\n    model_results = response.get(\"model_results\")\n    if model_results:\n        print(\"Model Results:\", json.dumps(model_results, indent=4))\n\n    xai_results = response.get(\"xai_results\")\n    if xai_results:\n        print(\"XAI Results Method:\", xai_results.get(\"xai_method\"))\n\n    # Handle binary image response\n    encoded_image = xai_results.get(\"image\")\n    if encoded_image:\n        image_bytes = base64.b64decode(encoded_image)\n\n        # Load the image into Pillow\n        image = Image.open(BytesIO(image_bytes))\n\n        # Save image to disk\n        image.save(\"xai_output.png\")\n\n        # Display the image using matplotlib\n        plt.imshow(image)\n        plt.axis(\"off\")\n        plt.show()\n\n\ndef option_profile_run_with_xai():\n    \"\"\"Run AI service with XAI and profile the run.\"\"\"\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n    print(\n        \"Note that currently only GradCAM methods on image-classification models are supported.\"\n    )\n\n    # ask for target class for explanation\n    target_category_indexes = input(\n        \"Please input target category indexes for explanation (comma-separated, e.g., 111, 32, 44, ...): \"\n    )\n    if not target_category_indexes or not target_category_indexes.strip():\n        print(\n            \"No target category indexes provided. Defaulting to explaining the top confident category.\"\n        )\n        target_category_indexes = []\n    else:\n        target_category_indexes = [\n            int(i.strip()) for i in target_category_indexes.split(\",\")\n        ]\n\n    num_requests = int(input(\"Enter the number of requests to send: \"))\n\n    try:\n        for gradcam_method_name in XAI_GRADCAM_METHODS:\n\n            data = {\n                **data,\n                \"ue_id\": UE_ID,\n                \"gradcam_method_name\": gradcam_method_name,\n                \"target_category_indexes\": target_category_indexes,\n            }\n            print(\"Data: \", data)\n\n            profile_result_processor = ProfileResultProcessor(SERVER_URL)\n\n            for _ in range(num_requests):\n                response, process_time, node_id, k8s_pod_name = send_post_request(\n                    f\"{SERVER_URL}/xai_model/profile_run\", data, files\n                )\n                print(\"Process Time: \", process_time)\n                print(\"Node ID: \", node_id)\n                print(\"K8S_POD_NAME: \", k8s_pod_name)\n                if not response:\n                    print(\"No profile response received.\")\n                    continue\n\n                # Handle JSON response\n                model_results = response.get(\"model_results\")\n                if model_results:\n                    print(\"Model Results:\", json.dumps(model_results, indent=4))\n\n                profile_result_processor.process_new_response(\n                    response,\n                    process_time=process_time,\n                    node_id=node_id,\n                    k8s_pod_name=k8s_pod_name,\n                    gradcam_method_name=gradcam_method_name,\n                )\n\n            # Print the final profile result and update the service_data.json\n            profile_result_processor.complete_profile()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\n\nOPTIONS = [\n    {\n        \"label\": \"Get help information\",\n        \"action\": option_help,\n    },\n    {\n        \"label\": \"Run AI service\",\n        \"action\": option_run,\n    },\n    {\n        \"label\": \"Profile AI service\",\n        \"action\": option_profile_run,\n    },\n    {\n        \"label\": \"Run AI service with XAI (only image-classification models)\",\n        \"action\": option_run_with_xai,\n    },\n    {\n        \"label\": \"Profile AI service with XAI (only image-classification models)\",\n        \"action\": option_profile_run_with_xai,\n    },\n]\n\n\nif __name__ == \"__main__\":\n    while True:\n        print(\"\\nOptions:\")\n        for i, option in enumerate(OPTIONS, start=1):\n            print(f\"{i}. {option['label']}\")\n        print(\"q. Quit\")\n        choice = input(\"Enter your choice: \")\n\n        if choice == \"q\":\n            print(\"Exiting the client. Goodbye!\")\n            break\n        else:\n            try:\n                choice = int(choice)\n                if 1 <= choice <= len(OPTIONS):\n                    OPTIONS[choice - 1][\"action\"]()\n                else:\n                    print(\"Invalid choice. Please try again.\")\n            except ValueError:\n                print(\"Invalid input. Please enter a number.\")\n",
        "ai_client_utils_script_content": "from typing import Dict\n\ndef prepare_ai_service_request_files() -> Dict[str, bytes]:\n    \"\"\"Prepare the `files` part for the AI service request.\"\"\"\n    files = {}\n    image_file_path = input(\"Please input the image file path: \")\n    with open(image_file_path, \"rb\") as image_file:\n        files[\"file\"] = image_file.read()\n    return files\n\ndef prepare_ai_service_request_data() -> Dict[str, str]:\n    \"\"\"Prepare the `data` part for the AI service request.\"\"\"\n    data = {}\n    ue_id = input(\"Please input the unique execution ID: \")\n    data[\"ue_id\"] = ue_id\n    return data",
        "model_script_content": "# import server utils\nfrom ai_server_utils import (\n    get_image_classification_results_from_model_output_logits,\n    profile_activities,\n    prepare_profile_results,\n)\n# import profile utils\nfrom torch.profiler import profile, record_function\n\n# import necessary libs for AI model inference and request handling\nimport torch\nfrom fastapi import APIRouter, File, Form, UploadFile\nfrom fastapi.responses import JSONResponse\nfrom transformers import AutoImageProcessor, ViTForImageClassification\nfrom PIL import Image\n\n# --------------------------------\n# Device configuration\n# --------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# --------------------------------\n# Model-specific configuration\n# make sure the variables `MODEL_NAME` and `model` are defined here.\n# --------------------------------\nMODEL_NAME = \"trpakov/vit-face-expression\"\nprocessor = AutoImageProcessor.from_pretrained(MODEL_NAME, use_fast=True)\nmodel = ViTForImageClassification.from_pretrained(MODEL_NAME).to(device)\nmodel.eval()\n\n# Initialize the FastAPI router\nrouter = APIRouter()\n\n@router.post(\"/run\")\nasync def run_model(file: UploadFile = File(...), ue_id: str = Form(...)):\n    try:\n        # Prepare the model input\n        image = Image.open(file.file).convert(\"RGB\")\n        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n\n        # Perform inference\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        # Process the model outputs\n        predictions = get_image_classification_results_from_model_output_logits(model, outputs.logits)\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"model_results\": predictions,\n            }\n        )\n    except Exception as e:\n        print(f\"Error processing file: {e}\")\n        return JSONResponse(\n            content={\"error\": \"Failed to process the image. {e}\".format(e=str(e))},\n            status_code=500,\n        )\n\n@router.post(\"/profile_run\")\nasync def profile_run(file: UploadFile = File(...), ue_id: str = Form(...)):\n    \"\"\"\n    Endpoint to profile the AI model execution.\n    \"\"\"\n    try:\n        # Prepare the model input\n        image = Image.open(file.file).convert(\"RGB\")\n        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n\n        # perform profiling\n        with profile(\n            activities=profile_activities,\n            profile_memory=True,\n        ) as prof:\n            with record_function(\"model_run\"):\n                with torch.no_grad():\n                    model_outputs = model(**inputs)\n\n        profile_result = prepare_profile_results(prof)\n\n        # Process the model outputs\n        predictions = get_image_classification_results_from_model_output_logits(model, model_outputs.logits)\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"profile_result\": profile_result,\n                \"model_results\": predictions,\n            }\n        )\n\n    except Exception as e:\n        print(f\"Error processing request: {e}\")\n        return JSONResponse(\n            content={\"error\": f\"Failed to process the request. {e}\"},\n            status_code=500,\n        )\n\n# Below are the model input and output specifications to be used by the `/help` endpoint\nMODEL_INPUT_FORM_SPEC = {\n    \"file\": {\n        \"type\": \"file upload\",\n        \"description\": \"The image file to be classified.\",\n        \"required\": True,\n        \"example\": \"face.png\",\n    }\n}\n\nMODEL_OUTPUT_JSON_SPEC = {\n    \"ue_id\": \"unique execution ID\",\n    \"model_results\": [\n        {\n            \"category_id\": \"category id\",\n            \"label\": \"category label\",\n            \"probability\": \"probability value\",\n        }\n    ],\n}",
        "healthcheck_script_content": "import requests\n\nprint(requests.get(\"http://localhost:8000/help\"))",
        "xai_model_script_content": "from typing import Callable, List, Optional\nfrom fastapi import APIRouter, File, Form, UploadFile\nfrom fastapi.responses import JSONResponse\nfrom PIL import Image\nfrom torch.profiler import profile, record_function\nfrom pytorch_grad_cam import (\n    GradCAM,\n    HiResCAM,\n    AblationCAM,\n    XGradCAM,\n    GradCAMPlusPlus,\n    ScoreCAM,\n    LayerCAM,\n    EigenCAM,\n    EigenGradCAM,\n    KPCA_CAM,\n    RandomCAM,\n)\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\nfrom PIL import Image\nimport numpy as np\nimport torch\nfrom torchvision import transforms\nfrom typing import List, Optional\n\n\n# import model utilities\nfrom ai_server_utils import (\n    encode_image,\n    prepare_profile_results,\n    get_image_classification_results_from_model_output_logits,\n    profile_activities,\n)\n\n# Currently only support GradCAM on image-classification models.\n# so we import the model directly from the model.py file\nfrom model import model, device, processor as resize_and_normalize_processor\n\nresize_only_processor = transforms.Compose(\n    [\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n    ],\n)\n\nGRADCAM_METHODS = {\n    \"GradCAM\": GradCAM,\n    \"HiResCAM\": HiResCAM,\n    \"AblationCAM\": AblationCAM,\n    \"XGradCAM\": XGradCAM,\n    \"GradCAMPlusPlus\": GradCAMPlusPlus,\n    \"ScoreCAM\": ScoreCAM,\n    \"LayerCAM\": LayerCAM,\n    \"EigenCAM\": EigenCAM,\n    \"EigenGradCAM\": EigenGradCAM,\n    \"KPCA_CAM\": KPCA_CAM,\n    \"RandomCAM\": RandomCAM,\n}\n\n\nclass HuggingfaceToTensorModelWrapper(torch.nn.Module):\n    \"\"\"Model wrapper to return a tensor\"\"\"\n\n    def __init__(self, model):\n        super(HuggingfaceToTensorModelWrapper, self).__init__()\n        self.model = model\n\n    def forward(self, x):\n        return self.model(x).logits\n\n\ndef get_model_to_tensor_wrapper_class():\n    \"\"\"Helper function to get the model wrapper class.\"\"\"\n    return HuggingfaceToTensorModelWrapper\n\n\ndef get_target_layers_for_grad_cam(model: torch.nn.Module):\n    \"\"\"Helper function to get the target layer for GradCAM.\"\"\"\n    return [model.resnet.encoder.stages[-1].layers[-1]]\n\n\ndef get_classifier_output_target_class():\n    \"\"\"Helper function to get the classifier output target class.\"\"\"\n    return ClassifierOutputTarget\n\n\ndef get_reshape_transform():\n    \"\"\"Helper function to get the reshape transform for GradCAM.\"\"\"\n    return None\n\n\ndef run_grad_cam_on_image(\n    model: torch.nn.Module,\n    target_layers: List[torch.nn.Module],\n    targets_for_gradcam: Optional[List[Callable]],\n    reshape_transform: Optional[Callable],\n    input_tensor: torch.Tensor,\n    input_image: torch.Tensor,\n    gradcam_method: Callable,\n):\n    \"\"\"Helper function to run GradCAM on an image and create a visualization.\n    Since the classification target is None, the highest scoring category will be used for every image in the batch.\n    \"\"\"\n\n    with gradcam_method(\n        model=model,\n        target_layers=target_layers,\n        reshape_transform=reshape_transform,\n    ) as cam:\n\n        # Replicate the tensor for each of the categories we want to create Grad-CAM for:\n        repeated_tensor = input_tensor[None, :].repeat(\n            (\n                1\n                if targets_for_gradcam is None or len(targets_for_gradcam) == 0\n                else len(targets_for_gradcam)\n            ),\n            1,\n            1,\n            1,\n        )\n\n        batch_results = cam(\n            input_tensor=repeated_tensor,\n            targets=(\n                None\n                if targets_for_gradcam is None or len(targets_for_gradcam) == 0\n                else targets_for_gradcam\n            ),\n        )\n        results = []\n        for grayscale_cam in batch_results:\n            # adjust the shape of the input_image from (3, 244, 244) to (244, 244, 3)\n            visualization = show_cam_on_image(\n                np.float32(input_image.permute(1, 2, 0).numpy()),\n                grayscale_cam,\n                use_rgb=True,\n            )\n            results.append(visualization)\n        output_image = Image.fromarray(np.hstack(results))\n\n        return output_image, cam.outputs\n\n\n# Initialize the FastAPI router\nrouter = APIRouter()\n\n\n@router.post(\"/run\")\nasync def run_model(\n    file: UploadFile = File(...),\n    ue_id: str = Form(...),\n    gradcam_method_name: str = Form(...),\n    target_category_indexes: Optional[List[int]] = Form(None),\n):\n    \"\"\"\n    Endpoint to run the XAI model.\"\"\"\n\n    try:\n        # Prepare the model input\n        print(\"Preparing the model input...\")\n        image = Image.open(file.file).convert(\"RGB\")\n        normalized_image_tensor = (\n            resize_and_normalize_processor(images=image, return_tensors=\"pt\")[\n                \"pixel_values\"\n            ]\n            .squeeze(0)\n            .to(device)\n        )\n        original_image_tensor = resize_only_processor(image)\n\n        if target_category_indexes is None or len(target_category_indexes) == 0:\n            targets_for_gradcam = None\n        else:\n            # Convert to output target from category indexes\n            targets_for_gradcam = [\n                ClassifierOutputTarget(index) for index in target_category_indexes\n            ]\n        assert (\n            gradcam_method_name in GRADCAM_METHODS\n        ), f\"GradCAM method '{gradcam_method_name}' is not supported. \"\n        gradcam_method = GRADCAM_METHODS[gradcam_method_name]\n\n        model_wrapper_class = get_model_to_tensor_wrapper_class()\n        target_layers = get_target_layers_for_grad_cam(model)\n        reshape_transform = get_reshape_transform()\n\n        # Perform inference\n        print(\"Running GradCAM...\")\n        xai_image, model_output_logits = run_grad_cam_on_image(\n            model=model_wrapper_class(model),\n            target_layers=target_layers,\n            targets_for_gradcam=targets_for_gradcam,\n            reshape_transform=reshape_transform,\n            input_tensor=normalized_image_tensor,\n            input_image=original_image_tensor,\n            gradcam_method=gradcam_method,\n        )\n\n        predictions = get_image_classification_results_from_model_output_logits(model, model_output_logits)\n\n        return JSONResponse(\n            {\n                \"ue_id\": ue_id,\n                \"xai_results\": {\n                    \"image\": encode_image(xai_image),\n                    \"xai_method\": gradcam_method_name,\n                },\n                \"model_results\": predictions,\n            }\n        )\n\n    except Exception as e:\n        print(f\"Error processing file: {e}\")\n        return JSONResponse(\n            content={\"error\": \"Failed to process the image. {e}\".format(e=str(e))},\n            status_code=500,\n        )\n\n\n@router.post(\"/profile_run\")\nasync def profile_run(\n    file: UploadFile = File(...),\n    ue_id: str = Form(...),\n    gradcam_method_name: str = Form(...),\n    target_category_indexes: Optional[List[int]] = Form(None),\n):\n    \"\"\"\n    Endpoint to profile the XAI run.\n    \"\"\"\n    try:\n        # Prepare the model input\n        image = Image.open(file.file).convert(\"RGB\")\n        normalized_image_tensor = (\n            resize_and_normalize_processor(images=image, return_tensors=\"pt\")[\n                \"pixel_values\"\n            ]\n            .squeeze(0)\n            .to(device)\n        )\n        original_image_tensor = resize_only_processor(image)\n        if target_category_indexes is None or len(target_category_indexes) == 0:\n            targets_for_gradcam = None\n        else:\n            # Convert to output target from category indexes\n            targets_for_gradcam = [\n                ClassifierOutputTarget(index) for index in target_category_indexes\n            ]\n\n        assert (\n            gradcam_method_name in GRADCAM_METHODS\n        ), f\"GradCAM method '{gradcam_method_name}' is not supported. \"\n        gradcam_method = GRADCAM_METHODS[gradcam_method_name]\n\n        model_wrapper_class = get_model_to_tensor_wrapper_class()\n        target_layers = get_target_layers_for_grad_cam(model)\n        reshape_transform = get_reshape_transform()\n\n        # perform profiling\n        with profile(\n            activities=profile_activities,\n            profile_memory=True,\n        ) as prof:\n            with record_function(\"xai_model_run\"):\n\n                # Perform inference\n                xai_image, model_output_logits = run_grad_cam_on_image(\n                    model=model_wrapper_class(model),\n                    target_layers=target_layers,\n                    targets_for_gradcam=targets_for_gradcam,\n                    reshape_transform=reshape_transform,\n                    input_tensor=normalized_image_tensor,\n                    input_image=original_image_tensor,\n                    gradcam_method=gradcam_method,\n                )\n\n        return JSONResponse(\n            {\n                \"ue_id\": ue_id,\n                \"xai_results\": {\n                    \"image\": encode_image(xai_image),\n                    \"xai_method\": gradcam_method_name,\n                },\n                \"model_results\": get_image_classification_results_from_model_output_logits(\n                    model, model_output_logits\n                ),\n                \"profile_result\": prepare_profile_results(prof),\n            }\n        )\n\n    except Exception as e:\n        print(f\"Error processing request: {e}\")\n        return JSONResponse(\n            content={\"error\": f\"Failed to process the request. {e}\"},\n            status_code=500,\n        )\n\n\nXAI_OUTPUT_JSON_SPEC = {\n    \"xai_results\": {\n        \"image\": \"XAI image result\",\n        \"xai_method\": \"XAI method used\",\n    }\n}\n"
    }
}