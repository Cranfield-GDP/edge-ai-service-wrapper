{
    "model_name": "hexgrad/Kokoro-82M",
    "model_url": "https://huggingface.co/hexgrad/Kokoro-82M",
    "task": "text-to-speech",
    "task_detail": "The Kokoro-82M model is an advanced text-to-speech (TTS) system designed for generating high-quality speech synthesis. Despite its relatively lightweight architecture of 82 million parameters, Kokoro-82M offers performance comparable to larger TTS models while being significantly more efficient in terms of speed and cost. Its design and open Apache licensing allow for seamless deployment across various use cases, ranging from production environments to personal projects. \n\nKey functionalities include:\n1. **Input and Output Format:** \n   - **Input:** The model accepts text input in English, and potentially other languages as well, considering the availability of diverse voices mentioned in the README. The text input should typically be structured, resembling typical written content or dialogue intended for speech synthesis.\n   - **Output:** The output is a high-quality audio file, typically in WAV format, which captures the synthesized speech corresponding to the text input. The audio is generated in real time, making use of 24kHz sampling rates for clear and natural sound quality.\n\n2. **Architecture and Processing:**\n   - Kokoro-82M is based on advanced architectures like StyleTTS 2 and ISTFTNet, which contribute to its superior voice quality and expressive speech output. Its decoder-only design streamlines the processing pathway, avoiding diffusion or encoder complexities.\n   - The model operates using a Gen2Phonemes (G2P) library called 'misaki', which assists in accurately mapping text to phonetic structures for enhanced pronunciation and sound authenticity.\n\n3. **Voice and Language Capabilities:**\n   - The model supports multiple languages, as inferred from the mention of different voices and language capabilities. It offers a vast array of voices, allowing users to choose accents and tones that fit their application needs.\n   - With 54 distinct voices available in its v1.0 iteration, the model provides substantial flexibility for achieving various voice personalities, enhancing the personalization of synthesized speech.\n\n4. **Deployment and Cost Efficiency:**\n   - Known for its cost-efficient operation, Kokoro-82M is capable of processing approximately one minute of speech from 1000 characters of input text, all at a rate of under $1 per million characters or under $0.06 per audio hour.\n   - The model can be easily deployed via APIs or integrated into cloud-based solutions, making it a versatile choice for developers and businesses seeking economical TTS solutions.\n\n5. **Training and Dataset:**\n   - Kokoro-82M was trained using a diverse set of permissively licensed and public domain audio data. This includes data from the Koniwa and SIWIS datasets, ensuring a broad range of phonetic and prosodic features are captured.\n   - The training process was efficiently managed with a budget of approximately $1000, utilizing 1000 hours of high-powered GPU processing, highlighting its development within reasonable resource constraints.\n\nOverall, Kokoro-82M presents a powerful yet accessible tool for those needing advanced TTS capabilities, catering to a wide range of applications from artificial character voices in gaming to automated voice services in customer support systems. Its blend of performance, adaptability, and cost-efficiency make it a compelling option for both developers and enterprises looking to incorporate voice generation technology into their offerings.",
    "accuracy_info": "No accuracy information found.",
    "image_repository_url": "docker.io/cranfield6g/cranfield-edge-hexgrad-kokoro-82m",
    "service_disk_size_bytes": 3874193653,
    "profiles": [
        {
            "node_id": "LAP004262",
            "device_type": "DeviceType.CPU",
            "device_name": "None",
            "initialization_time_ms": 2555.1772117614746,
            "eviction_time_ms": 0,
            "initialization_cost": 0,
            "keep_alive_cost": 0,
            "energy_consumption_idle": 0,
            "inference": {
                "cpu_time_ms": 2358.8447149999997,
                "device_time_ms": 0.0,
                "cpu_memory_usage_MB": 532.6574363708496,
                "self_cpu_memory_usage_MB": -1976.697434425354,
                "device_memory_usage_MB": 0.0,
                "self_device_memory_usage_MB": 0.0,
                "energy_consumption_execution": 0,
                "disk_IO_MB": 0,
                "input_data_MB": 0,
                "output_data_MB": 0,
                "execution_time_ms": 4230.652650197347,
                "execution_cost": 0
            },
            "idle_container_cpu_memory_usage": "3.2GB",
            "idle_container_device_memory_usage": "0GB"
        }
    ],
    "feedback": {
        "likes": [],
        "dislikes": [],
        "comments": []
    },
    "code": {
        "readme_content": "---\nlicense: apache-2.0\nlanguage:\n- en\nbase_model:\n- yl4579/StyleTTS2-LJSpeech\npipeline_tag: text-to-speech\n---\n**Kokoro** is an open-weight TTS model with 82 million parameters. Despite its lightweight architecture, it delivers comparable quality to larger models while being significantly faster and more cost-efficient. With Apache-licensed weights, Kokoro can be deployed anywhere from production environments to personal projects.\n\n<audio controls><source src=\"https://huggingface.co/hexgrad/Kokoro-82M/resolve/main/samples/HEARME.wav\" type=\"audio/wav\"></audio>\n\n\ud83d\udc08 **GitHub**: https://github.com/hexgrad/kokoro\n\n\ud83d\ude80 **Demo**: https://hf.co/spaces/hexgrad/Kokoro-TTS\n\n> [!NOTE]\n> As of April 2025, the market rate of Kokoro served over API is **under $1 per million characters of text input**, or under $0.06 per hour of audio output. (On average, 1000 characters of input is about 1 minute of output.) Sources: [ArtificialAnalysis/Replicate at 65 cents per M chars](https://artificialanalysis.ai/text-to-speech/model-family/kokoro#price) and [DeepInfra at 80 cents per M chars](https://deepinfra.com/hexgrad/Kokoro-82M).\n>\n> This is an Apache-licensed model, and Kokoro has been deployed in numerous projects and commercial APIs. We welcome the deployment of the model in real use cases.\n\n> [!CAUTION]\n> Fake websites like kokorottsai_com (snapshot: https://archive.ph/nRRnk) and kokorotts_net (snapshot: https://archive.ph/60opa) are likely scams masquerading under the banner of a popular model.\n>\n> Any website containing \"kokoro\" in its root domain (e.g. kokorottsai_com, kokorotts_net) is **NOT owned by and NOT affiliated with this model page or its author**, and attempts to imply otherwise are red flags.\n\n- [Releases](#releases)\n- [Usage](#usage)\n- [EVAL.md](https://huggingface.co/hexgrad/Kokoro-82M/blob/main/EVAL.md) \u2197\ufe0f\n- [SAMPLES.md](https://huggingface.co/hexgrad/Kokoro-82M/blob/main/SAMPLES.md) \u2197\ufe0f\n- [VOICES.md](https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md) \u2197\ufe0f\n- [Model Facts](#model-facts)\n- [Training Details](#training-details)\n- [Creative Commons Attribution](#creative-commons-attribution)\n- [Acknowledgements](#acknowledgements)\n\n### Releases\n\n| Model | Published | Training Data | Langs & Voices | SHA256 |\n| ----- | --------- | ------------- | -------------- | ------ |\n| **v1.0** | **2025 Jan 27** | **Few hundred hrs** | [**8 & 54**](https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md) | `496dba11` |\n| [v0.19](https://huggingface.co/hexgrad/kLegacy/tree/main/v0.19) | 2024 Dec 25 | <100 hrs | 1 & 10 | `3b0c392f` |\n\n| Training Costs | v0.19 | v1.0 | **Total** |\n| -------------- | ----- | ---- | ----- |\n| in A100 80GB GPU hours | 500 | 500 | **1000** |\n| average hourly rate | $0.80/h | $1.20/h | **$1/h** |\n| in USD | $400 | $600 | **$1000** |\n\n### Usage\nYou can run this basic cell on [Google Colab](https://colab.research.google.com/). [Listen to samples](https://huggingface.co/hexgrad/Kokoro-82M/blob/main/SAMPLES.md). For more languages and details, see [Advanced Usage](https://github.com/hexgrad/kokoro?tab=readme-ov-file#advanced-usage).\n```py\n!pip install -q kokoro>=0.9.2 soundfile\n!apt-get -qq -y install espeak-ng > /dev/null 2>&1\nfrom kokoro import KPipeline\nfrom IPython.display import display, Audio\nimport soundfile as sf\nimport torch\npipeline = KPipeline(lang_code='a')\ntext = '''\n[Kokoro](/k\u02c8Ok\u0259\u0279O/) is an open-weight TTS model with 82 million parameters. Despite its lightweight architecture, it delivers comparable quality to larger models while being significantly faster and more cost-efficient. With Apache-licensed weights, [Kokoro](/k\u02c8Ok\u0259\u0279O/) can be deployed anywhere from production environments to personal projects.\n'''\ngenerator = pipeline(text, voice='af_heart')\nfor i, (gs, ps, audio) in enumerate(generator):\n    print(i, gs, ps)\n    display(Audio(data=audio, rate=24000, autoplay=i==0))\n    sf.write(f'{i}.wav', audio, 24000)\n```\nUnder the hood, `kokoro` uses [`misaki`](https://pypi.org/project/misaki/), a G2P library at https://github.com/hexgrad/misaki\n\n### Model Facts\n\n**Architecture:**\n- StyleTTS 2: https://arxiv.org/abs/2306.07691\n- ISTFTNet: https://arxiv.org/abs/2203.02395\n- Decoder only: no diffusion, no encoder release\n\n**Architected by:** Li et al @ https://github.com/yl4579/StyleTTS2\n\n**Trained by**: `@rzvzn` on Discord\n\n**Languages:** Multiple\n\n**Model SHA256 Hash:** `496dba118d1a58f5f3db2efc88dbdc216e0483fc89fe6e47ee1f2c53f18ad1e4`\n\n### Training Details\n\n**Data:** Kokoro was trained exclusively on **permissive/non-copyrighted audio data** and IPA phoneme labels. Examples of permissive/non-copyrighted audio include:\n- Public domain audio\n- Audio licensed under Apache, MIT, etc\n- Synthetic audio<sup>[1]</sup> generated by closed<sup>[2]</sup> TTS models from large providers<br/>\n[1] https://copyright.gov/ai/ai_policy_guidance.pdf<br/>\n[2] No synthetic audio from open TTS models or \"custom voice clones\"\n\n**Total Dataset Size:** A few hundred hours of audio\n\n**Total Training Cost:** About $1000 for 1000 hours of A100 80GB vRAM\n\n### Creative Commons Attribution\n\nThe following CC BY audio was part of the dataset used to train Kokoro v1.0.\n\n| Audio Data | Duration Used | License | Added to Training Set After |\n| ---------- | ------------- | ------- | --------------------------- |\n| [Koniwa](https://github.com/koniwa/koniwa) `tnc` | <1h | [CC BY 3.0](https://creativecommons.org/licenses/by/3.0/deed.ja) | v0.19 / 22 Nov 2024 |\n| [SIWIS](https://datashare.ed.ac.uk/handle/10283/2353) | <11h | [CC BY 4.0](https://datashare.ed.ac.uk/bitstream/handle/10283/2353/license_text) | v0.19 / 22 Nov 2024 |\n\n### Acknowledgements\n\n- \ud83d\udee0\ufe0f [@yl4579](https://huggingface.co/yl4579) for architecting StyleTTS 2.\n- \ud83c\udfc6 [@Pendrokar](https://huggingface.co/Pendrokar) for adding Kokoro as a contender in the TTS Spaces Arena.\n- \ud83d\udcca Thank you to everyone who contributed synthetic training data.\n- \u2764\ufe0f Special thanks to all compute sponsors.\n- \ud83d\udc7e Discord server: https://discord.gg/QuGxSWBfQy\n- \ud83e\udebd Kokoro is a Japanese word that translates to \"heart\" or \"spirit\". It is also the name of an [AI in the Terminator franchise](https://terminator.fandom.com/wiki/Kokoro).\n\n<img src=\"https://static0.gamerantimages.com/wordpress/wp-content/uploads/2024/08/terminator-zero-41-1.jpg\" width=\"400\" alt=\"kokoro\" />\n",
        "dockerfile_content": "# Base image for AI service powered by HuggingFace pre-trained AI models.\n# the image has the following packages/libraries installed already:\n# - python3.12, pip, git, fastapi, uvicorn, torch, torchvision, opencv-python, transformers, python-multipart, Pillow, requests\nFROM python3.12_ai_service_base:latest\n\n# Set working directory\nWORKDIR /app\n\n# Copy application code\nCOPY . .\n\nRUN apt-get update && apt-get install -y espeak-ng\n\n# Install additional dependencies\nRUN pip install kokoro>=0.9.2 soundfile misaki\n\n# Expose port 8000\nEXPOSE 8000\n\n# Start the FastAPI server\nCMD [\"uvicorn\", \"ai_server:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--timeout-keep-alive\", \"600\"]",
        "ai_server_script_content": "import json\nimport os\nimport time\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import JSONResponse\nfrom contextlib import asynccontextmanager\nfrom ai_server_utils import (\n    PROFILE_OUTPUT_JSON_SPEC,\n    NODE_ID,\n    K8S_POD_NAME,\n)\n\n\n# -------------------------------------------\n# App Lifespan setup\n# -------------------------------------------\n# Record the script start time (when uvicorn starts the process)\nSCRIPT_START_TIME = time.time()\nINITIALIZATION_DURATION = 0.0\nservice_endpoint_specs = {\n    \"model_input_form_spec\": None,\n    \"model_output_json_spec\": None,\n    \"profile_output_json_spec\": None,\n    \"xai_model_input_form_spec\": None,\n    \"xai_model_output_json_spec\": None,\n    \"xai_profile_output_json_spec\": None,\n}\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n\n    global INITIALIZATION_DURATION\n    global SCRIPT_START_TIME\n    global service_endpoint_specs\n\n    # Load the AI model\n    print(\"Loading AI model...\")\n    from model import (\n        MODEL_INPUT_FORM_SPEC,\n        MODEL_OUTPUT_JSON_SPEC,\n        router as model_router,\n    )\n\n    service_endpoint_specs[\"model_input_form_spec\"] = MODEL_INPUT_FORM_SPEC\n    service_endpoint_specs[\"model_output_json_spec\"] = MODEL_OUTPUT_JSON_SPEC\n    service_endpoint_specs[\"profile_output_json_spec\"] = PROFILE_OUTPUT_JSON_SPEC\n\n    app.include_router(model_router, prefix=\"/model\", tags=[\"AI Model\"])\n\n    # Load the XAI model\n    if os.path.exists(os.path.dirname(__file__) + \"/xai_model.py\"):\n        print(\"Loading XAI model...\")\n        from xai_model import (\n            XAI_OUTPUT_JSON_SPEC,\n            router as xai_model_router,\n        )\n\n        # by default, the xai_model input form spec is the same as the model input form spec\n        service_endpoint_specs[\"xai_model_input_form_spec\"] = MODEL_INPUT_FORM_SPEC\n        service_endpoint_specs[\"xai_model_output_json_spec\"] = MODEL_OUTPUT_JSON_SPEC\n        service_endpoint_specs[\"xai_model_output_json_spec\"].update(\n            XAI_OUTPUT_JSON_SPEC\n        )\n        service_endpoint_specs[\"xai_profile_output_json_spec\"] = (\n            PROFILE_OUTPUT_JSON_SPEC\n        )\n        service_endpoint_specs[\"xai_profile_output_json_spec\"].update(\n            XAI_OUTPUT_JSON_SPEC\n        )\n\n        app.include_router(xai_model_router, prefix=\"/xai_model\", tags=[\"XAI Model\"])\n\n    # Record the initialization duration\n    INITIALIZATION_DURATION = time.time() - SCRIPT_START_TIME\n\n    print(f\"AI service loaded in {INITIALIZATION_DURATION:.2f} seconds.\")\n\n    yield\n\n    # Clean up the models and release the resources\n    service_endpoint_specs.clear()\n\n\n# -------------------------------------------\n# FastAPI application setup\n# -------------------------------------------\napp = FastAPI(lifespan=lifespan)\n\n\n# -------------------------------------------\n# Middlewares\n# -------------------------------------------\n@app.middleware(\"http\")\nasync def prepare_header_middleware(request: Request, call_next):\n    start_time = time.perf_counter()\n    response = await call_next(request)\n    process_time = time.perf_counter() - start_time\n    response.headers[\"X-Process-Time\"] = str(process_time)\n    response.headers[\"X-NODE-ID\"] = NODE_ID\n    response.headers[\"X-K8S-POD-NAME\"] = K8S_POD_NAME\n    return response\n\n\n# -------------------------------------------\n# General Endpoints\n# -------------------------------------------\n@app.get(\"/initialization_duration\")\ndef get_initialization_duration():\n    \"\"\"\n    Endpoint to retrieve the initialization duration of the AI model.\n    \"\"\"\n    global INITIALIZATION_DURATION\n\n    if INITIALIZATION_DURATION == 0.0:\n        return JSONResponse(\n            content={\"error\": \"Model not initialized.\"},\n            status_code=500,\n        )\n    return JSONResponse(\n        content={\n            \"initialization_duration\": INITIALIZATION_DURATION,\n            \"script_start_time\": SCRIPT_START_TIME,\n        }\n    )\n\n\n@app.get(\"/help\")\ndef get_help():\n    global service_endpoint_specs\n    help_info = {\n        \"endpoints\": {\n            \"/model/run\": {\n                \"method\": \"POST\",\n                \"description\": \"Executes the AI model with the provided input data.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_output_json_spec\"],\n                },\n            },\n            \"/model/profile_run\": {\n                \"method\": \"POST\",\n                \"description\": \"Profiles the AI model execution.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    \"profile_result\": \"Profiling results of the AI model execution.\",\n                    **service_endpoint_specs[\"profile_output_json_spec\"],\n                },\n            },\n            \"/initialization_duration\": {\n                \"method\": \"GET\",\n                \"description\": \"Retrieves the initialization duration of the AI model.\",\n                \"response\": {\n                    \"initialization_duration\": \"Time taken to initialize the model (in seconds).\",\n                    \"script_start_time\": \"Timestamp when the script started (in seconds since epoch).\",\n                },\n            },\n        }\n    }\n\n    if service_endpoint_specs[\"xai_model_input_form_spec\"] is not None:\n        help_info[\"endpoints\"][\"/xai_model/run\"] = {\n            \"method\": \"POST\",\n            \"description\": \"Executes the XAI model with the provided input data.\",\n            \"parameters\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_input_form_spec\"],\n            },\n            \"response\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_output_json_spec\"],\n            },\n        }\n\n        help_info[\"endpoints\"][\"/xai_model/profile_run\"] = {\n            \"method\": \"POST\",\n            \"description\": \"Profiles the XAI model execution.\",\n            \"parameters\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_input_form_spec\"],\n            },\n            \"response\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                \"profile_result\": \"Profiling results of the XAI model execution.\",\n                **service_endpoint_specs[\"xai_profile_output_json_spec\"],\n            },\n        }\n    \n    return JSONResponse(content=help_info)\n",
        "ai_server_utils_script_content": "import os\nimport socket\nimport torch\nfrom io import BytesIO\nimport base64\n\nfrom torch.profiler import profile, ProfilerActivity, record_function\n\n\n# -------------------------------------------\n# ENV Variables\n# -------------------------------------------\nNODE_ID = os.getenv(\"NODE_ID\", socket.gethostname())\nK8S_POD_NAME = os.getenv(\"K8S_POD_NAME\", \"UNKNOWN\")\n\n\n# -------------------------------------------\n# Profile Utils\n# -------------------------------------------\nprofile_activities = [\n    ProfilerActivity.CPU,\n    ProfilerActivity.CUDA,\n    ProfilerActivity.MTIA,\n    ProfilerActivity.XPU,\n]\n\ndef get_image_classification_results_from_model_output_logits(model, model_output_logits):\n    \"\"\"\n    Process the model outputs to prepare for the response.\n    \"\"\"\n    probabilities = torch.nn.functional.softmax(model_output_logits[0], dim=0)\n\n    # Return the top 5 predictions with labels\n    top5_prob, top5_catid = torch.topk(probabilities, 5)\n    predictions = []\n    for i in range(top5_prob.size(0)):\n        category_id = top5_catid[i].item()\n        predictions.append(\n            {\n                \"category_id\": category_id,\n                \"label\": model.config.id2label[category_id],\n                \"probability\": top5_prob[i].item(),\n            }\n        )\n    return predictions\n\ndef prepare_profile_results(prof):\n    \"\"\"\n    Prepare the profile results for the model inputs and outputs.\n    \"\"\"\n    print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n\n    profile_event = prof.key_averages()[0]\n\n    profile_result = {\n        \"name\": profile_event.key,\n        \"device_type\": str(profile_event.device_type),\n        \"device_name\": str(profile_event.use_device),\n        \"cpu_memory_usage\": profile_event.cpu_memory_usage,\n        \"self_cpu_memory_usage\": profile_event.self_cpu_memory_usage,\n        \"device_memory_usage\": profile_event.device_memory_usage,\n        \"self_device_memory_usage\": profile_event.self_device_memory_usage,\n        \"cpu_time_total\": profile_event.cpu_time_total,\n        \"self_cpu_time_total\": profile_event.self_cpu_time_total,\n        \"device_time_total\": profile_event.device_time_total,\n        \"self_device_time_total\": profile_event.self_device_time_total,\n    }\n    return profile_result\n\n\ndef encode_image(image):\n    \"\"\"\n    Encode the image to bytes\n    \"\"\"\n    buffered = BytesIO()\n    image.save(buffered, format=\"PNG\")\n    encoded_image = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n    return encoded_image\n\n\nPROFILE_OUTPUT_JSON_SPEC = {\n    \"ue_id\": \"unique execution ID\",\n    \"profile_result\": {\n        \"name\": \"name of the profile event\",\n        \"device_type\": \"type of device used (e.g., CPU, GPU, ...)\",\n        \"device_name\": \"name of the device used\",\n        \"cpu_memory_usage\": \"CPU memory usage in bytes\",\n        \"self_cpu_memory_usage\": \"self CPU memory usage in bytes\",\n        \"device_memory_usage\": \"device memory usage in bytes\",\n        \"self_device_memory_usage\": \"self device memory usage in bytes\",\n        \"cpu_time_total\": \"total CPU time in microseconds\",\n        \"self_cpu_time_total\": \"self total CPU time in microseconds\",\n        \"device_time_total\": \"total device time in microseconds\",\n        \"self_device_time_total\": \"self total device time in microseconds\",\n    },\n    \"model_results\": \"the AI service model results\",\n}\n",
        "ai_client_script_content": "import base64\nimport json\nimport time\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nfrom ai_client_utils import (\n    prepare_ai_service_request_files,\n    prepare_ai_service_request_data,\n)\n\nXAI_GRADCAM_METHODS = [\n    \"GradCAM\",\n    \"HiResCAM\",\n    \"AblationCAM\",\n    \"XGradCAM\",\n    \"GradCAMPlusPlus\",\n    \"ScoreCAM\",\n    \"LayerCAM\",\n    \"EigenCAM\",\n    \"EigenGradCAM\",\n    \"KPCA_CAM\",\n    \"RandomCAM\",\n]\n\n# -------------------------------------\n# prompt for necessary inputs\n# -------------------------------------\nSERVER_URL = input(\"Please input server URL (default to http://localhost:9000): \")\nif SERVER_URL.strip() == \"\":\n    SERVER_URL = \"http://localhost:9000\"\nUE_ID = input(\"Please input UE_ID (default to 123456): \")\nif UE_ID.strip() == \"\":\n    UE_ID = \"123456\"\n\n\ndef send_post_request(url, data, files):\n    \"\"\"Send request to run AI service and display AI service responses.\"\"\"\n    try:\n        response = requests.post(url, files=files, data=data)\n        # get the process time, node id and k8s pod name from the response headers\n        process_time = response.headers.get(\"X-Process-Time\")\n        node_id = response.headers.get(\"X-NODE-ID\")\n        k8s_pod_name = response.headers.get(\"X-K8S-POD-NAME\")\n        if response.status_code == 200:\n            return response.json(), process_time, node_id, k8s_pod_name\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n\ndef send_get_request(url, params=None):\n    \"\"\"Send GET request to the specified URL and return the response.\"\"\"\n    try:\n        response = requests.get(url, params=params)\n        process_time = response.headers.get(\"X-Process-Time\")\n        node_id = response.headers.get(\"X-NODE-ID\")\n        k8s_pod_name = response.headers.get(\"X-K8S-POD-NAME\")\n        if response.status_code == 200:\n            return response.json(), process_time, node_id, k8s_pod_name\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n            return None\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n\nclass ProfileResultProcessor:\n\n    def __init__(self, server_url):\n        self.server_url = server_url\n        self.start_time = time.time()\n        self.service_initialization_duration = 0\n        self.response_counter = 0\n        self.profile_name = None\n        self.device_type = None\n        self.device_name = None\n        self.node_id = None\n        self.k8s_pod_name = None\n        self.cpu_memory_usage_bytes = 0\n        self.self_cpu_memory_usage_bytes = 0\n        self.device_memory_usage_bytes = 0\n        self.self_device_memory_usage_bytes = 0\n        self.cpu_time_total_us = 0\n        self.self_cpu_time_total_us = 0\n        self.device_time_total_us = 0\n        self.self_device_time_total_us = 0\n\n        # xai related\n        self.gradcam_method_name = None\n\n        self.fetch_service_initialization_duration()\n\n    def fetch_service_initialization_duration(self):\n        \"\"\"Fetch the service initialization duration from the server.\"\"\"\n        response, process_time, node_id, k8s_pod_name = send_get_request(\n            f\"{self.server_url}/initialization_duration\"\n        )\n        if response:\n            self.service_initialization_duration = response.get(\n                \"initialization_duration\", 0\n            )\n        else:\n            print(\"Failed to fetch initialization duration.\")\n            self.service_initialization_duration = 0\n\n    def process_new_response(\n        self,\n        profile_response,\n        process_time=None,\n        node_id=None,\n        k8s_pod_name=None,\n        gradcam_method_name=None,\n    ):\n        if not profile_response:\n            return\n\n        profile_result = profile_response[\"profile_result\"]\n\n        if not profile_result:\n            return\n\n        if self.profile_name is None:\n            self.profile_name = profile_result[\"name\"]\n        if self.device_type is None:\n            self.device_type = profile_result[\"device_type\"]\n        if self.device_name is None:\n            self.device_name = profile_result[\"device_name\"]\n        if self.node_id is None:\n            self.node_id = node_id\n        if self.k8s_pod_name is None:\n            self.k8s_pod_name = k8s_pod_name\n\n        if self.gradcam_method_name is None:\n            self.gradcam_method_name = gradcam_method_name\n\n        # update the max profile result\n        self.cpu_memory_usage_bytes = max(\n            self.cpu_memory_usage_bytes, profile_result[\"cpu_memory_usage\"]\n        )\n        # self cpu memory usage could be negative. here we take the value that has the max absolute value\n        if abs(profile_result[\"self_cpu_memory_usage\"]) > abs( self.self_cpu_memory_usage_bytes):\n            self.self_cpu_memory_usage_bytes = profile_result[\"self_cpu_memory_usage\"]\n        self.device_memory_usage_bytes = max(\n            self.device_memory_usage_bytes, profile_result[\"device_memory_usage\"]\n        )\n        # same as self cpu memory usage\n        if abs(profile_result[\"self_device_memory_usage\"]) > abs(self.self_device_memory_usage_bytes):\n            self.self_device_memory_usage_bytes = profile_result[\"self_device_memory_usage\"]\n        self.cpu_time_total_us = max(\n            self.cpu_time_total_us, profile_result[\"cpu_time_total\"]\n        )\n        self.self_cpu_time_total_us = max(\n            self.self_cpu_time_total_us, profile_result[\"self_cpu_time_total\"]\n        )\n        self.device_time_total_us = max(\n            self.device_time_total_us, profile_result[\"device_time_total\"]\n        )\n        self.self_device_time_total_us = max(\n            self.self_device_time_total_us, profile_result[\"self_device_time_total\"]\n        )\n\n        self.response_counter += 1\n\n    def complete_profile(self):\n        print(\"\\n--------- PROFILE EVENT ---------\\n\")\n        print(f\"Name: {self.profile_name}\")\n        print(f\"Device Type: {self.device_type}\")\n        print(f\"Device Name: {self.device_name}\")\n        print(f\"Node ID: {self.node_id}\")\n        print(f\"K8S_POD_NAME: {self.k8s_pod_name}\")\n\n        if self.gradcam_method_name:\n            print(f\"GradCAM Method Name: {self.gradcam_method_name}\")\n\n        print(\"\\n--------- LATENCY RESULT ---------\\n\")\n        print(\"Service Initialization Duration: \", self.service_initialization_duration)\n        print(\"Total Requests: \", self.response_counter)\n        print(f\"Total Time Taken: {time.time() - self.start_time:.2f} seconds\")\n        print(\n            f\"Average Time Taken: {(time.time() - self.start_time) / self.response_counter:.2f} seconds\"\n        )\n\n        print(\"\\n--------- RESOURCE USAGE ---------\\n\")\n        print(f\"CPU Memory Usage: {self.cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\")\n        print(\n            f\"Self CPU Memory Usage: {self.self_cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Device Memory Usage: {self.device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Self Device Memory Usage: {self.self_device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(f\"CPU Time Total: {self.cpu_time_total_us / 1000:.2f} ms\")\n        print(f\"Self CPU Time Total: {self.self_cpu_time_total_us / 1000:.2f} ms\")\n        print(f\"Device Time Total: {self.device_time_total_us / 1000:.2f} ms\")\n        print(f\"Self Device Time Total: {self.self_device_time_total_us / 1000:.2f} ms\")\n\n        # update the service_data.json automatically\n        with open(\"service_data.json\", \"r\") as f:\n            service_data = json.load(f)\n\n        if not self.gradcam_method_name:\n            complete_profile_data_to_save = {\n                \"node_id\": self.node_id,\n                \"device_type\": self.device_type,\n                \"device_name\": self.device_name,\n                \"initialization_time_ms\": self.service_initialization_duration * 1000,\n                \"eviction_time_ms\": 0,\n                \"initialization_cost\": 0,\n                \"keep_alive_cost\": 0,\n                \"energy_consumption_idle\": 0,\n                \"inference\": {\n                    \"cpu_time_ms\": self.cpu_time_total_us / 1000,\n                    \"device_time_ms\": self.device_time_total_us / 1000,\n                    \"cpu_memory_usage_MB\": self.cpu_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"self_cpu_memory_usage_MB\": self.self_cpu_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"device_memory_usage_MB\": self.device_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"self_device_memory_usage_MB\": self.self_device_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"energy_consumption_execution\": 0,\n                    \"disk_IO_MB\": 0,\n                    \"input_data_MB\": 0,\n                    \"output_data_MB\": 0,\n                    \"execution_time_ms\": (time.time() - self.start_time)\n                    / self.response_counter\n                    * 1000,\n                    \"execution_cost\": 0,\n                },\n            }\n\n            # check if there is already a profile for this node id\n            profile_found = False\n            for profile in service_data[\"profiles\"]:\n                if profile[\"node_id\"] == self.node_id:\n                    profile[\"inference\"] = complete_profile_data_to_save[\"inference\"]\n                    profile_found = True\n                    break\n            if not profile_found:\n                service_data[\"profiles\"].append(complete_profile_data_to_save)\n\n            # save the updated service_data.json\n            with open(\"service_data.json\", \"w\") as f:\n                json.dump(service_data, f, indent=4)\n            print(\"\\n--------- SERVICE DATA UPDATED ---------\\n\")\n\n        else:\n            complete_xai_profile_data_to_save = {\n                \"node_id\": self.node_id,\n                \"device_type\": self.device_type,\n                \"device_name\": self.device_name,\n                \"initialization_time_ms\": self.service_initialization_duration * 1000,\n                \"eviction_time_ms\": 0,\n                \"initialization_cost\": 0,\n                \"keep_alive_cost\": 0,\n                \"energy_consumption_idle\": 0,\n                \"xai\": [\n                    {\n                        \"xai_method\": self.gradcam_method_name,\n                        \"cpu_time_ms\": self.cpu_time_total_us / 1000,\n                        \"device_time_ms\": self.device_time_total_us / 1000,\n                        \"cpu_memory_usage_MB\": self.cpu_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"self_cpu_memory_usage_MB\": self.self_cpu_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"device_memory_usage_MB\": self.device_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"self_device_memory_usage_MB\": self.self_device_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"energy_consumption_execution\": 0,\n                        \"disk_IO_MB\": 0,\n                        \"input_data_MB\": 0,\n                        \"output_data_MB\": 0,\n                        \"execution_time_ms\": (time.time() - self.start_time)\n                        / self.response_counter\n                        * 1000,\n                        \"execution_cost\": 0,\n                    }\n                ],\n            }\n\n            # check if there is already a profile for this node id\n            profile_found = False\n            for profile in service_data[\"profiles\"]:\n                if profile[\"node_id\"] == self.node_id:\n                    profile_found = True\n\n                    # check if there is already a profile for this xai method\n                    xai_method_found = False\n                    if not profile.get(\"xai\"):\n                        profile[\"xai\"] = []\n                    for xai_profile in profile[\"xai\"]:\n                        if xai_profile[\"xai_method\"] == self.gradcam_method_name:\n                            xai_profile.update(complete_xai_profile_data_to_save[\"xai\"][0]) \n                            xai_method_found = True\n                            break\n                    if not xai_method_found:\n                        profile[\"xai\"].append(complete_xai_profile_data_to_save[\"xai\"][0])\n                    break\n\n            if not profile_found:\n                service_data[\"profiles\"].append(complete_xai_profile_data_to_save)\n\n            # save the updated service_data.json\n            with open(\"service_data.json\", \"w\") as f:\n                json.dump(service_data, f, indent=4)\n            print(\"\\n--------- SERVICE DATA UPDATED ---------\\n\")\n\n\ndef option_run():\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n    data = {**data, \"ue_id\": UE_ID}\n    response, process_time, node_id, pod_name = send_post_request(\n        f\"{SERVER_URL}/model/run\", data, files\n    )\n    print(\"Process Time: \", process_time)\n    print(\"Node ID: \", node_id)\n    print(\"K8S_POD_NAME: \", pod_name)\n    print(\"Response\")\n    print(json.dumps(response, indent=4))\n\n\ndef option_profile_run():\n    data = prepare_ai_service_request_data()\n    data = {**data, \"ue_id\": UE_ID}\n    files = prepare_ai_service_request_files()\n    num_requests = int(input(\"Enter the number of requests to send: \"))\n\n    try:\n        profile_result_processor = ProfileResultProcessor(SERVER_URL)\n\n        for _ in range(num_requests):\n            profile_response, process_time, node_id, k8s_node_name = send_post_request(\n                f\"{SERVER_URL}/model/profile_run\", data, files\n            )\n            print(\"Process Time: \", process_time)\n            print(\"Node ID: \", node_id)\n            print(\"K8S_POD_NAME: \", k8s_node_name)\n            print(\"Response\")\n            print(json.dumps(profile_response, indent=4))\n            if not profile_response:\n                print(\"No profile response received.\")\n                continue\n\n            profile_result_processor.process_new_response(\n                profile_response,\n                process_time=process_time,\n                node_id=node_id,\n                k8s_pod_name=k8s_node_name,\n            )\n\n        # Print the final profile result and update the service_data.json\n        profile_result_processor.complete_profile()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\n\ndef option_help():\n    \"\"\"Get help information from the server.\"\"\"\n    try:\n        response, process_time, node_id, pod_name = send_get_request(\n            f\"{SERVER_URL}/help\"\n        )\n        print(\"Process Time: \", process_time)\n        print(\"Node ID: \", node_id)\n        print(\"K8S_POD_NAME: \", pod_name)\n        print(\"Response\")\n        print(json.dumps(response, indent=4))\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n\n\ndef option_run_with_xai():\n    \"\"\"Run AI service with XAI.\"\"\"\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n\n    print(\n        \"Note that currently only GradCAM methods on image-classification models are supported.\"\n    )\n    while True:\n        gradcam_method_name = input(\n            f\"Please select a GradCAM method (options: {XAI_GRADCAM_METHODS}): \"\n        )\n        if gradcam_method_name not in XAI_GRADCAM_METHODS:\n            print(f\"Invalid GradCAM method. Please select again.\")\n        else:\n            break\n\n    # ask for target class for explanation\n    target_category_indexes = input(\n        \"Please input target category indexes for explanation (comma-separated, e.g., 111, 32, 44, ...): \"\n    )\n    if not target_category_indexes or not target_category_indexes.strip():\n        print(\n            \"No target category indexes provided. Defaulting to explaining the top confident category.\"\n        )\n        target_category_indexes = []\n    else:\n        target_category_indexes = [\n            int(i.strip()) for i in target_category_indexes.split(\",\")\n        ]\n\n    data = {\n        **data,\n        \"ue_id\": UE_ID,\n        \"gradcam_method_name\": gradcam_method_name,\n        \"target_category_indexes\": target_category_indexes,\n    }\n    print(\"Data: \", data)\n    response, process_time, node_id, k8s_pod_name = send_post_request(\n        f\"{SERVER_URL}/xai_model/run\", data, files\n    )\n    print(\"Process Time: \", process_time)\n    print(\"Node ID: \", node_id)\n    print(\"K8S_POD_NAME: \", k8s_pod_name)\n\n    # Handle JSON response\n    model_results = response.get(\"model_results\")\n    if model_results:\n        print(\"Model Results:\", json.dumps(model_results, indent=4))\n\n    xai_results = response.get(\"xai_results\")\n    if xai_results:\n        print(\"XAI Results Method:\", xai_results.get(\"xai_method\"))\n\n    # Handle binary image response\n    encoded_image = xai_results.get(\"image\")\n    if encoded_image:\n        image_bytes = base64.b64decode(encoded_image)\n\n        # Load the image into Pillow\n        image = Image.open(BytesIO(image_bytes))\n\n        # Save image to disk\n        image.save(\"xai_output.png\")\n\n        # Display the image using matplotlib\n        plt.imshow(image)\n        plt.axis(\"off\")\n        plt.show()\n\n\ndef option_profile_run_with_xai():\n    \"\"\"Run AI service with XAI and profile the run.\"\"\"\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n    print(\n        \"Note that currently only GradCAM methods on image-classification models are supported.\"\n    )\n\n    # ask for target class for explanation\n    target_category_indexes = input(\n        \"Please input target category indexes for explanation (comma-separated, e.g., 111, 32, 44, ...): \"\n    )\n    if not target_category_indexes or not target_category_indexes.strip():\n        print(\n            \"No target category indexes provided. Defaulting to explaining the top confident category.\"\n        )\n        target_category_indexes = []\n    else:\n        target_category_indexes = [\n            int(i.strip()) for i in target_category_indexes.split(\",\")\n        ]\n\n    num_requests = int(input(\"Enter the number of requests to send: \"))\n\n    try:\n        for gradcam_method_name in XAI_GRADCAM_METHODS:\n\n            data = {\n                **data,\n                \"ue_id\": UE_ID,\n                \"gradcam_method_name\": gradcam_method_name,\n                \"target_category_indexes\": target_category_indexes,\n            }\n            print(\"Data: \", data)\n\n            profile_result_processor = ProfileResultProcessor(SERVER_URL)\n\n            for _ in range(num_requests):\n                response, process_time, node_id, k8s_pod_name = send_post_request(\n                    f\"{SERVER_URL}/xai_model/profile_run\", data, files\n                )\n                print(\"Process Time: \", process_time)\n                print(\"Node ID: \", node_id)\n                print(\"K8S_POD_NAME: \", k8s_pod_name)\n                if not response:\n                    print(\"No profile response received.\")\n                    continue\n\n                # Handle JSON response\n                model_results = response.get(\"model_results\")\n                if model_results:\n                    print(\"Model Results:\", json.dumps(model_results, indent=4))\n\n                profile_result_processor.process_new_response(\n                    response,\n                    process_time=process_time,\n                    node_id=node_id,\n                    k8s_pod_name=k8s_pod_name,\n                    gradcam_method_name=gradcam_method_name,\n                )\n\n            # Print the final profile result and update the service_data.json\n            profile_result_processor.complete_profile()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\n\nOPTIONS = [\n    {\n        \"label\": \"Get help information\",\n        \"action\": option_help,\n    },\n    {\n        \"label\": \"Run AI service\",\n        \"action\": option_run,\n    },\n    {\n        \"label\": \"Profile AI service\",\n        \"action\": option_profile_run,\n    },\n    {\n        \"label\": \"Run AI service with XAI (only image-classification models)\",\n        \"action\": option_run_with_xai,\n    },\n    {\n        \"label\": \"Profile AI service with XAI (only image-classification models)\",\n        \"action\": option_profile_run_with_xai,\n    },\n]\n\n\nif __name__ == \"__main__\":\n    while True:\n        print(\"\\nOptions:\")\n        for i, option in enumerate(OPTIONS, start=1):\n            print(f\"{i}. {option['label']}\")\n        print(\"q. Quit\")\n        choice = input(\"Enter your choice: \")\n\n        if choice == \"q\":\n            print(\"Exiting the client. Goodbye!\")\n            break\n        else:\n            try:\n                choice = int(choice)\n                if 1 <= choice <= len(OPTIONS):\n                    OPTIONS[choice - 1][\"action\"]()\n                else:\n                    print(\"Invalid choice. Please try again.\")\n            except ValueError:\n                print(\"Invalid input. Please enter a number.\")\n",
        "ai_client_utils_script_content": "PERMITTED_LANGUAGES = {\n    \"a\": \"American English\",\n    \"b\": \"British English\",\n    \"e\": \"Spanish\",\n    \"f\": \"French\",\n    \"h\": \"Hindi\",\n    \"i\": \"Italian\",\n    \"j\": \"Japanese\",\n    \"p\": \"Brazilian Portuguese\",\n    \"z\": \"Mandarin Chinese\",\n}\n\ndef get_permitted_voices(language_code: str) -> list[str]:\n    \"\"\"Get the list of permitted voices for a given language code.\"\"\"\n    voices = {\n        \"a\": [\n            \"af\",\n            \"af_alloy\",\n            \"af_aoede\",\n            \"af_bella\",\n            \"af_heart\",\n            \"af_jessica\",\n            \"af_kore\",\n            \"af_nicole\",\n            \"af_nova\",\n            \"af_river\",\n            \"af_sarah\",\n            \"af_sky\",\n            \"am_adam\",\n            \"am_echo\",\n            \"am_eric\",\n            \"am_fenrir\",\n            \"am_liam\",\n            \"am_michael\",\n            \"am_onyx\",\n            \"am_puck\",\n            \"am_santa\",\n        ],\n        \"b\": [\n            \"bf_alice\",\n            \"bf_emma\",\n            \"bf_isabella\",\n            \"bf_lily\",\n            \"bm_daniel\",\n            \"bm_fable\",\n            \"bm_george\",\n            \"bm_lewis\",\n        ],\n        \"e\": [\n            \"ef_dora\",\n            \"em_alex\",\n            \"em_santa\",\n        ],\n        \"f\": [\n            \"ff_siwis\",\n            \"hf_alpha\",\n            \"hf_beta\",\n        ],\n        \"h\": [\n            \"hm_omega\",\n            \"hm_psi\",\n        ],\n        \"i\": [\n            \"if_sara\",\n            \"im_nicola\",\n        ],\n        \"j\": [\n            \"jf_alpha\",\n            \"jf_gongitsune\",\n            \"jf_nezumi\",\n            \"jf_tebukuro\",\n            \"jm_kumo\",\n        ],\n        \"p\": [\n            \"pf_dora\",\n            \"pm_alex\",\n            \"pm_santa\",\n        ],\n        \"z\": [\n            \"zf_xiaobei\",\n            \"zf_xiaoni\",\n            \"zf_xiaoxiao\",\n            \"zf_xiaoyi\",\n            \"zm_yunjian\",\n            \"zm_yunxi\",\n            \"zm_yunxia\",\n            \"zm_yunyang\",\n        ],\n    }\n    return voices.get(language_code, [])\n\n\ndef prepare_ai_service_request_data():\n    \"\"\"Prepare the `data` part for the AI service request, including text and voice selection.\"\"\"\n    data = {}\n\n    # prompt the user to select a language\n    while True:\n        print(\"Available languages:\")\n        for code, name in PERMITTED_LANGUAGES.items():\n            print(f\"{code}: {name}\")\n        language_code = input(\"Please select a language code: (default to: american english: a) \").strip().lower()\n        if language_code in PERMITTED_LANGUAGES:\n            data[\"lang_code\"] = language_code\n            break\n        if not language_code:\n            data[\"lang_code\"] = \"a\"\n            break\n\n        print(f\"Invalid language code. Please select from {', '.join(PERMITTED_LANGUAGES.keys())}.\")\n    \n    # prompt the user to select a voice\n    voices = get_permitted_voices(data[\"lang_code\"])\n    if not voices:\n        print(f\"No voices available for language code '{data['lang_code']}'.\")\n        return None\n    \n    while True:\n        print(\"Available voices:\")\n        for voice in voices:\n            print(f\"- {voice}\")\n        voice = input(f\"Please select a voice: (default to {voices[0]})\").strip()\n        if voice in voices:\n            data[\"voice\"] = voice\n            break\n        if not voice:\n            data[\"voice\"] = voices[0]\n            break\n\n        print(f\"Invalid voice. Please select from {', '.join(voices)}.\")\n    \n    while True:\n        # Prompt the user to input the text they want to convert to speech\n        data[\"text\"] = input(\"Please input the text to convert to speech: \")\n\n        if data[\"text\"].strip():\n            break\n\n        print(\"Text cannot be empty. Please try again.\")\n    \n    return data\n\ndef prepare_ai_service_request_files():\n    \"\"\"Prepare the `files` part for the AI service request (for FastAPI server).\"\"\"\n    files = {}\n    # Ask the user for any other additional files if needed in future\n    return files",
        "model_script_content": "# import server utils\nfrom ai_server_utils import (\n    profile_activities,\n    prepare_profile_results,\n)\n# import profile utils\nfrom torch.profiler import profile, record_function\n\n# import necessary libs for AI model inference and request handling\nimport torch\nfrom fastapi import APIRouter, Form\nfrom fastapi.responses import JSONResponse\nfrom kokoro import KPipeline\nimport base64\nimport io\n\n# --------------------------------\n# Device configuration\n# --------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# --------------------------------\n# Model-specific configuration\n# make sure the variables `MODEL_NAME` and `pipeline` are defined here.\n# --------------------------------\nMODEL_NAME = \"hexgrad/Kokoro-82M\"\n\n\n# Initialize the FastAPI router\nrouter = APIRouter()\n\n@router.post(\"/run\")\nasync def run_model(lang_code: str = Form(...), voice: str = Form(...), speed: float = Form(...), text: str = Form(...), ue_id: str = Form(...)):\n    try:\n\n        pipeline = KPipeline(lang_code=lang_code, device=device)\n\n        # Prepare the model input\n        generator = pipeline(text, voice=voice, speed=1.0)\n\n        # Perform inference and encode audio to base64\n        audio_data = []\n        for _, _, audio in generator:\n            buffer = io.BytesIO()\n            torch.save(audio, buffer)\n            audio_data.append(base64.b64encode(buffer.getvalue()).decode('utf-8'))\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"model_results\": audio_data,\n            }\n        )\n    except Exception as e:\n        print(f\"Error processing text: {e}\")\n        return JSONResponse(\n            content={\"error\": \"Failed to process the text. {e}\".format(e=str(e))},\n            status_code=500,\n        )\n\n@router.post(\"/profile_run\")\nasync def profile_run(lang_code: str = Form(...), voice: str = Form(...), speed: float = Form(...), text: str = Form(...), ue_id: str = Form(...)):\n    \"\"\"\n    Endpoint to profile the AI model execution.\n    \"\"\"\n    try:\n        # perform profiling\n        with profile(\n            activities=profile_activities,\n            profile_memory=True,\n        ) as prof:\n            with record_function(\"model_run\"):\n\n                pipeline = KPipeline(lang_code=lang_code, device=device)\n\n                # Prepare the model input\n                generator = pipeline(text, voice=voice, speed=1.0)\n\n                audio_data = []\n                for _, _, audio in generator:\n                    buffer = io.BytesIO()\n                    torch.save(audio, buffer)\n                    audio_data.append(base64.b64encode(buffer.getvalue()).decode('utf-8'))\n\n        profile_result = prepare_profile_results(prof)\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"profile_result\": profile_result,\n                \"model_results\": audio_data,\n            }\n        )\n\n    except Exception as e:\n        print(f\"Error processing request: {e}\")\n        return JSONResponse(\n            content={\"error\": f\"Failed to process the request. {e}\"},\n            status_code=500,\n        )\n\n# Below are the model input and output specifications to be used by the `/help` endpoint\nMODEL_INPUT_FORM_SPEC = {\n    \"text\": {\n        \"type\": \"string\",\n        \"description\": \"The text input to be converted to speech.\",\n        \"required\": True,\n        \"example\": \"Hello, world!\",\n    },\n    \"lang_code\": {\n        \"type\": \"string\",\n        \"description\": \"The language code for the text input.\",\n        \"required\": True,\n        \"example\": \"en\",\n    },\n    \"voice\": {\n        \"type\": \"string\",\n        \"description\": \"The voice to be used for speech synthesis.\",\n        \"required\": True,\n        \"example\": \"en-US-Wavenet-D\",\n    },\n    \"speed\": {\n        \"type\": \"float\",\n        \"description\": \"The speed of the speech synthesis. Normal speed is 1.0.\",\n        \"required\": True,\n        \"example\": 1.0,\n    },\n}\n\nMODEL_OUTPUT_JSON_SPEC = {\n    \"ue_id\": \"unique execution ID\",\n    \"model_results\": [\n        {\n            \"audio_data\": \"base64 encoded audio data\",\n        }\n    ],\n}",
        "healthcheck_script_content": "import requests\n\nprint(requests.get(\"http://localhost:8000/help\"))"
    }
}