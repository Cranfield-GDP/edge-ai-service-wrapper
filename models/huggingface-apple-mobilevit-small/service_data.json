{
    "model_name": "apple/mobilevit-small",
    "model_url": "https://huggingface.co/apple/mobilevit-small",
    "task": "image-classification",
    "task_detail": "The MobileViT-small model is a pre-trained convolutional neural network designed for image classification tasks. Developed as part of a family of models called MobileViT, this model is particularly noted for its lightweight and mobile-friendly architecture, making it ideal for deployment in resource-constrained environments.\n\n**Functionality and Application:**\n\n1. **Image Classification:** The primary functionality of the MobileViT-small model is image classification. It effectively categorizes images into one of 1,000 predefined classes, as specified by the ImageNet-1k dataset. These classes range from common everyday objects, such as teapots and houses, to various species of animals, including tigers and birds.\n\n2. **Input Format:** The model requires input images to be preprocessed before classification. Images should be resized to 288x288 pixels, and center-cropped to 256x256 pixels to ensure consistency in input dimensions. Additionally, the pixels are normalized to a range between 0 and 1, and the images must be in BGR format, as opposed to the more common RGB format.\n\n3. **Output Format:** When an image is passed through the MobileViT-small model, the output is a set of logits. These logits represent the model's confidence scores across all 1,000 ImageNet classes. The class with the highest score is identified as the predicted class. The output includes not only the class label but also an index and the associated probability score, indicating the confidence level of the prediction.\n\n4. **Architecture:** The MobileViT-small model uniquely integrates the lightweight, efficient operations of MobileNetV2 with the expressive power of Vision Transformers (ViT). It features MobileViT blocks that replace localized convolutional processing with global context using transformers, enhancing its ability to learn complex patterns. Importantly, this architecture eliminates the need for positional embeddings, typically required in vision transformers, thereby simplifying the model and improving computational efficiency.\n\n5. **Training and Evaluation:** The MobileViT-small model was pre-trained on the ImageNet-1k dataset, which comprises approximately 1 million images. During training, data augmentations such as random resized cropping and horizontal flipping were employed. For evaluation, the model achieves an ImageNet top-1 accuracy of 78.4% and a top-5 accuracy of 94.1%, indicative of its strong performance in accurately classifying objects across a vast range of categories.\n\n6. **Use Cases:** Due to its compact size and remarkable accuracy, the MobileViT-small model is suitable for various applications in domains that require image recognition capabilities on mobile devices or other systems with limited computational resources. It can be utilized in applications such as mobile photography enhancement, real-time object recognition and tracking in robotics, and augmented reality experiences where efficient and reliable image classification is crucial.\n\nOverall, the MobileViT-small model represents a significant advancement in the realm of lightweight convolutional neural networks, seamlessly blending the advantages of modern transformer architectures with traditional CNN operations to deliver high-performance image classification in a resource-efficient manner.",
    "accuracy_info": "The MobileViT-small model achieves a top-1 accuracy of 78.4% and a top-5 accuracy of 94.1% on the ImageNet-1k dataset.",
    "image_repository_url": "docker.io/cranfield6g/cranfield-edge-apple-mobilevit-small",
    "service_disk_size_bytes": 0,
    "profiles": [
        {
            "node_id": "LAP004262",
            "device_type": "DeviceType.CPU",
            "device_name": "None",
            "initialization_time_ms": 7460.091114044189,
            "eviction_time_ms": 0,
            "initialization_cost": 0,
            "keep_alive_cost": 0,
            "energy_consumption_idle": 0,
            "inference": {
                "cpu_time_ms": 70.00529,
                "device_time_ms": 0.0,
                "cpu_memory_usage_MB": 0.003814697265625,
                "self_cpu_memory_usage_MB": -208.92529296875,
                "device_memory_usage_MB": 0.0,
                "self_device_memory_usage_MB": 0.0,
                "energy_consumption_execution": 0,
                "disk_IO_MB": 0,
                "input_data_MB": 0,
                "output_data_MB": 0,
                "execution_time_ms": 196.0726579030355,
                "execution_cost": 0
            },
            "xai": [
                {
                    "xai_method": "GradCAM",
                    "cpu_time_ms": 179.29429199999998,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 180.4101333618164,
                    "self_cpu_memory_usage_MB": -56.800262451171875,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 768.7631448109945,
                    "execution_cost": 0
                },
                {
                    "xai_method": "HiResCAM",
                    "cpu_time_ms": 191.120041,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 180.4101333618164,
                    "self_cpu_memory_usage_MB": -56.917449951171875,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 763.7475331624349,
                    "execution_cost": 0
                },
                {
                    "xai_method": "XGradCAM",
                    "cpu_time_ms": 203.12948500000002,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 180.4101333618164,
                    "self_cpu_memory_usage_MB": -57.362762451171875,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 824.3966897328695,
                    "execution_cost": 0
                },
                {
                    "xai_method": "GradCAMPlusPlus",
                    "cpu_time_ms": 212.08675200000002,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 180.4101333618164,
                    "self_cpu_memory_usage_MB": -56.858856201171875,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 734.1773509979248,
                    "execution_cost": 0
                },
                {
                    "xai_method": "LayerCAM",
                    "cpu_time_ms": 187.68081400000003,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 180.4101333618164,
                    "self_cpu_memory_usage_MB": -56.976043701171875,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 765.8891677856445,
                    "execution_cost": 0
                },
                {
                    "xai_method": "EigenCAM",
                    "cpu_time_ms": 106.584043,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 180.3710708618164,
                    "self_cpu_memory_usage_MB": -35.51946258544922,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 392.6692803700765,
                    "execution_cost": 0
                },
                {
                    "xai_method": "EigenGradCAM",
                    "cpu_time_ms": 199.042449,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 180.4101333618164,
                    "self_cpu_memory_usage_MB": -56.858856201171875,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 800.7550239562988,
                    "execution_cost": 0
                },
                {
                    "xai_method": "KPCA_CAM",
                    "cpu_time_ms": 82.091222,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 180.3710708618164,
                    "self_cpu_memory_usage_MB": -35.51946258544922,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 228.60844930013022,
                    "execution_cost": 0
                },
                {
                    "xai_method": "RandomCAM",
                    "cpu_time_ms": 197.330633,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 180.4101333618164,
                    "self_cpu_memory_usage_MB": -56.800262451171875,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 814.6819273630778,
                    "execution_cost": 0
                }
            ],
            "idle_container_cpu_memory_usage": "1GB",
            "idle_container_device_memory_usage": "0GB"
        }
    ],
    "feedback": {
        "likes": [],
        "dislikes": [],
        "comments": []
    },
    "code": {
        "readme_content": "---\nlicense: other\ntags:\n- vision\n- image-classification\ndatasets:\n- imagenet-1k\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\n  example_title: Tiger\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg\n  example_title: Teapot\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg\n  example_title: Palace\n---\n\n# MobileViT (small-sized model)\n\nMobileViT model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178) by Sachin Mehta and Mohammad Rastegari, and first released in [this repository](https://github.com/apple/ml-cvnets). The license used is [Apple sample code license](https://github.com/apple/ml-cvnets/blob/main/LICENSE).\n\nDisclaimer: The team releasing MobileViT did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nMobileViT is a light-weight, low latency convolutional neural network that combines MobileNetV2-style layers with a new block that replaces local processing in convolutions with global processing using transformers. As with ViT (Vision Transformer), the image data is converted into flattened patches before it is processed by the transformer layers. Afterwards, the patches are \"unflattened\" back into feature maps. This allows the MobileViT-block to be placed anywhere inside a CNN. MobileViT does not require any positional embeddings.\n\n## Intended uses & limitations\n\nYou can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=mobilevit) to look for fine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\n\n```python\nfrom transformers import MobileViTFeatureExtractor, MobileViTForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nfeature_extractor = MobileViTFeatureExtractor.from_pretrained(\"apple/mobilevit-small\")\nmodel = MobileViTForImageClassification.from_pretrained(\"apple/mobilevit-small\")\n\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\n\noutputs = model(**inputs)\nlogits = outputs.logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n```\n\nCurrently, both the feature extractor and model support PyTorch.\n\n## Training data\n\nThe MobileViT model was pretrained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k), a dataset consisting of 1 million images and 1,000 classes. \n\n## Training procedure\n\n### Preprocessing\n\nTraining requires only basic data augmentation, i.e. random resized cropping and horizontal flipping. \n\nTo learn multi-scale representations without requiring fine-tuning, a multi-scale sampler was used during training, with image sizes randomly sampled from: (160, 160), (192, 192), (256, 256), (288, 288), (320, 320).\n\nAt inference time, images are resized/rescaled to the same resolution (288x288), and center-cropped at 256x256.\n\nPixels are normalized to the range [0, 1]. Images are expected to be in BGR pixel order, not RGB.\n\n### Pretraining\n\nThe MobileViT networks are trained from scratch for 300 epochs on ImageNet-1k on 8 NVIDIA GPUs with an effective batch size of 1024 and learning rate warmup for 3k steps, followed by cosine annealing. Also used were label smoothing cross-entropy loss and L2 weight decay. Training resolution varies from 160x160 to 320x320, using multi-scale sampling.\n\n## Evaluation results\n\n| Model            | ImageNet top-1 accuracy | ImageNet top-5 accuracy | # params  | URL                                             |\n|------------------|-------------------------|-------------------------|-----------|-------------------------------------------------|\n| MobileViT-XXS    | 69.0                    | 88.9                    | 1.3 M     | https://huggingface.co/apple/mobilevit-xx-small |\n| MobileViT-XS     | 74.8                    | 92.3                    | 2.3 M     | https://huggingface.co/apple/mobilevit-x-small  |\n| **MobileViT-S**  | **78.4**                | **94.1**                | **5.6 M** | https://huggingface.co/apple/mobilevit-small    |\n\n### BibTeX entry and citation info\n\n```bibtex\n@inproceedings{vision-transformer,\ntitle = {MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer},\nauthor = {Sachin Mehta and Mohammad Rastegari},\nyear = {2022},\nURL = {https://arxiv.org/abs/2110.02178}\n}\n```\n",
        "dockerfile_content": "# Base image for AI service powered by HuggingFace pre-trained AI models.\n# the image has the following packages/libraries installed already:\n# - python3.12, pip, git, fastapi, uvicorn, torch, torchvision, opencv-python, transformers, python-multipart, Pillow, requests\nFROM python3.12_ai_service_base:latest\n\n# Set working directory\nWORKDIR /app\n\n# Copy application code\nCOPY . .\n\n# Install additional dependencies\nRUN pip install --no-cache-dir transformers\n\n# Expose port 8000\nEXPOSE 8000\n\n# Start the FastAPI server\nCMD [\"uvicorn\", \"ai_server:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--timeout-keep-alive\", \"600\"]",
        "ai_server_script_content": "import json\nimport os\nimport time\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import JSONResponse\nfrom contextlib import asynccontextmanager\nfrom ai_server_utils import (\n    PROFILE_OUTPUT_JSON_SPEC,\n    NODE_ID,\n    K8S_POD_NAME,\n)\n\n\n# -------------------------------------------\n# App Lifespan setup\n# -------------------------------------------\n# Record the script start time (when uvicorn starts the process)\nSCRIPT_START_TIME = time.time()\nINITIALIZATION_DURATION = 0.0\nservice_endpoint_specs = {\n    \"model_input_form_spec\": None,\n    \"model_output_json_spec\": None,\n    \"profile_output_json_spec\": None,\n    \"xai_model_input_form_spec\": None,\n    \"xai_model_output_json_spec\": None,\n    \"xai_profile_output_json_spec\": None,\n}\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n\n    global INITIALIZATION_DURATION\n    global SCRIPT_START_TIME\n    global service_endpoint_specs\n\n    # Load the AI model\n    print(\"Loading AI model...\")\n    from model import (\n        MODEL_INPUT_FORM_SPEC,\n        MODEL_OUTPUT_JSON_SPEC,\n        router as model_router,\n    )\n\n    service_endpoint_specs[\"model_input_form_spec\"] = MODEL_INPUT_FORM_SPEC\n    service_endpoint_specs[\"model_output_json_spec\"] = MODEL_OUTPUT_JSON_SPEC\n    service_endpoint_specs[\"profile_output_json_spec\"] = PROFILE_OUTPUT_JSON_SPEC\n\n    app.include_router(model_router, prefix=\"/model\", tags=[\"AI Model\"])\n\n    # Load the XAI model\n    if os.path.exists(os.path.dirname(__file__) + \"/xai_model.py\"):\n        print(\"Loading XAI model...\")\n        from xai_model import (\n            XAI_OUTPUT_JSON_SPEC,\n            router as xai_model_router,\n        )\n\n        # by default, the xai_model input form spec is the same as the model input form spec\n        service_endpoint_specs[\"xai_model_input_form_spec\"] = MODEL_INPUT_FORM_SPEC\n        service_endpoint_specs[\"xai_model_output_json_spec\"] = MODEL_OUTPUT_JSON_SPEC\n        service_endpoint_specs[\"xai_model_output_json_spec\"].update(\n            XAI_OUTPUT_JSON_SPEC\n        )\n        service_endpoint_specs[\"xai_profile_output_json_spec\"] = (\n            PROFILE_OUTPUT_JSON_SPEC\n        )\n        service_endpoint_specs[\"xai_profile_output_json_spec\"].update(\n            XAI_OUTPUT_JSON_SPEC\n        )\n\n        app.include_router(xai_model_router, prefix=\"/xai_model\", tags=[\"XAI Model\"])\n\n    # Record the initialization duration\n    INITIALIZATION_DURATION = time.time() - SCRIPT_START_TIME\n\n    print(f\"AI service loaded in {INITIALIZATION_DURATION:.2f} seconds.\")\n\n    yield\n\n    # Clean up the models and release the resources\n    service_endpoint_specs.clear()\n\n\n# -------------------------------------------\n# FastAPI application setup\n# -------------------------------------------\napp = FastAPI(lifespan=lifespan)\n\n\n# -------------------------------------------\n# Middlewares\n# -------------------------------------------\n@app.middleware(\"http\")\nasync def prepare_header_middleware(request: Request, call_next):\n    start_time = time.perf_counter()\n    response = await call_next(request)\n    process_time = time.perf_counter() - start_time\n    response.headers[\"X-Process-Time\"] = str(process_time)\n    response.headers[\"X-NODE-ID\"] = NODE_ID\n    response.headers[\"X-K8S-POD-NAME\"] = K8S_POD_NAME\n    return response\n\n\n# -------------------------------------------\n# General Endpoints\n# -------------------------------------------\n@app.get(\"/initialization_duration\")\ndef get_initialization_duration():\n    \"\"\"\n    Endpoint to retrieve the initialization duration of the AI model.\n    \"\"\"\n    global INITIALIZATION_DURATION\n\n    if INITIALIZATION_DURATION == 0.0:\n        return JSONResponse(\n            content={\"error\": \"Model not initialized.\"},\n            status_code=500,\n        )\n    return JSONResponse(\n        content={\n            \"initialization_duration\": INITIALIZATION_DURATION,\n            \"script_start_time\": SCRIPT_START_TIME,\n        }\n    )\n\n\n@app.get(\"/help\")\ndef get_help():\n    global service_endpoint_specs\n    help_info = {\n        \"endpoints\": {\n            \"/model/run\": {\n                \"method\": \"POST\",\n                \"description\": \"Executes the AI model with the provided input data.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_output_json_spec\"],\n                },\n            },\n            \"/model/profile_run\": {\n                \"method\": \"POST\",\n                \"description\": \"Profiles the AI model execution.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    \"profile_result\": \"Profiling results of the AI model execution.\",\n                    **service_endpoint_specs[\"profile_output_json_spec\"],\n                },\n            },\n            \"/initialization_duration\": {\n                \"method\": \"GET\",\n                \"description\": \"Retrieves the initialization duration of the AI model.\",\n                \"response\": {\n                    \"initialization_duration\": \"Time taken to initialize the model (in seconds).\",\n                    \"script_start_time\": \"Timestamp when the script started (in seconds since epoch).\",\n                },\n            },\n        }\n    }\n\n    if service_endpoint_specs[\"xai_model_input_form_spec\"] is not None:\n        help_info[\"endpoints\"][\"/xai_model/run\"] = {\n            \"method\": \"POST\",\n            \"description\": \"Executes the XAI model with the provided input data.\",\n            \"parameters\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_input_form_spec\"],\n            },\n            \"response\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_output_json_spec\"],\n            },\n        }\n\n        help_info[\"endpoints\"][\"/xai_model/profile_run\"] = {\n            \"method\": \"POST\",\n            \"description\": \"Profiles the XAI model execution.\",\n            \"parameters\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_input_form_spec\"],\n            },\n            \"response\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                \"profile_result\": \"Profiling results of the XAI model execution.\",\n                **service_endpoint_specs[\"xai_profile_output_json_spec\"],\n            },\n        }\n    \n    return JSONResponse(content=help_info)\n",
        "ai_server_utils_script_content": "import os\nimport socket\nimport torch\nfrom io import BytesIO\nimport base64\n\nfrom torch.profiler import profile, ProfilerActivity, record_function\n\n\n# -------------------------------------------\n# ENV Variables\n# -------------------------------------------\nNODE_ID = os.getenv(\"NODE_ID\", socket.gethostname())\nK8S_POD_NAME = os.getenv(\"K8S_POD_NAME\", \"UNKNOWN\")\n\n\n# -------------------------------------------\n# Profile Utils\n# -------------------------------------------\nprofile_activities = [\n    ProfilerActivity.CPU,\n    ProfilerActivity.CUDA,\n    ProfilerActivity.MTIA,\n    ProfilerActivity.XPU,\n]\n\ndef process_model_output_logits(model, model_output_logits):\n    \"\"\"\n    Process the model outputs to prepare for the response.\n    \"\"\"\n    probabilities = torch.nn.functional.softmax(model_output_logits[0], dim=0)\n\n    # Return the top 5 predictions with labels\n    top5_prob, top5_catid = torch.topk(probabilities, 5)\n    predictions = []\n    for i in range(top5_prob.size(0)):\n        category_id = top5_catid[i].item()\n        predictions.append(\n            {\n                \"category_id\": category_id,\n                \"label\": model.config.id2label[category_id],\n                \"probability\": top5_prob[i].item(),\n            }\n        )\n    return predictions\n\ndef prepare_profile_results(prof):\n    \"\"\"\n    Prepare the profile results for the model inputs and outputs.\n    \"\"\"\n    print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n\n    profile_event = prof.key_averages()[0]\n\n    profile_result = {\n        \"name\": profile_event.key,\n        \"device_type\": str(profile_event.device_type),\n        \"device_name\": str(profile_event.use_device),\n        \"cpu_memory_usage\": profile_event.cpu_memory_usage,\n        \"self_cpu_memory_usage\": profile_event.self_cpu_memory_usage,\n        \"device_memory_usage\": profile_event.device_memory_usage,\n        \"self_device_memory_usage\": profile_event.self_device_memory_usage,\n        \"cpu_time_total\": profile_event.cpu_time_total,\n        \"self_cpu_time_total\": profile_event.self_cpu_time_total,\n        \"device_time_total\": profile_event.device_time_total,\n        \"self_device_time_total\": profile_event.self_device_time_total,\n    }\n    return profile_result\n\n\ndef encode_image(image):\n    \"\"\"\n    Encode the image to bytes\n    \"\"\"\n    buffered = BytesIO()\n    image.save(buffered, format=\"PNG\")\n    encoded_image = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n    return encoded_image\n\n\nPROFILE_OUTPUT_JSON_SPEC = {\n    \"ue_id\": \"unique execution ID\",\n    \"profile_result\": {\n        \"name\": \"name of the profile event\",\n        \"device_type\": \"type of device used (e.g., CPU, GPU, ...)\",\n        \"device_name\": \"name of the device used\",\n        \"cpu_memory_usage\": \"CPU memory usage in bytes\",\n        \"self_cpu_memory_usage\": \"self CPU memory usage in bytes\",\n        \"device_memory_usage\": \"device memory usage in bytes\",\n        \"self_device_memory_usage\": \"self device memory usage in bytes\",\n        \"cpu_time_total\": \"total CPU time in microseconds\",\n        \"self_cpu_time_total\": \"self total CPU time in microseconds\",\n        \"device_time_total\": \"total device time in microseconds\",\n        \"self_device_time_total\": \"self total device time in microseconds\",\n    },\n    \"model_results\": \"the AI service model results\",\n}\n",
        "ai_client_script_content": "import base64\nimport json\nimport time\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nfrom ai_client_utils import (\n    prepare_ai_service_request_files,\n    prepare_ai_service_request_data,\n)\n\nXAI_GRADCAM_METHODS = [\n    \"GradCAM\",\n    \"HiResCAM\",\n    # \"AblationCAM\",\n    \"XGradCAM\",\n    \"GradCAMPlusPlus\",\n    # \"ScoreCAM\",\n    \"LayerCAM\",\n    \"EigenCAM\",\n    \"EigenGradCAM\",\n    \"KPCA_CAM\",\n    \"RandomCAM\",\n]\n\n# -------------------------------------\n# prompt for necessary inputs\n# -------------------------------------\nSERVER_URL = input(\"Please input server URL (default to http://localhost:9000): \")\nif SERVER_URL.strip() == \"\":\n    SERVER_URL = \"http://localhost:9000\"\nUE_ID = input(\"Please input UE_ID (default to 123456): \")\nif UE_ID.strip() == \"\":\n    UE_ID = \"123456\"\n\n\ndef send_post_request(url, data, files):\n    \"\"\"Send request to run AI service and display AI service responses.\"\"\"\n    try:\n        response = requests.post(url, files=files, data=data)\n        # get the process time, node id and k8s pod name from the response headers\n        process_time = response.headers.get(\"X-Process-Time\")\n        node_id = response.headers.get(\"X-NODE-ID\")\n        k8s_pod_name = response.headers.get(\"X-K8S-POD-NAME\")\n        if response.status_code == 200:\n            return response.json(), process_time, node_id, k8s_pod_name\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n\ndef send_get_request(url, params=None):\n    \"\"\"Send GET request to the specified URL and return the response.\"\"\"\n    try:\n        response = requests.get(url, params=params)\n        process_time = response.headers.get(\"X-Process-Time\")\n        node_id = response.headers.get(\"X-NODE-ID\")\n        k8s_pod_name = response.headers.get(\"X-K8S-POD-NAME\")\n        if response.status_code == 200:\n            return response.json(), process_time, node_id, k8s_pod_name\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n            return None\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n\nclass ProfileResultProcessor:\n\n    def __init__(self, server_url):\n        self.server_url = server_url\n        self.start_time = time.time()\n        self.service_initialization_duration = 0\n        self.response_counter = 0\n        self.profile_name = None\n        self.device_type = None\n        self.device_name = None\n        self.node_id = None\n        self.k8s_pod_name = None\n        self.cpu_memory_usage_bytes = 0\n        self.self_cpu_memory_usage_bytes = 0\n        self.device_memory_usage_bytes = 0\n        self.self_device_memory_usage_bytes = 0\n        self.cpu_time_total_us = 0\n        self.self_cpu_time_total_us = 0\n        self.device_time_total_us = 0\n        self.self_device_time_total_us = 0\n\n        # xai related\n        self.gradcam_method_name = None\n\n        self.fetch_service_initialization_duration()\n\n    def fetch_service_initialization_duration(self):\n        \"\"\"Fetch the service initialization duration from the server.\"\"\"\n        response, process_time, node_id, k8s_pod_name = send_get_request(\n            f\"{self.server_url}/initialization_duration\"\n        )\n        if response:\n            self.service_initialization_duration = response.get(\n                \"initialization_duration\", 0\n            )\n        else:\n            print(\"Failed to fetch initialization duration.\")\n            self.service_initialization_duration = 0\n\n    def process_new_response(\n        self,\n        profile_response,\n        process_time=None,\n        node_id=None,\n        k8s_pod_name=None,\n        gradcam_method_name=None,\n    ):\n        if not profile_response:\n            return\n\n        profile_result = profile_response[\"profile_result\"]\n\n        if not profile_result:\n            return\n\n        if self.profile_name is None:\n            self.profile_name = profile_result[\"name\"]\n        if self.device_type is None:\n            self.device_type = profile_result[\"device_type\"]\n        if self.device_name is None:\n            self.device_name = profile_result[\"device_name\"]\n        if self.node_id is None:\n            self.node_id = node_id\n        if self.k8s_pod_name is None:\n            self.k8s_pod_name = k8s_pod_name\n\n        if self.gradcam_method_name is None:\n            self.gradcam_method_name = gradcam_method_name\n\n        # update the max profile result\n        self.cpu_memory_usage_bytes = max(\n            self.cpu_memory_usage_bytes, profile_result[\"cpu_memory_usage\"]\n        )\n        # self cpu memory usage could be negative. here we take the value that has the max absolute value\n        if abs(profile_result[\"self_cpu_memory_usage\"]) > abs( self.self_cpu_memory_usage_bytes):\n            self.self_cpu_memory_usage_bytes = profile_result[\"self_cpu_memory_usage\"]\n        self.device_memory_usage_bytes = max(\n            self.device_memory_usage_bytes, profile_result[\"device_memory_usage\"]\n        )\n        # same as self cpu memory usage\n        if abs(profile_result[\"self_device_memory_usage\"]) > abs(self.self_device_memory_usage_bytes):\n            self.self_device_memory_usage_bytes = profile_result[\"self_device_memory_usage\"]\n        self.cpu_time_total_us = max(\n            self.cpu_time_total_us, profile_result[\"cpu_time_total\"]\n        )\n        self.self_cpu_time_total_us = max(\n            self.self_cpu_time_total_us, profile_result[\"self_cpu_time_total\"]\n        )\n        self.device_time_total_us = max(\n            self.device_time_total_us, profile_result[\"device_time_total\"]\n        )\n        self.self_device_time_total_us = max(\n            self.self_device_time_total_us, profile_result[\"self_device_time_total\"]\n        )\n\n        self.response_counter += 1\n\n    def complete_profile(self):\n        print(\"\\n--------- PROFILE EVENT ---------\\n\")\n        print(f\"Name: {self.profile_name}\")\n        print(f\"Device Type: {self.device_type}\")\n        print(f\"Device Name: {self.device_name}\")\n        print(f\"Node ID: {self.node_id}\")\n        print(f\"K8S_POD_NAME: {self.k8s_pod_name}\")\n\n        if self.gradcam_method_name:\n            print(f\"GradCAM Method Name: {self.gradcam_method_name}\")\n\n        print(\"\\n--------- LATENCY RESULT ---------\\n\")\n        print(\"Service Initialization Duration: \", self.service_initialization_duration)\n        print(\"Total Requests: \", self.response_counter)\n        print(f\"Total Time Taken: {time.time() - self.start_time:.2f} seconds\")\n        print(\n            f\"Average Time Taken: {(time.time() - self.start_time) / self.response_counter:.2f} seconds\"\n        )\n\n        print(\"\\n--------- RESOURCE USAGE ---------\\n\")\n        print(f\"CPU Memory Usage: {self.cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\")\n        print(\n            f\"Self CPU Memory Usage: {self.self_cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Device Memory Usage: {self.device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Self Device Memory Usage: {self.self_device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(f\"CPU Time Total: {self.cpu_time_total_us / 1000:.2f} ms\")\n        print(f\"Self CPU Time Total: {self.self_cpu_time_total_us / 1000:.2f} ms\")\n        print(f\"Device Time Total: {self.device_time_total_us / 1000:.2f} ms\")\n        print(f\"Self Device Time Total: {self.self_device_time_total_us / 1000:.2f} ms\")\n\n        # update the service_data.json automatically\n        with open(\"service_data.json\", \"r\") as f:\n            service_data = json.load(f)\n\n        if not self.gradcam_method_name:\n            complete_profile_data_to_save = {\n                \"node_id\": self.node_id,\n                \"device_type\": self.device_type,\n                \"device_name\": self.device_name,\n                \"initialization_time_ms\": self.service_initialization_duration * 1000,\n                \"eviction_time_ms\": 0,\n                \"initialization_cost\": 0,\n                \"keep_alive_cost\": 0,\n                \"energy_consumption_idle\": 0,\n                \"inference\": {\n                    \"cpu_time_ms\": self.cpu_time_total_us / 1000,\n                    \"device_time_ms\": self.device_time_total_us / 1000,\n                    \"cpu_memory_usage_MB\": self.cpu_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"self_cpu_memory_usage_MB\": self.self_cpu_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"device_memory_usage_MB\": self.device_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"self_device_memory_usage_MB\": self.self_device_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"energy_consumption_execution\": 0,\n                    \"disk_IO_MB\": 0,\n                    \"input_data_MB\": 0,\n                    \"output_data_MB\": 0,\n                    \"execution_time_ms\": (time.time() - self.start_time)\n                    / self.response_counter\n                    * 1000,\n                    \"execution_cost\": 0,\n                },\n            }\n\n            # check if there is already a profile for this node id\n            profile_found = False\n            for profile in service_data[\"profiles\"]:\n                if profile[\"node_id\"] == self.node_id:\n                    profile[\"inference\"] = complete_profile_data_to_save[\"inference\"]\n                    profile_found = True\n                    break\n            if not profile_found:\n                service_data[\"profiles\"].append(complete_profile_data_to_save)\n\n            # save the updated service_data.json\n            with open(\"service_data.json\", \"w\") as f:\n                json.dump(service_data, f, indent=4)\n            print(\"\\n--------- SERVICE DATA UPDATED ---------\\n\")\n\n        else:\n            complete_xai_profile_data_to_save = {\n                \"node_id\": self.node_id,\n                \"device_type\": self.device_type,\n                \"device_name\": self.device_name,\n                \"initialization_time_ms\": self.service_initialization_duration * 1000,\n                \"eviction_time_ms\": 0,\n                \"initialization_cost\": 0,\n                \"keep_alive_cost\": 0,\n                \"energy_consumption_idle\": 0,\n                \"xai\": [\n                    {\n                        \"xai_method\": self.gradcam_method_name,\n                        \"cpu_time_ms\": self.cpu_time_total_us / 1000,\n                        \"device_time_ms\": self.device_time_total_us / 1000,\n                        \"cpu_memory_usage_MB\": self.cpu_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"self_cpu_memory_usage_MB\": self.self_cpu_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"device_memory_usage_MB\": self.device_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"self_device_memory_usage_MB\": self.self_device_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"energy_consumption_execution\": 0,\n                        \"disk_IO_MB\": 0,\n                        \"input_data_MB\": 0,\n                        \"output_data_MB\": 0,\n                        \"execution_time_ms\": (time.time() - self.start_time)\n                        / self.response_counter\n                        * 1000,\n                        \"execution_cost\": 0,\n                    }\n                ],\n            }\n\n            # check if there is already a profile for this node id\n            profile_found = False\n            for profile in service_data[\"profiles\"]:\n                if profile[\"node_id\"] == self.node_id:\n                    profile_found = True\n\n                    # check if there is already a profile for this xai method\n                    xai_method_found = False\n                    if not profile.get(\"xai\"):\n                        profile[\"xai\"] = []\n                    for xai_profile in profile[\"xai\"]:\n                        if xai_profile[\"xai_method\"] == self.gradcam_method_name:\n                            xai_profile.update(complete_xai_profile_data_to_save[\"xai\"][0]) \n                            xai_method_found = True\n                            break\n                    if not xai_method_found:\n                        profile[\"xai\"].append(complete_xai_profile_data_to_save[\"xai\"][0])\n                    break\n\n            if not profile_found:\n                service_data[\"profiles\"].append(complete_xai_profile_data_to_save)\n\n            # save the updated service_data.json\n            with open(\"service_data.json\", \"w\") as f:\n                json.dump(service_data, f, indent=4)\n            print(\"\\n--------- SERVICE DATA UPDATED ---------\\n\")\n\n\ndef option_run():\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n    data = {**data, \"ue_id\": UE_ID}\n    response, process_time, node_id, pod_name = send_post_request(\n        f\"{SERVER_URL}/model/run\", data, files\n    )\n    print(\"Process Time: \", process_time)\n    print(\"Node ID: \", node_id)\n    print(\"K8S_POD_NAME: \", pod_name)\n    print(\"Response\")\n    print(json.dumps(response, indent=4))\n\n\ndef option_profile_run():\n    data = prepare_ai_service_request_data()\n    data = {**data, \"ue_id\": UE_ID}\n    files = prepare_ai_service_request_files()\n    num_requests = int(input(\"Enter the number of requests to send: \"))\n\n    try:\n        profile_result_processor = ProfileResultProcessor(SERVER_URL)\n\n        for _ in range(num_requests):\n            profile_response, process_time, node_id, k8s_node_name = send_post_request(\n                f\"{SERVER_URL}/model/profile_run\", data, files\n            )\n            print(\"Process Time: \", process_time)\n            print(\"Node ID: \", node_id)\n            print(\"K8S_POD_NAME: \", k8s_node_name)\n            print(\"Response\")\n            print(json.dumps(profile_response, indent=4))\n            if not profile_response:\n                print(\"No profile response received.\")\n                continue\n\n            profile_result_processor.process_new_response(\n                profile_response,\n                process_time=process_time,\n                node_id=node_id,\n                k8s_pod_name=k8s_node_name,\n            )\n\n        # Print the final profile result and update the service_data.json\n        profile_result_processor.complete_profile()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\n\ndef option_help():\n    \"\"\"Get help information from the server.\"\"\"\n    try:\n        response, process_time, node_id, pod_name = send_get_request(\n            f\"{SERVER_URL}/help\"\n        )\n        print(\"Process Time: \", process_time)\n        print(\"Node ID: \", node_id)\n        print(\"K8S_POD_NAME: \", pod_name)\n        print(\"Response\")\n        print(json.dumps(response, indent=4))\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n\n\ndef option_run_with_xai():\n    \"\"\"Run AI service with XAI.\"\"\"\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n\n    print(\n        \"Note that currently only GradCAM methods on image-classification models are supported.\"\n    )\n    while True:\n        gradcam_method_name = input(\n            f\"Please select a GradCAM method (options: {XAI_GRADCAM_METHODS}): \"\n        )\n        if gradcam_method_name not in XAI_GRADCAM_METHODS:\n            print(f\"Invalid GradCAM method. Please select again.\")\n        else:\n            break\n\n    # ask for target class for explanation\n    target_category_indexes = input(\n        \"Please input target category indexes for explanation (comma-separated, e.g., 111, 32, 44, ...): \"\n    )\n    if not target_category_indexes or not target_category_indexes.strip():\n        print(\n            \"No target category indexes provided. Defaulting to explaining the top confident category.\"\n        )\n        target_category_indexes = []\n    else:\n        target_category_indexes = [\n            int(i.strip()) for i in target_category_indexes.split(\",\")\n        ]\n\n    data = {\n        **data,\n        \"ue_id\": UE_ID,\n        \"gradcam_method_name\": gradcam_method_name,\n        \"target_category_indexes\": target_category_indexes,\n    }\n    print(\"Data: \", data)\n    response, process_time, node_id, k8s_pod_name = send_post_request(\n        f\"{SERVER_URL}/xai_model/run\", data, files\n    )\n    print(\"Process Time: \", process_time)\n    print(\"Node ID: \", node_id)\n    print(\"K8S_POD_NAME: \", k8s_pod_name)\n\n    # Handle JSON response\n    model_results = response.get(\"model_results\")\n    if model_results:\n        print(\"Model Results:\", json.dumps(model_results, indent=4))\n\n    xai_results = response.get(\"xai_results\")\n    if xai_results:\n        print(\"XAI Results Method:\", xai_results.get(\"xai_method\"))\n\n    # Handle binary image response\n    encoded_image = xai_results.get(\"image\")\n    if encoded_image:\n        image_bytes = base64.b64decode(encoded_image)\n\n        # Load the image into Pillow\n        image = Image.open(BytesIO(image_bytes))\n\n        # Save image to disk\n        image.save(\"xai_output.png\")\n\n        # Display the image using matplotlib\n        plt.imshow(image)\n        plt.axis(\"off\")\n        plt.show()\n\n\ndef option_profile_run_with_xai():\n    \"\"\"Run AI service with XAI and profile the run.\"\"\"\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n    print(\n        \"Note that currently only GradCAM methods on image-classification models are supported.\"\n    )\n\n    # ask for target class for explanation\n    target_category_indexes = input(\n        \"Please input target category indexes for explanation (comma-separated, e.g., 111, 32, 44, ...): \"\n    )\n    if not target_category_indexes or not target_category_indexes.strip():\n        print(\n            \"No target category indexes provided. Defaulting to explaining the top confident category.\"\n        )\n        target_category_indexes = []\n    else:\n        target_category_indexes = [\n            int(i.strip()) for i in target_category_indexes.split(\",\")\n        ]\n\n    num_requests = int(input(\"Enter the number of requests to send: \"))\n\n    try:\n        for gradcam_method_name in XAI_GRADCAM_METHODS:\n\n            data = {\n                **data,\n                \"ue_id\": UE_ID,\n                \"gradcam_method_name\": gradcam_method_name,\n                \"target_category_indexes\": target_category_indexes,\n            }\n            print(\"Data: \", data)\n\n            profile_result_processor = ProfileResultProcessor(SERVER_URL)\n\n            for _ in range(num_requests):\n                response, process_time, node_id, k8s_pod_name = send_post_request(\n                    f\"{SERVER_URL}/xai_model/profile_run\", data, files\n                )\n                print(\"Process Time: \", process_time)\n                print(\"Node ID: \", node_id)\n                print(\"K8S_POD_NAME: \", k8s_pod_name)\n                if not response:\n                    print(\"No profile response received.\")\n                    continue\n\n                # Handle JSON response\n                model_results = response.get(\"model_results\")\n                if model_results:\n                    print(\"Model Results:\", json.dumps(model_results, indent=4))\n\n                profile_result_processor.process_new_response(\n                    response,\n                    process_time=process_time,\n                    node_id=node_id,\n                    k8s_pod_name=k8s_pod_name,\n                    gradcam_method_name=gradcam_method_name,\n                )\n\n            # Print the final profile result and update the service_data.json\n            profile_result_processor.complete_profile()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\n\nOPTIONS = [\n    {\n        \"label\": \"Get help information\",\n        \"action\": option_help,\n    },\n    {\n        \"label\": \"Run AI service\",\n        \"action\": option_run,\n    },\n    {\n        \"label\": \"Profile AI service\",\n        \"action\": option_profile_run,\n    },\n    {\n        \"label\": \"Run AI service with XAI (only image-classification models)\",\n        \"action\": option_run_with_xai,\n    },\n    {\n        \"label\": \"Profile AI service with XAI (only image-classification models)\",\n        \"action\": option_profile_run_with_xai,\n    },\n]\n\n\nif __name__ == \"__main__\":\n    while True:\n        print(\"\\nOptions:\")\n        for i, option in enumerate(OPTIONS, start=1):\n            print(f\"{i}. {option['label']}\")\n        print(\"q. Quit\")\n        choice = input(\"Enter your choice: \")\n\n        if choice == \"q\":\n            print(\"Exiting the client. Goodbye!\")\n            break\n        else:\n            try:\n                choice = int(choice)\n                if 1 <= choice <= len(OPTIONS):\n                    OPTIONS[choice - 1][\"action\"]()\n                else:\n                    print(\"Invalid choice. Please try again.\")\n            except ValueError:\n                print(\"Invalid input. Please enter a number.\")\n",
        "ai_client_utils_script_content": "# import necessary libraries\nimport os\n\ndef prepare_ai_service_request_files():\n    \"\"\"Prepare the `files` part for the AI service request.\"\"\"\n    files = {}\n    image_file_path = input(\"Please input the image file path: \")\n    if not os.path.exists(image_file_path):\n        raise FileNotFoundError(f\"The image file {image_file_path} does not exist.\")\n    with open(image_file_path, \"rb\") as image_file:\n        files[\"file\"] = image_file.read()\n    return files\n\ndef prepare_ai_service_request_data():\n    \"\"\"Prepare the `data` part including `ue_id` for the AI service request.\"\"\"\n    data = {}\n    ue_id = input(\"Please input the unique execution ID (ue_id): \")\n    data[\"ue_id\"] = ue_id\n    return data",
        "model_script_content": "# import server utils\nfrom ai_server_utils import (\n    process_model_output_logits,\n    profile_activities,\n    prepare_profile_results,\n)\n# import profile utils\nfrom torch.profiler import profile, record_function\n\n# import necessary libs for AI model inference and request handling\nimport torch\nfrom fastapi import APIRouter, File, Form, UploadFile\nfrom fastapi.responses import JSONResponse\nfrom transformers import AutoImageProcessor, MobileViTForImageClassification\nfrom PIL import Image\n\n# --------------------------------\n# Model-specific configuration\n# make sure the variables `MODEL_NAME` and `model` are defined here.\n# --------------------------------\nMODEL_NAME = \"apple/mobilevit-small\"\nprocessor = AutoImageProcessor.from_pretrained(MODEL_NAME)\nmodel = MobileViTForImageClassification.from_pretrained(MODEL_NAME)\nmodel.eval()\n\n# Initialize the FastAPI router\nrouter = APIRouter()\n\n@router.post(\"/run\")\nasync def run_model(file: UploadFile = File(...), ue_id: str = Form(...)):\n    try:\n        # Prepare the model input\n        image = Image.open(file.file).convert(\"RGB\")\n        inputs = processor(images=image, return_tensors=\"pt\")\n\n        # Perform inference\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        # Process the model outputs\n        predictions = process_model_output_logits(model, outputs.logits)\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"model_results\": predictions,\n            }\n        )\n    except Exception as e:\n        print(f\"Error processing file: {e}\")\n        return JSONResponse(\n            content={\"error\": \"Failed to process the image. {e}\".format(e=str(e))},\n            status_code=500,\n        )\n\n@router.post(\"/profile_run\")\nasync def profile_run(file: UploadFile = File(...), ue_id: str = Form(...)):\n    \"\"\"\n    Endpoint to profile the AI model execution.\n    \"\"\"\n    try:\n        # Prepare the model input\n        image = Image.open(file.file).convert(\"RGB\")\n        inputs = processor(images=image, return_tensors=\"pt\")\n\n        # perform profiling\n        with profile(\n            activities=profile_activities,\n            profile_memory=True,\n        ) as prof:\n            with record_function(\"model_run\"):\n                with torch.no_grad():\n                    model_outputs = model(**inputs)\n\n        profile_result = prepare_profile_results(prof)\n\n        # Process the model outputs\n        predictions = process_model_output_logits(model, model_outputs.logits)\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"profile_result\": profile_result,\n                \"model_results\": predictions,\n            }\n        )\n\n    except Exception as e:\n        print(f\"Error processing request: {e}\")\n        return JSONResponse(\n            content={\"error\": f\"Failed to process the request. {e}\"},\n            status_code=500,\n        )\n\n# Below are the model input and output specifications to be used by the `/help` endpoint\nMODEL_INPUT_FORM_SPEC = {\n    \"file\": {\n        \"type\": \"file upload\",\n        \"description\": \"The image file to be classified.\",\n        \"required\": True,\n        \"example\": \"puppy.png\",\n    }\n}\n\nMODEL_OUTPUT_JSON_SPEC = {\n    \"ue_id\": \"unique execution ID\",\n    \"model_results\": [\n        {\n            \"category_id\": \"category id\",\n            \"label\": \"category label\",\n            \"probability\": \"probability value\",\n        }\n    ],\n}",
        "healthcheck_script_content": "import requests\n\nprint(requests.get(\"http://localhost:8000/help\"))",
        "xai_model_script_content": "from typing import Callable, List, Optional\nfrom fastapi import APIRouter, File, Form, UploadFile\nfrom fastapi.responses import JSONResponse\nfrom PIL import Image\nfrom torch.profiler import profile, record_function\nfrom pytorch_grad_cam import (\n    GradCAM,\n    HiResCAM,\n    AblationCAM,\n    XGradCAM,\n    GradCAMPlusPlus,\n    ScoreCAM,\n    LayerCAM,\n    EigenCAM,\n    EigenGradCAM,\n    KPCA_CAM,\n    RandomCAM,\n)\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\nfrom PIL import Image\nimport numpy as np\nimport torch\nfrom torchvision import transforms\nfrom transformers import AutoImageProcessor\nfrom typing import List, Optional\n\n\n# import model utilities\nfrom ai_server_utils import (\n    encode_image,\n    prepare_profile_results,\n    process_model_output_logits,\n    profile_activities,\n)\n\n# Currently only support GradCAM on image-classification models.\n# so we import the model directly from the model.py file\nfrom model import model, processor as resize_and_normalize_processor\n\n\nresize_only_processor = transforms.Compose(\n    [\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n    ],\n)\n\nGRADCAM_METHODS = {\n    \"GradCAM\": GradCAM,\n    \"HiResCAM\": HiResCAM,\n    \"AblationCAM\": AblationCAM,\n    \"XGradCAM\": XGradCAM,\n    \"GradCAMPlusPlus\": GradCAMPlusPlus,\n    \"ScoreCAM\": ScoreCAM,\n    \"LayerCAM\": LayerCAM,\n    \"EigenCAM\": EigenCAM,\n    \"EigenGradCAM\": EigenGradCAM,\n    \"KPCA_CAM\": KPCA_CAM,\n    \"RandomCAM\": RandomCAM,\n}\n\n\nclass HuggingfaceToTensorModelWrapper(torch.nn.Module):\n    \"\"\"Model wrapper to return a tensor\"\"\"\n\n    def __init__(self, model):\n        super(HuggingfaceToTensorModelWrapper, self).__init__()\n        self.model = model\n\n    def forward(self, x):\n        return self.model(x).logits\n\n\ndef get_model_to_tensor_wrapper_class():\n    \"\"\"Helper function to get the model wrapper class.\"\"\"\n    return HuggingfaceToTensorModelWrapper\n\n\ndef get_target_layers_for_grad_cam(model: torch.nn.Module):\n    \"\"\"Helper function to get the target layer for GradCAM.\"\"\"\n    return [model.mobilevit.encoder.layer[-1]]\n\n\ndef get_classifier_output_target_class():\n    \"\"\"Helper function to get the classifier output target class.\"\"\"\n    return ClassifierOutputTarget\n\n\ndef get_reshape_transform():\n    \"\"\"Helper function to get the reshape transform for GradCAM.\"\"\"\n    return None\n\n\ndef run_grad_cam_on_image(\n    model: torch.nn.Module,\n    target_layers: List[torch.nn.Module],\n    targets_for_gradcam: Optional[List[Callable]],\n    reshape_transform: Optional[Callable],\n    input_tensor: torch.nn.Module,\n    input_image: Image,\n    gradcam_method: Callable,\n):\n    \"\"\"Helper function to run GradCAM on an image and create a visualization.\n    Since the classification target is None, the highest scoring category will be used for every image in the batch.\n    \"\"\"\n\n    with gradcam_method(\n        model=model,\n        target_layers=target_layers,\n        reshape_transform=reshape_transform,\n    ) as cam:\n\n        # Replicate the tensor for each of the categories we want to create Grad-CAM for:\n        repeated_tensor = input_tensor[None, :].repeat(\n            (\n                1\n                if targets_for_gradcam is None or len(targets_for_gradcam) == 0\n                else len(targets_for_gradcam)\n            ),\n            1,\n            1,\n            1,\n        )\n\n        batch_results = cam(\n            input_tensor=repeated_tensor,\n            targets=(\n                None\n                if targets_for_gradcam is None or len(targets_for_gradcam) == 0\n                else targets_for_gradcam\n            ),\n        )\n        results = []\n        for grayscale_cam in batch_results:\n            # adjust the shape of the input_image from (3, 244, 244) to (244, 244, 3)\n            visualization = show_cam_on_image(\n                np.float32(input_image.permute(1, 2, 0).numpy()),\n                grayscale_cam,\n                use_rgb=True,\n            )\n            results.append(visualization)\n        output_image = Image.fromarray(np.hstack(results))\n\n        return output_image, cam.outputs\n\n\n# Initialize the FastAPI router\nrouter = APIRouter()\n\n\n@router.post(\"/run\")\nasync def run_model(\n    file: UploadFile = File(...),\n    ue_id: str = Form(...),\n    gradcam_method_name: str = Form(...),\n    target_category_indexes: Optional[List[int]] = Form(None),\n):\n    \"\"\"\n    Endpoint to run the XAI model.\"\"\"\n\n    try:\n        # Prepare the model input\n        print(\"Preparing the model input...\")\n        image = Image.open(file.file).convert(\"RGB\")\n        normalized_image_tensor = resize_and_normalize_processor(\n            images=image, return_tensors=\"pt\"\n        )[\"pixel_values\"].squeeze(0)\n        original_image_tensor = resize_only_processor(image)\n\n        if target_category_indexes is None or len(target_category_indexes) == 0:\n            targets_for_gradcam = None\n        else:\n            # Convert to output target from category indexes\n            targets_for_gradcam = [\n                ClassifierOutputTarget(index) for index in target_category_indexes\n            ]\n        assert (\n            gradcam_method_name in GRADCAM_METHODS\n        ), f\"GradCAM method '{gradcam_method_name}' is not supported. \"\n        gradcam_method = GRADCAM_METHODS[gradcam_method_name]\n\n        model_wrapper_class = get_model_to_tensor_wrapper_class()\n        target_layers = get_target_layers_for_grad_cam(model)\n        reshape_transform = get_reshape_transform()\n\n        # Perform inference\n        print(\"Running GradCAM...\")\n        xai_image, model_output_logits = run_grad_cam_on_image(\n            model=model_wrapper_class(model),\n            target_layers=target_layers,\n            targets_for_gradcam=targets_for_gradcam,\n            reshape_transform=reshape_transform,\n            input_tensor=normalized_image_tensor,\n            input_image=original_image_tensor,\n            gradcam_method=gradcam_method,\n        )\n\n        predictions = process_model_output_logits(model, model_output_logits)\n\n        return JSONResponse(\n            {\n                \"ue_id\": ue_id,\n                \"xai_results\": {\n                    \"image\": encode_image(xai_image),\n                    \"xai_method\": gradcam_method_name,\n                },\n                \"model_results\": predictions,\n            }\n        )\n\n    except Exception as e:\n        print(f\"Error processing file: {e}\")\n        return JSONResponse(\n            content={\"error\": \"Failed to process the image. {e}\".format(e=str(e))},\n            status_code=500,\n        )\n\n\n@router.post(\"/profile_run\")\nasync def profile_run(\n    file: UploadFile = File(...),\n    ue_id: str = Form(...),\n    gradcam_method_name: str = Form(...),\n    target_category_indexes: Optional[List[int]] = Form(None),\n):\n    \"\"\"\n    Endpoint to profile the XAI run.\n    \"\"\"\n    try:\n        # Prepare the model input\n        image = Image.open(file.file).convert(\"RGB\")\n        normalized_image_tensor = resize_and_normalize_processor(\n            images=image, return_tensors=\"pt\"\n        )[\"pixel_values\"].squeeze(0)\n        original_image_tensor = resize_only_processor(image)\n        if target_category_indexes is None or len(target_category_indexes) == 0:\n            targets_for_gradcam = None\n        else:\n            # Convert to output target from category indexes\n            targets_for_gradcam = [\n                ClassifierOutputTarget(index) for index in target_category_indexes\n            ]\n\n        assert (\n            gradcam_method_name in GRADCAM_METHODS\n        ), f\"GradCAM method '{gradcam_method_name}' is not supported. \"\n        gradcam_method = GRADCAM_METHODS[gradcam_method_name]\n\n        model_wrapper_class = get_model_to_tensor_wrapper_class()\n        target_layers = get_target_layers_for_grad_cam(model)\n        reshape_transform = get_reshape_transform()\n\n        # perform profiling\n        with profile(\n            activities=profile_activities,\n            profile_memory=True,\n        ) as prof:\n            with record_function(\"xai_model_run\"):\n\n                # Perform inference\n                xai_image, model_output_logits = run_grad_cam_on_image(\n                    model=model_wrapper_class(model),\n                    target_layers=target_layers,\n                    targets_for_gradcam=targets_for_gradcam,\n                    reshape_transform=reshape_transform,\n                    input_tensor=normalized_image_tensor,\n                    input_image=original_image_tensor,\n                    gradcam_method=gradcam_method,\n                )\n\n        return JSONResponse(\n            {\n                \"ue_id\": ue_id,\n                \"xai_results\": {\n                    \"image\": encode_image(xai_image),\n                    \"xai_method\": gradcam_method_name,\n                },\n                \"model_results\": process_model_output_logits(\n                    model, model_output_logits\n                ),\n                \"profile_result\": prepare_profile_results(prof),\n            }\n        )\n\n    except Exception as e:\n        print(f\"Error processing request: {e}\")\n        return JSONResponse(\n            content={\"error\": f\"Failed to process the request. {e}\"},\n            status_code=500,\n        )\n\n\nXAI_OUTPUT_JSON_SPEC = {\n    \"xai_results\": {\n        \"image\": \"XAI image result\",\n        \"xai_method\": \"XAI method used\",\n    }\n}\n"
    }
}