{
    "model_name": "nvidia/mit-b0",
    "model_url": "https://huggingface.co/nvidia/mit-b0",
    "task": "image-classification",
    "task_detail": "The NVIDIA/mit-b0 pre-trained AI model is a hierarchical Transformer-based encoder specifically designed for applications in computer vision, particularly revolving around semantic segmentation tasks. It represents one of the variants in the SegFormer model suite, known for delivering optimal performance in semantic segmentation by leveraging both the power of Transformers and Multi-Layer Perceptrons (MLPs).\n\n### Functionality Description\n\n#### Core Capabilities\n1. **Hierarchical Transformer Encoder**: The model employs a hierarchical approach, utilizing Transformer architectures that intricately capture various levels of spatial hierarchies present in visual data. This enhances its capacity to understand and represent complex image features at different granularities.\n\n2. **Pre-training and Fine-tuning Paradigm**: Initially pre-trained on the extensive ImageNet-1k dataset, this model processes a broad spectrum of visual concepts, offering a robust foundation for subsequent fine-tuning. Fine-tuning adapts the model to specific downstream tasks, particularly efficient for semantic segmentation.\n\n3. **Semantic Segmentation**: While the repository provides the pre-trained encoder without the lightweight MLP decode head, the model can be effectively adapted for fine-tuning to achieve semantic segmentation across varied visual datasets. It has demonstrated high performance in benchmark tasks such as ADE20K and Cityscapes.\n\n#### Intended Use Cases\n- **Semantic Segmentation**: Practical for scenarios requiring precise delineation of objects and structures within images, such as autonomous driving (road and obstacle identification), medical imaging (structure delineation), and urban planning (land use classification).\n- **Transfer Learning**: Users can harness the model for transfer learning, adapting its pre-trained knowledge for diverse image classification and segmentation tasks tailored to custom datasets beyond ImageNet-1k.\n\n#### Input and Output Format\n- **Input Format**: The model processes images formatted typically as PIL Image objects, oftentimes sourced from URLs or local storage. It expects inputs that adhere to ImageNet-1k standards for optimum performance.\n- **Output Format**: After processing, the model generates logits representing the probability across the 1,000 ImageNet classes, from which the predicted class can be identified by determining the class index with the highest score.\n\nWith its foundation on Transformer architectures, this model is an embodiment of modern advancements in computer vision, striking a balance between simplicity and efficiency. As part of the Hugging Face repository, it also offers seamless integration with their ecosystem, providing expansive resources and documentation for users aiming to leverage its capabilities for advanced image processing tasks. \n\nThis detailed functionality description is crafted to assist in semantic linking against use case scenarios, optimizing both comprehension and applicability for varied industry needs.",
    "accuracy_info": "No accuracy information found.",
    "image_repository_url": "",
    "service_disk_size_bytes": 3609359837,
    "profiles": [
        {
            "node_id": "ugurcan.celik",
            "device_type": "DeviceType.CPU",
            "device_name": "cuda",
            "initialization_time_ms": 3737.701177597046,
            "eviction_time_ms": 0,
            "initialization_cost": 0,
            "keep_alive_cost": 0,
            "energy_consumption_idle": 0,
            "inference": {
                "cpu_time_ms": 185.22334999999998,
                "device_time_ms": 6.892565999999687,
                "cpu_memory_usage_MB": 0.0,
                "self_cpu_memory_usage_MB": 0.0,
                "device_memory_usage_MB": 8.12890625,
                "self_device_memory_usage_MB": -367.2666015625,
                "energy_consumption_execution": 0,
                "disk_IO_MB": 0,
                "input_data_MB": 0,
                "output_data_MB": 0,
                "execution_time_ms": 290.22804896036786,
                "execution_cost": 0
            },
            "xai": [
                {
                    "xai_method": "GradCAM",
                    "cpu_time_ms": 217.371792,
                    "device_time_ms": 3.2873749999999893,
                    "cpu_memory_usage_MB": 0.50006103515625,
                    "self_cpu_memory_usage_MB": 0.246246337890625,
                    "device_memory_usage_MB": 242.74658203125,
                    "self_device_memory_usage_MB": -147.3759765625,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 784.5080693562826,
                    "execution_cost": 0
                },
                {
                    "xai_method": "HiResCAM",
                    "cpu_time_ms": 38.375471000000005,
                    "device_time_ms": 3.261615999999993,
                    "cpu_memory_usage_MB": 0.50006103515625,
                    "self_cpu_memory_usage_MB": 0.246246337890625,
                    "device_memory_usage_MB": 221.8876953125,
                    "self_device_memory_usage_MB": -147.3759765625,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 704.667886098226,
                    "execution_cost": 0
                },
                {
                    "xai_method": "XGradCAM",
                    "cpu_time_ms": 35.735322999999994,
                    "device_time_ms": 3.263443000000007,
                    "cpu_memory_usage_MB": 0.50006103515625,
                    "self_cpu_memory_usage_MB": 0.246246337890625,
                    "device_memory_usage_MB": 221.8876953125,
                    "self_device_memory_usage_MB": -147.3759765625,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 705.5991490681967,
                    "execution_cost": 0
                },
                {
                    "xai_method": "GradCAMPlusPlus",
                    "cpu_time_ms": 38.253601,
                    "device_time_ms": 3.269622999999996,
                    "cpu_memory_usage_MB": 0.50006103515625,
                    "self_cpu_memory_usage_MB": 0.246246337890625,
                    "device_memory_usage_MB": 221.8876953125,
                    "self_device_memory_usage_MB": -147.3759765625,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 718.1201775868734,
                    "execution_cost": 0
                },
                {
                    "xai_method": "ScoreCAM",
                    "cpu_time_ms": 1043.635375,
                    "device_time_ms": 1001.0616359999989,
                    "cpu_memory_usage_MB": 64.25006103515625,
                    "self_cpu_memory_usage_MB": -0.006683349609375,
                    "device_memory_usage_MB": 221.8876953125,
                    "self_device_memory_usage_MB": -94840.15625,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 4572.356144587199,
                    "execution_cost": 0
                },
                {
                    "xai_method": "LayerCAM",
                    "cpu_time_ms": 35.811116999999996,
                    "device_time_ms": 3.2891309999999905,
                    "cpu_memory_usage_MB": 0.50006103515625,
                    "self_cpu_memory_usage_MB": 0.246246337890625,
                    "device_memory_usage_MB": 221.8876953125,
                    "self_device_memory_usage_MB": -147.3759765625,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 803.4454981486002,
                    "execution_cost": 0
                },
                {
                    "xai_method": "EigenCAM",
                    "cpu_time_ms": 29.557176,
                    "device_time_ms": 3.284845000000002,
                    "cpu_memory_usage_MB": 0.25006103515625,
                    "self_cpu_memory_usage_MB": -0.003753662109375,
                    "device_memory_usage_MB": 221.8876953125,
                    "self_device_memory_usage_MB": -147.375,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 280.8982531229655,
                    "execution_cost": 0
                },
                {
                    "xai_method": "EigenGradCAM",
                    "cpu_time_ms": 170.629817,
                    "device_time_ms": 3.2961749999999954,
                    "cpu_memory_usage_MB": 0.50006103515625,
                    "self_cpu_memory_usage_MB": 0.246246337890625,
                    "device_memory_usage_MB": 221.8876953125,
                    "self_device_memory_usage_MB": -147.3759765625,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 816.7763551076254,
                    "execution_cost": 0
                },
                {
                    "xai_method": "KPCA_CAM",
                    "cpu_time_ms": 21.954662,
                    "device_time_ms": 3.2845579999999943,
                    "cpu_memory_usage_MB": 0.25006103515625,
                    "self_cpu_memory_usage_MB": -0.003753662109375,
                    "device_memory_usage_MB": 221.8876953125,
                    "self_device_memory_usage_MB": -147.375,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 319.51133410135907,
                    "execution_cost": 0
                },
                {
                    "xai_method": "RandomCAM",
                    "cpu_time_ms": 34.616454000000004,
                    "device_time_ms": 3.2927590000000047,
                    "cpu_memory_usage_MB": 0.50006103515625,
                    "self_cpu_memory_usage_MB": 0.246246337890625,
                    "device_memory_usage_MB": 221.8876953125,
                    "self_device_memory_usage_MB": -147.3759765625,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 733.8398297627767,
                    "execution_cost": 0
                }
            ],
            "idle_container_cpu_memory_usage": "1.3GB",
            "idle_container_device_memory_usage": "2.3GB"
        },
        {
            "node_id": "LAP004262",
            "device_type": "DeviceType.CPU",
            "device_name": "None",
            "initialization_time_ms": 6111.06014251709,
            "eviction_time_ms": 0,
            "initialization_cost": 0,
            "keep_alive_cost": 0,
            "energy_consumption_idle": 0,
            "inference": {
                "cpu_time_ms": 108.19508499999999,
                "device_time_ms": 0.0,
                "cpu_memory_usage_MB": 0.003814697265625,
                "self_cpu_memory_usage_MB": -363.267578125,
                "device_memory_usage_MB": 0.0,
                "self_device_memory_usage_MB": 0.0,
                "energy_consumption_execution": 0,
                "disk_IO_MB": 0,
                "input_data_MB": 0,
                "output_data_MB": 0,
                "execution_time_ms": 203.20367813110352,
                "execution_cost": 0
            },
            "xai": [
                {
                    "xai_method": "GradCAM",
                    "cpu_time_ms": 427.19369,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 234.030517578125,
                    "self_cpu_memory_usage_MB": -160.7990493774414,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 947.9987621307373,
                    "execution_cost": 0
                },
                {
                    "xai_method": "HiResCAM",
                    "cpu_time_ms": 505.29799099999997,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 220.38766479492188,
                    "self_cpu_memory_usage_MB": -160.7990493774414,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 975.2175013224283,
                    "execution_cost": 0
                },
                {
                    "xai_method": "XGradCAM",
                    "cpu_time_ms": 447.40744200000006,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 220.38766479492188,
                    "self_cpu_memory_usage_MB": -160.7990493774414,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 1084.2050711313884,
                    "execution_cost": 0
                },
                {
                    "xai_method": "GradCAMPlusPlus",
                    "cpu_time_ms": 469.73238,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 220.38766479492188,
                    "self_cpu_memory_usage_MB": -160.7677993774414,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 923.3861764272054,
                    "execution_cost": 0
                },
                {
                    "xai_method": "LayerCAM",
                    "cpu_time_ms": 461.69106,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 220.38766479492188,
                    "self_cpu_memory_usage_MB": -160.7677993774414,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 1095.572868982951,
                    "execution_cost": 0
                },
                {
                    "xai_method": "EigenCAM",
                    "cpu_time_ms": 639.372952,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 220.13766479492188,
                    "self_cpu_memory_usage_MB": -147.12493896484375,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 543.7111854553223,
                    "execution_cost": 0
                },
                {
                    "xai_method": "EigenGradCAM",
                    "cpu_time_ms": 524.837486,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 220.38766479492188,
                    "self_cpu_memory_usage_MB": -160.7677993774414,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 1155.2560329437256,
                    "execution_cost": 0
                },
                {
                    "xai_method": "KPCA_CAM",
                    "cpu_time_ms": 138.432713,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 220.13766479492188,
                    "self_cpu_memory_usage_MB": -147.12493896484375,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 331.7454655965169,
                    "execution_cost": 0
                },
                {
                    "xai_method": "RandomCAM",
                    "cpu_time_ms": 454.69828,
                    "device_time_ms": 0.0,
                    "cpu_memory_usage_MB": 220.38766479492188,
                    "self_cpu_memory_usage_MB": -160.7677993774414,
                    "device_memory_usage_MB": 0.0,
                    "self_device_memory_usage_MB": 0.0,
                    "energy_consumption_execution": 0,
                    "disk_IO_MB": 0,
                    "input_data_MB": 0,
                    "output_data_MB": 0,
                    "execution_time_ms": 1093.5981273651123,
                    "execution_cost": 0
                }
            ],
            "idle_container_cpu_memory_usage": "1.7GB",
            "idle_container_device_memory_usage": "0GB"
        }
    ],
    "feedback": {
        "likes": [],
        "dislikes": [],
        "comments": []
    },
    "code": {
        "readme_content": "---\nlicense: other\ntags:\n- vision\ndatasets:\n- imagenet_1k\nwidget:\n- src: https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\n  example_title: House\n- src: https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000002.jpg\n  example_title: Castle\n---\n\n# SegFormer (b0-sized) encoder pre-trained-only\n\nSegFormer encoder fine-tuned on Imagenet-1k. It was introduced in the paper [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203) by Xie et al. and first released in [this repository](https://github.com/NVlabs/SegFormer). \n\nDisclaimer: The team releasing SegFormer did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nSegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes. The hierarchical Transformer is first pre-trained on ImageNet-1k, after which a decode head is added and fine-tuned altogether on a downstream dataset.\n\nThis repository only contains the pre-trained hierarchical Transformer, hence it can be used for fine-tuning purposes.\n\n## Intended uses & limitations\n\nYou can use the model for fine-tuning of semantic segmentation. See the [model hub](https://huggingface.co/models?other=segformer) to look for fine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\n\n```python\nfrom transformers import SegformerImageProcessor, SegformerForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nimage_processor = SegformerImageProcessor.from_pretrained(\"nvidia/mit-b0\")\nmodel = SegformerForImageClassification.from_pretrained(\"nvidia/mit-b0\")\n\ninputs = image_processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n```\n\nFor more code examples, we refer to the [documentation](https://huggingface.co/transformers/model_doc/segformer.html#).\n\n### License\n\nThe license for this model can be found [here](https://github.com/NVlabs/SegFormer/blob/master/LICENSE).\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-2105-15203,\n  author    = {Enze Xie and\n               Wenhai Wang and\n               Zhiding Yu and\n               Anima Anandkumar and\n               Jose M. Alvarez and\n               Ping Luo},\n  title     = {SegFormer: Simple and Efficient Design for Semantic Segmentation with\n               Transformers},\n  journal   = {CoRR},\n  volume    = {abs/2105.15203},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2105.15203},\n  eprinttype = {arXiv},\n  eprint    = {2105.15203},\n  timestamp = {Wed, 02 Jun 2021 11:46:42 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-15203.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n",
        "dockerfile_content": "FROM python3.12_ai_service_base:latest\n\nWORKDIR /app\n\nCOPY . .\n\nRUN pip install --no-cache-dir fastapi uvicorn transformers Pillow requests\n\nEXPOSE 8000\n\nCMD [\"uvicorn\", \"ai_server:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--timeout-keep-alive\", \"600\"]",
        "ai_server_script_content": "import json\nimport os\nimport time\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import JSONResponse\nfrom contextlib import asynccontextmanager\nfrom ai_server_utils import (\n    PROFILE_OUTPUT_JSON_SPEC,\n    NODE_ID,\n    K8S_POD_NAME,\n)\n\n\n# -------------------------------------------\n# App Lifespan setup\n# -------------------------------------------\n# Record the script start time (when uvicorn starts the process)\nSCRIPT_START_TIME = time.time()\nINITIALIZATION_DURATION = 0.0\nservice_endpoint_specs = {\n    \"model_input_form_spec\": None,\n    \"model_output_json_spec\": None,\n    \"profile_output_json_spec\": None,\n    \"xai_model_input_form_spec\": None,\n    \"xai_model_output_json_spec\": None,\n    \"xai_profile_output_json_spec\": None,\n}\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n\n    global INITIALIZATION_DURATION\n    global SCRIPT_START_TIME\n    global service_endpoint_specs\n\n    # Load the AI model\n    print(\"Loading AI model...\")\n    from model import (\n        MODEL_INPUT_FORM_SPEC,\n        MODEL_OUTPUT_JSON_SPEC,\n        router as model_router,\n    )\n\n    service_endpoint_specs[\"model_input_form_spec\"] = MODEL_INPUT_FORM_SPEC\n    service_endpoint_specs[\"model_output_json_spec\"] = MODEL_OUTPUT_JSON_SPEC\n    service_endpoint_specs[\"profile_output_json_spec\"] = PROFILE_OUTPUT_JSON_SPEC\n\n    app.include_router(model_router, prefix=\"/model\", tags=[\"AI Model\"])\n\n    # Load the XAI model\n    if os.path.exists(os.path.dirname(__file__) + \"/xai_model.py\"):\n        print(\"Loading XAI model...\")\n        from xai_model import (\n            XAI_OUTPUT_JSON_SPEC,\n            router as xai_model_router,\n        )\n\n        # by default, the xai_model input form spec is the same as the model input form spec\n        service_endpoint_specs[\"xai_model_input_form_spec\"] = MODEL_INPUT_FORM_SPEC\n        service_endpoint_specs[\"xai_model_output_json_spec\"] = MODEL_OUTPUT_JSON_SPEC\n        service_endpoint_specs[\"xai_model_output_json_spec\"].update(\n            XAI_OUTPUT_JSON_SPEC\n        )\n        service_endpoint_specs[\"xai_profile_output_json_spec\"] = (\n            PROFILE_OUTPUT_JSON_SPEC\n        )\n        service_endpoint_specs[\"xai_profile_output_json_spec\"].update(\n            XAI_OUTPUT_JSON_SPEC\n        )\n\n        app.include_router(xai_model_router, prefix=\"/xai_model\", tags=[\"XAI Model\"])\n\n    # Record the initialization duration\n    INITIALIZATION_DURATION = time.time() - SCRIPT_START_TIME\n\n    print(f\"AI service loaded in {INITIALIZATION_DURATION:.2f} seconds.\")\n\n    yield\n\n    # Clean up the models and release the resources\n    service_endpoint_specs.clear()\n\n\n# -------------------------------------------\n# FastAPI application setup\n# -------------------------------------------\napp = FastAPI(lifespan=lifespan)\n\n\n# -------------------------------------------\n# Middlewares\n# -------------------------------------------\n@app.middleware(\"http\")\nasync def prepare_header_middleware(request: Request, call_next):\n    start_time = time.perf_counter()\n    response = await call_next(request)\n    process_time = time.perf_counter() - start_time\n    response.headers[\"X-Process-Time\"] = str(process_time)\n    response.headers[\"X-NODE-ID\"] = NODE_ID\n    response.headers[\"X-K8S-POD-NAME\"] = K8S_POD_NAME\n    return response\n\n\n# -------------------------------------------\n# General Endpoints\n# -------------------------------------------\n@app.get(\"/initialization_duration\")\ndef get_initialization_duration():\n    \"\"\"\n    Endpoint to retrieve the initialization duration of the AI model.\n    \"\"\"\n    global INITIALIZATION_DURATION\n\n    if INITIALIZATION_DURATION == 0.0:\n        return JSONResponse(\n            content={\"error\": \"Model not initialized.\"},\n            status_code=500,\n        )\n    return JSONResponse(\n        content={\n            \"initialization_duration\": INITIALIZATION_DURATION,\n            \"script_start_time\": SCRIPT_START_TIME,\n        }\n    )\n\n\n@app.get(\"/help\")\ndef get_help():\n    global service_endpoint_specs\n    help_info = {\n        \"endpoints\": {\n            \"/model/run\": {\n                \"method\": \"POST\",\n                \"description\": \"Executes the AI model with the provided input data.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_output_json_spec\"],\n                },\n            },\n            \"/model/profile_run\": {\n                \"method\": \"POST\",\n                \"description\": \"Profiles the AI model execution.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    \"profile_result\": \"Profiling results of the AI model execution.\",\n                    **service_endpoint_specs[\"profile_output_json_spec\"],\n                },\n            },\n            \"/initialization_duration\": {\n                \"method\": \"GET\",\n                \"description\": \"Retrieves the initialization duration of the AI model.\",\n                \"response\": {\n                    \"initialization_duration\": \"Time taken to initialize the model (in seconds).\",\n                    \"script_start_time\": \"Timestamp when the script started (in seconds since epoch).\",\n                },\n            },\n        }\n    }\n\n    if service_endpoint_specs[\"xai_model_input_form_spec\"] is not None:\n        help_info[\"endpoints\"][\"/xai_model/run\"] = {\n            \"method\": \"POST\",\n            \"description\": \"Executes the XAI model with the provided input data.\",\n            \"parameters\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_input_form_spec\"],\n            },\n            \"response\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_output_json_spec\"],\n            },\n        }\n\n        help_info[\"endpoints\"][\"/xai_model/profile_run\"] = {\n            \"method\": \"POST\",\n            \"description\": \"Profiles the XAI model execution.\",\n            \"parameters\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_input_form_spec\"],\n            },\n            \"response\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                \"profile_result\": \"Profiling results of the XAI model execution.\",\n                **service_endpoint_specs[\"xai_profile_output_json_spec\"],\n            },\n        }\n    \n    return JSONResponse(content=help_info)\n",
        "ai_server_utils_script_content": "import os\nimport socket\nimport torch\nfrom io import BytesIO\nimport base64\n\nfrom torch.profiler import profile, ProfilerActivity, record_function\n\n\n# -------------------------------------------\n# ENV Variables\n# -------------------------------------------\nNODE_ID = os.getenv(\"NODE_ID\", socket.gethostname())\nK8S_POD_NAME = os.getenv(\"K8S_POD_NAME\", \"UNKNOWN\")\n\n\n# -------------------------------------------\n# Profile Utils\n# -------------------------------------------\nprofile_activities = [\n    ProfilerActivity.CPU,\n    ProfilerActivity.CUDA,\n    ProfilerActivity.MTIA,\n    ProfilerActivity.XPU,\n]\n\ndef get_image_classification_results_from_model_output_logits(model, model_output_logits):\n    \"\"\"\n    Process the model outputs to prepare for the response.\n    \"\"\"\n    probabilities = torch.nn.functional.softmax(model_output_logits[0], dim=0)\n\n    # Return the top 5 predictions with labels\n    top5_prob, top5_catid = torch.topk(probabilities, 5)\n    predictions = []\n    for i in range(top5_prob.size(0)):\n        category_id = top5_catid[i].item()\n        predictions.append(\n            {\n                \"category_id\": category_id,\n                \"label\": model.config.id2label[category_id],\n                \"probability\": top5_prob[i].item(),\n            }\n        )\n    return predictions\n\ndef prepare_profile_results(prof):\n    \"\"\"\n    Prepare the profile results for the model inputs and outputs.\n    \"\"\"\n    print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n\n    profile_event = prof.key_averages()[0]\n\n    profile_result = {\n        \"name\": profile_event.key,\n        \"device_type\": str(profile_event.device_type),\n        \"device_name\": str(profile_event.use_device),\n        \"cpu_memory_usage\": profile_event.cpu_memory_usage,\n        \"self_cpu_memory_usage\": profile_event.self_cpu_memory_usage,\n        \"device_memory_usage\": profile_event.device_memory_usage,\n        \"self_device_memory_usage\": profile_event.self_device_memory_usage,\n        \"cpu_time_total\": profile_event.cpu_time_total,\n        \"self_cpu_time_total\": profile_event.self_cpu_time_total,\n        \"device_time_total\": profile_event.device_time_total,\n        \"self_device_time_total\": profile_event.self_device_time_total,\n    }\n    return profile_result\n\n\ndef encode_image(image):\n    \"\"\"\n    Encode the image to bytes\n    \"\"\"\n    buffered = BytesIO()\n    image.save(buffered, format=\"PNG\")\n    encoded_image = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n    return encoded_image\n\n\nPROFILE_OUTPUT_JSON_SPEC = {\n    \"ue_id\": \"unique execution ID\",\n    \"profile_result\": {\n        \"name\": \"name of the profile event\",\n        \"device_type\": \"type of device used (e.g., CPU, GPU, ...)\",\n        \"device_name\": \"name of the device used\",\n        \"cpu_memory_usage\": \"CPU memory usage in bytes\",\n        \"self_cpu_memory_usage\": \"self CPU memory usage in bytes\",\n        \"device_memory_usage\": \"device memory usage in bytes\",\n        \"self_device_memory_usage\": \"self device memory usage in bytes\",\n        \"cpu_time_total\": \"total CPU time in microseconds\",\n        \"self_cpu_time_total\": \"self total CPU time in microseconds\",\n        \"device_time_total\": \"total device time in microseconds\",\n        \"self_device_time_total\": \"self total device time in microseconds\",\n    },\n    \"model_results\": \"the AI service model results\",\n}\n",
        "ai_client_script_content": "import base64\nimport json\nimport time\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nfrom ai_client_utils import (\n    prepare_ai_service_request_files,\n    prepare_ai_service_request_data,\n)\n\nXAI_GRADCAM_METHODS = [\n    \"GradCAM\",\n    \"HiResCAM\",\n    # \"AblationCAM\",\n    \"XGradCAM\",\n    \"GradCAMPlusPlus\",\n    \"ScoreCAM\",\n    \"LayerCAM\",\n    \"EigenCAM\",\n    \"EigenGradCAM\",\n    \"KPCA_CAM\",\n    \"RandomCAM\",\n]\n\n# -------------------------------------\n# prompt for necessary inputs\n# -------------------------------------\nSERVER_URL = input(\"Please input server URL (default to http://localhost:9000): \")\nif SERVER_URL.strip() == \"\":\n    SERVER_URL = \"http://localhost:9000\"\nUE_ID = input(\"Please input UE_ID (default to 123456): \")\nif UE_ID.strip() == \"\":\n    UE_ID = \"123456\"\n\n\ndef send_post_request(url, data, files):\n    \"\"\"Send request to run AI service and display AI service responses.\"\"\"\n    try:\n        response = requests.post(url, files=files, data=data)\n        # get the process time, node id and k8s pod name from the response headers\n        process_time = response.headers.get(\"X-Process-Time\")\n        node_id = response.headers.get(\"X-NODE-ID\")\n        k8s_pod_name = response.headers.get(\"X-K8S-POD-NAME\")\n        if response.status_code == 200:\n            return response.json(), process_time, node_id, k8s_pod_name\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n\ndef send_get_request(url, params=None):\n    \"\"\"Send GET request to the specified URL and return the response.\"\"\"\n    try:\n        response = requests.get(url, params=params)\n        process_time = response.headers.get(\"X-Process-Time\")\n        node_id = response.headers.get(\"X-NODE-ID\")\n        k8s_pod_name = response.headers.get(\"X-K8S-POD-NAME\")\n        if response.status_code == 200:\n            return response.json(), process_time, node_id, k8s_pod_name\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n            return None\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n\nclass ProfileResultProcessor:\n\n    def __init__(self, server_url):\n        self.server_url = server_url\n        self.start_time = time.time()\n        self.service_initialization_duration = 0\n        self.response_counter = 0\n        self.profile_name = None\n        self.device_type = None\n        self.device_name = None\n        self.node_id = None\n        self.k8s_pod_name = None\n        self.cpu_memory_usage_bytes = 0\n        self.self_cpu_memory_usage_bytes = 0\n        self.device_memory_usage_bytes = 0\n        self.self_device_memory_usage_bytes = 0\n        self.cpu_time_total_us = 0\n        self.self_cpu_time_total_us = 0\n        self.device_time_total_us = 0\n        self.self_device_time_total_us = 0\n\n        # xai related\n        self.gradcam_method_name = None\n\n        self.fetch_service_initialization_duration()\n\n    def fetch_service_initialization_duration(self):\n        \"\"\"Fetch the service initialization duration from the server.\"\"\"\n        response, process_time, node_id, k8s_pod_name = send_get_request(\n            f\"{self.server_url}/initialization_duration\"\n        )\n        if response:\n            self.service_initialization_duration = response.get(\n                \"initialization_duration\", 0\n            )\n        else:\n            print(\"Failed to fetch initialization duration.\")\n            self.service_initialization_duration = 0\n\n    def process_new_response(\n        self,\n        profile_response,\n        process_time=None,\n        node_id=None,\n        k8s_pod_name=None,\n        gradcam_method_name=None,\n    ):\n        if not profile_response:\n            return\n\n        profile_result = profile_response[\"profile_result\"]\n\n        if not profile_result:\n            return\n\n        if self.profile_name is None:\n            self.profile_name = profile_result[\"name\"]\n        if self.device_type is None:\n            self.device_type = profile_result[\"device_type\"]\n        if self.device_name is None:\n            self.device_name = profile_result[\"device_name\"]\n        if self.node_id is None:\n            self.node_id = node_id\n        if self.k8s_pod_name is None:\n            self.k8s_pod_name = k8s_pod_name\n\n        if self.gradcam_method_name is None:\n            self.gradcam_method_name = gradcam_method_name\n\n        # update the max profile result\n        self.cpu_memory_usage_bytes = max(\n            self.cpu_memory_usage_bytes, profile_result[\"cpu_memory_usage\"]\n        )\n        # self cpu memory usage could be negative. here we take the value that has the max absolute value\n        if abs(profile_result[\"self_cpu_memory_usage\"]) > abs( self.self_cpu_memory_usage_bytes):\n            self.self_cpu_memory_usage_bytes = profile_result[\"self_cpu_memory_usage\"]\n        self.device_memory_usage_bytes = max(\n            self.device_memory_usage_bytes, profile_result[\"device_memory_usage\"]\n        )\n        # same as self cpu memory usage\n        if abs(profile_result[\"self_device_memory_usage\"]) > abs(self.self_device_memory_usage_bytes):\n            self.self_device_memory_usage_bytes = profile_result[\"self_device_memory_usage\"]\n        self.cpu_time_total_us = max(\n            self.cpu_time_total_us, profile_result[\"cpu_time_total\"]\n        )\n        self.self_cpu_time_total_us = max(\n            self.self_cpu_time_total_us, profile_result[\"self_cpu_time_total\"]\n        )\n        self.device_time_total_us = max(\n            self.device_time_total_us, profile_result[\"device_time_total\"]\n        )\n        self.self_device_time_total_us = max(\n            self.self_device_time_total_us, profile_result[\"self_device_time_total\"]\n        )\n\n        self.response_counter += 1\n\n    def complete_profile(self):\n        print(\"\\n--------- PROFILE EVENT ---------\\n\")\n        print(f\"Name: {self.profile_name}\")\n        print(f\"Device Type: {self.device_type}\")\n        print(f\"Device Name: {self.device_name}\")\n        print(f\"Node ID: {self.node_id}\")\n        print(f\"K8S_POD_NAME: {self.k8s_pod_name}\")\n\n        if self.gradcam_method_name:\n            print(f\"GradCAM Method Name: {self.gradcam_method_name}\")\n\n        print(\"\\n--------- LATENCY RESULT ---------\\n\")\n        print(\"Service Initialization Duration: \", self.service_initialization_duration)\n        print(\"Total Requests: \", self.response_counter)\n        print(f\"Total Time Taken: {time.time() - self.start_time:.2f} seconds\")\n        print(\n            f\"Average Time Taken: {(time.time() - self.start_time) / self.response_counter:.2f} seconds\"\n        )\n\n        print(\"\\n--------- RESOURCE USAGE ---------\\n\")\n        print(f\"CPU Memory Usage: {self.cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\")\n        print(\n            f\"Self CPU Memory Usage: {self.self_cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Device Memory Usage: {self.device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Self Device Memory Usage: {self.self_device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(f\"CPU Time Total: {self.cpu_time_total_us / 1000:.2f} ms\")\n        print(f\"Self CPU Time Total: {self.self_cpu_time_total_us / 1000:.2f} ms\")\n        print(f\"Device Time Total: {self.device_time_total_us / 1000:.2f} ms\")\n        print(f\"Self Device Time Total: {self.self_device_time_total_us / 1000:.2f} ms\")\n\n        # update the service_data.json automatically\n        with open(\"service_data.json\", \"r\") as f:\n            service_data = json.load(f)\n\n        if not self.gradcam_method_name:\n            complete_profile_data_to_save = {\n                \"node_id\": self.node_id,\n                \"device_type\": self.device_type,\n                \"device_name\": self.device_name,\n                \"initialization_time_ms\": self.service_initialization_duration * 1000,\n                \"eviction_time_ms\": 0,\n                \"initialization_cost\": 0,\n                \"keep_alive_cost\": 0,\n                \"energy_consumption_idle\": 0,\n                \"inference\": {\n                    \"cpu_time_ms\": self.cpu_time_total_us / 1000,\n                    \"device_time_ms\": self.device_time_total_us / 1000,\n                    \"cpu_memory_usage_MB\": self.cpu_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"self_cpu_memory_usage_MB\": self.self_cpu_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"device_memory_usage_MB\": self.device_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"self_device_memory_usage_MB\": self.self_device_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"energy_consumption_execution\": 0,\n                    \"disk_IO_MB\": 0,\n                    \"input_data_MB\": 0,\n                    \"output_data_MB\": 0,\n                    \"execution_time_ms\": (time.time() - self.start_time)\n                    / self.response_counter\n                    * 1000,\n                    \"execution_cost\": 0,\n                },\n            }\n\n            # check if there is already a profile for this node id\n            profile_found = False\n            for profile in service_data[\"profiles\"]:\n                if profile[\"node_id\"] == self.node_id:\n                    profile[\"inference\"] = complete_profile_data_to_save[\"inference\"]\n                    profile_found = True\n                    break\n            if not profile_found:\n                service_data[\"profiles\"].append(complete_profile_data_to_save)\n\n            # save the updated service_data.json\n            with open(\"service_data.json\", \"w\") as f:\n                json.dump(service_data, f, indent=4)\n            print(\"\\n--------- SERVICE DATA UPDATED ---------\\n\")\n\n        else:\n            complete_xai_profile_data_to_save = {\n                \"node_id\": self.node_id,\n                \"device_type\": self.device_type,\n                \"device_name\": self.device_name,\n                \"initialization_time_ms\": self.service_initialization_duration * 1000,\n                \"eviction_time_ms\": 0,\n                \"initialization_cost\": 0,\n                \"keep_alive_cost\": 0,\n                \"energy_consumption_idle\": 0,\n                \"xai\": [\n                    {\n                        \"xai_method\": self.gradcam_method_name,\n                        \"cpu_time_ms\": self.cpu_time_total_us / 1000,\n                        \"device_time_ms\": self.device_time_total_us / 1000,\n                        \"cpu_memory_usage_MB\": self.cpu_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"self_cpu_memory_usage_MB\": self.self_cpu_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"device_memory_usage_MB\": self.device_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"self_device_memory_usage_MB\": self.self_device_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"energy_consumption_execution\": 0,\n                        \"disk_IO_MB\": 0,\n                        \"input_data_MB\": 0,\n                        \"output_data_MB\": 0,\n                        \"execution_time_ms\": (time.time() - self.start_time)\n                        / self.response_counter\n                        * 1000,\n                        \"execution_cost\": 0,\n                    }\n                ],\n            }\n\n            # check if there is already a profile for this node id\n            profile_found = False\n            for profile in service_data[\"profiles\"]:\n                if profile[\"node_id\"] == self.node_id:\n                    profile_found = True\n\n                    # check if there is already a profile for this xai method\n                    xai_method_found = False\n                    if not profile.get(\"xai\"):\n                        profile[\"xai\"] = []\n                    for xai_profile in profile[\"xai\"]:\n                        if xai_profile[\"xai_method\"] == self.gradcam_method_name:\n                            xai_profile.update(complete_xai_profile_data_to_save[\"xai\"][0]) \n                            xai_method_found = True\n                            break\n                    if not xai_method_found:\n                        profile[\"xai\"].append(complete_xai_profile_data_to_save[\"xai\"][0])\n                    break\n\n            if not profile_found:\n                service_data[\"profiles\"].append(complete_xai_profile_data_to_save)\n\n            # save the updated service_data.json\n            with open(\"service_data.json\", \"w\") as f:\n                json.dump(service_data, f, indent=4)\n            print(\"\\n--------- SERVICE DATA UPDATED ---------\\n\")\n\n\ndef option_run():\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n    data = {**data, \"ue_id\": UE_ID}\n    response, process_time, node_id, pod_name = send_post_request(\n        f\"{SERVER_URL}/model/run\", data, files\n    )\n    print(\"Process Time: \", process_time)\n    print(\"Node ID: \", node_id)\n    print(\"K8S_POD_NAME: \", pod_name)\n    print(\"Response\")\n    print(json.dumps(response, indent=4))\n\n\ndef option_profile_run():\n    data = prepare_ai_service_request_data()\n    data = {**data, \"ue_id\": UE_ID}\n    files = prepare_ai_service_request_files()\n    num_requests = int(input(\"Enter the number of requests to send: \"))\n\n    try:\n        profile_result_processor = ProfileResultProcessor(SERVER_URL)\n\n        for _ in range(num_requests):\n            profile_response, process_time, node_id, k8s_node_name = send_post_request(\n                f\"{SERVER_URL}/model/profile_run\", data, files\n            )\n            print(\"Process Time: \", process_time)\n            print(\"Node ID: \", node_id)\n            print(\"K8S_POD_NAME: \", k8s_node_name)\n            print(\"Response\")\n            print(json.dumps(profile_response, indent=4))\n            if not profile_response:\n                print(\"No profile response received.\")\n                continue\n\n            profile_result_processor.process_new_response(\n                profile_response,\n                process_time=process_time,\n                node_id=node_id,\n                k8s_pod_name=k8s_node_name,\n            )\n\n        # Print the final profile result and update the service_data.json\n        profile_result_processor.complete_profile()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\n\ndef option_help():\n    \"\"\"Get help information from the server.\"\"\"\n    try:\n        response, process_time, node_id, pod_name = send_get_request(\n            f\"{SERVER_URL}/help\"\n        )\n        print(\"Process Time: \", process_time)\n        print(\"Node ID: \", node_id)\n        print(\"K8S_POD_NAME: \", pod_name)\n        print(\"Response\")\n        print(json.dumps(response, indent=4))\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n\n\ndef option_run_with_xai():\n    \"\"\"Run AI service with XAI.\"\"\"\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n\n    print(\n        \"Note that currently only GradCAM methods on image-classification models are supported.\"\n    )\n    while True:\n        gradcam_method_name = input(\n            f\"Please select a GradCAM method (options: {XAI_GRADCAM_METHODS}): \"\n        )\n        if gradcam_method_name not in XAI_GRADCAM_METHODS:\n            print(f\"Invalid GradCAM method. Please select again.\")\n        else:\n            break\n\n    # ask for target class for explanation\n    target_category_indexes = input(\n        \"Please input target category indexes for explanation (comma-separated, e.g., 111, 32, 44, ...): \"\n    )\n    if not target_category_indexes or not target_category_indexes.strip():\n        print(\n            \"No target category indexes provided. Defaulting to explaining the top confident category.\"\n        )\n        target_category_indexes = []\n    else:\n        target_category_indexes = [\n            int(i.strip()) for i in target_category_indexes.split(\",\")\n        ]\n\n    data = {\n        **data,\n        \"ue_id\": UE_ID,\n        \"gradcam_method_name\": gradcam_method_name,\n        \"target_category_indexes\": target_category_indexes,\n    }\n    print(\"Data: \", data)\n    response, process_time, node_id, k8s_pod_name = send_post_request(\n        f\"{SERVER_URL}/xai_model/run\", data, files\n    )\n    print(\"Process Time: \", process_time)\n    print(\"Node ID: \", node_id)\n    print(\"K8S_POD_NAME: \", k8s_pod_name)\n\n    # Handle JSON response\n    model_results = response.get(\"model_results\")\n    if model_results:\n        print(\"Model Results:\", json.dumps(model_results, indent=4))\n\n    xai_results = response.get(\"xai_results\")\n    if xai_results:\n        print(\"XAI Results Method:\", xai_results.get(\"xai_method\"))\n\n    # Handle binary image response\n    encoded_image = xai_results.get(\"image\")\n    if encoded_image:\n        image_bytes = base64.b64decode(encoded_image)\n\n        # Load the image into Pillow\n        image = Image.open(BytesIO(image_bytes))\n\n        # Save image to disk\n        image.save(\"xai_output.png\")\n\n        # Display the image using matplotlib\n        plt.imshow(image)\n        plt.axis(\"off\")\n        plt.show()\n\n\ndef option_profile_run_with_xai():\n    \"\"\"Run AI service with XAI and profile the run.\"\"\"\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n    print(\n        \"Note that currently only GradCAM methods on image-classification models are supported.\"\n    )\n\n    # ask for target class for explanation\n    target_category_indexes = input(\n        \"Please input target category indexes for explanation (comma-separated, e.g., 111, 32, 44, ...): \"\n    )\n    if not target_category_indexes or not target_category_indexes.strip():\n        print(\n            \"No target category indexes provided. Defaulting to explaining the top confident category.\"\n        )\n        target_category_indexes = []\n    else:\n        target_category_indexes = [\n            int(i.strip()) for i in target_category_indexes.split(\",\")\n        ]\n\n    num_requests = int(input(\"Enter the number of requests to send: \"))\n\n    try:\n        for gradcam_method_name in XAI_GRADCAM_METHODS:\n\n            data = {\n                **data,\n                \"ue_id\": UE_ID,\n                \"gradcam_method_name\": gradcam_method_name,\n                \"target_category_indexes\": target_category_indexes,\n            }\n            print(\"Data: \", data)\n\n            profile_result_processor = ProfileResultProcessor(SERVER_URL)\n\n            for _ in range(num_requests):\n                response, process_time, node_id, k8s_pod_name = send_post_request(\n                    f\"{SERVER_URL}/xai_model/profile_run\", data, files\n                )\n                print(\"Process Time: \", process_time)\n                print(\"Node ID: \", node_id)\n                print(\"K8S_POD_NAME: \", k8s_pod_name)\n                if not response:\n                    print(\"No profile response received.\")\n                    continue\n\n                # Handle JSON response\n                model_results = response.get(\"model_results\")\n                if model_results:\n                    print(\"Model Results:\", json.dumps(model_results, indent=4))\n\n                profile_result_processor.process_new_response(\n                    response,\n                    process_time=process_time,\n                    node_id=node_id,\n                    k8s_pod_name=k8s_pod_name,\n                    gradcam_method_name=gradcam_method_name,\n                )\n\n            # Print the final profile result and update the service_data.json\n            profile_result_processor.complete_profile()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\n\nOPTIONS = [\n    {\n        \"label\": \"Get help information\",\n        \"action\": option_help,\n    },\n    {\n        \"label\": \"Run AI service\",\n        \"action\": option_run,\n    },\n    {\n        \"label\": \"Profile AI service\",\n        \"action\": option_profile_run,\n    },\n    {\n        \"label\": \"Run AI service with XAI (only image-classification models)\",\n        \"action\": option_run_with_xai,\n    },\n    {\n        \"label\": \"Profile AI service with XAI (only image-classification models)\",\n        \"action\": option_profile_run_with_xai,\n    },\n]\n\n\nif __name__ == \"__main__\":\n    while True:\n        print(\"\\nOptions:\")\n        for i, option in enumerate(OPTIONS, start=1):\n            print(f\"{i}. {option['label']}\")\n        print(\"q. Quit\")\n        choice = input(\"Enter your choice: \")\n\n        if choice == \"q\":\n            print(\"Exiting the client. Goodbye!\")\n            break\n        else:\n            try:\n                choice = int(choice)\n                if 1 <= choice <= len(OPTIONS):\n                    OPTIONS[choice - 1][\"action\"]()\n                else:\n                    print(\"Invalid choice. Please try again.\")\n            except ValueError:\n                print(\"Invalid input. Please enter a number.\")\n",
        "ai_client_utils_script_content": "# import any necessary libraries.\n\ndef prepare_ai_service_request_files():\n    \"\"\"Prepare the `files` part for the AI service request.\"\"\"\n    files = {}\n    image_file_path = input(\"Please input the image file path: \")\n    with open(image_file_path, \"rb\") as image_file:\n        files[\"file\"] = image_file.read()\n    return files\n\ndef prepare_ai_service_request_data():\n    \"\"\"Prepare the `data` part including `ue_id` for the AI service request.\"\"\"\n    data = {}\n    ue_id = input(\"Please input the unique execution ID (ue_id): \")\n    data[\"ue_id\"] = ue_id\n    return data",
        "model_script_content": "# import server utils\nfrom ai_server_utils import (\n    get_image_classification_results_from_model_output_logits,\n    profile_activities,\n    prepare_profile_results,\n)\n# import profile utils\nfrom torch.profiler import profile, record_function\n\n# import necessary libs for AI model inference and request handling\nimport torch\nfrom fastapi import APIRouter, File, Form, UploadFile\nfrom fastapi.responses import JSONResponse\nfrom transformers import SegformerImageProcessor, SegformerForImageClassification\nfrom PIL import Image\n\n# --------------------------------\n# Device configuration\n# --------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# --------------------------------\n# Model-specific configuration\n# make sure the variables `MODEL_NAME` and `model` are defined here.\n# --------------------------------\nMODEL_NAME = \"nvidia/mit-b0\"\nprocessor = SegformerImageProcessor.from_pretrained(MODEL_NAME)\nmodel = SegformerForImageClassification.from_pretrained(MODEL_NAME).to(device)\nmodel.eval()\n\n\n# Initialize the FastAPI router\nrouter = APIRouter()\n\n\n@router.post(\"/run\")\nasync def run_model(file: UploadFile = File(...), ue_id: str = Form(...)):\n    try:\n        # Prepare the model input\n        image = Image.open(file.file).convert(\"RGB\")\n        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n\n        # Perform inference\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        # Process the model outputs\n        predictions = get_image_classification_results_from_model_output_logits(model, outputs.logits)\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"model_results\": predictions,\n            }\n        )\n    except Exception as e:\n        print(f\"Error processing file: {e}\")\n        return JSONResponse(\n            content={\"error\": \"Failed to process the image. {e}\".format(e=str(e))},\n            status_code=500,\n        )\n\n\n@router.post(\"/profile_run\")\nasync def profile_run(file: UploadFile = File(...), ue_id: str = Form(...)):\n    \"\"\"\n    Endpoint to profile the AI model execution.\n    \"\"\"\n    try:\n        # Prepare the model input\n        image = Image.open(file.file).convert(\"RGB\")\n        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n\n        # perform profiling\n        with profile(\n            activities=profile_activities,\n            profile_memory=True,\n        ) as prof:\n            with record_function(\"model_run\"):\n                with torch.no_grad():\n                    model_outputs = model(**inputs)\n\n        profile_result = prepare_profile_results(prof)\n\n        # Process the model outputs\n        predictions = get_image_classification_results_from_model_output_logits(model, model_outputs.logits)\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"profile_result\": profile_result,\n                \"model_results\": predictions,\n            }\n        )\n\n    except Exception as e:\n        print(f\"Error processing request: {e}\")\n        return JSONResponse(\n            content={\"error\": f\"Failed to process the request. {e}\"},\n            status_code=500,\n        )\n\n\n# Below are the model input and output specifications to be used by the `/help` endpoint\nMODEL_INPUT_FORM_SPEC = {\n    \"file\": {\n        \"type\": \"file upload\",\n        \"description\": \"The image file to be classified.\",\n        \"required\": True,\n        \"example\": \"puppy.png\",\n    }\n}\n\nMODEL_OUTPUT_JSON_SPEC = {\n    \"ue_id\": \"unique execution ID\",\n    \"model_results\": [\n        {\n            \"category_id\": \"category id\",\n            \"label\": \"category label\",\n            \"probability\": \"probability value\",\n        }\n    ],\n}",
        "healthcheck_script_content": "import requests\n\nprint(requests.get(\"http://localhost:8000/help\"))",
        "xai_model_script_content": "from functools import partial\nfrom typing import Callable, List, Optional\nfrom fastapi import APIRouter, File, Form, UploadFile\nfrom fastapi.responses import JSONResponse\nfrom PIL import Image\nfrom torch.profiler import profile, record_function\nfrom pytorch_grad_cam import (\n    GradCAM,\n    HiResCAM,\n    AblationCAM,\n    XGradCAM,\n    GradCAMPlusPlus,\n    ScoreCAM,\n    LayerCAM,\n    EigenCAM,\n    EigenGradCAM,\n    KPCA_CAM,\n    RandomCAM,\n)\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\nfrom PIL import Image\nimport numpy as np\nimport torch\nfrom torchvision import transforms\nfrom typing import List, Optional\n\n\n# import model utilities\nfrom ai_server_utils import (\n    encode_image,\n    prepare_profile_results,\n    get_image_classification_results_from_model_output_logits,\n    profile_activities,\n)\n\n# Currently only support GradCAM on image-classification models.\n# so we import the model directly from the model.py file\nfrom model import model, device, processor as resize_and_normalize_processor\n\n\nresize_only_processor = transforms.Compose(\n    [\n        transforms.Resize((512, 512)),\n        transforms.ToTensor(),\n    ],\n)\n\nGRADCAM_METHODS = {\n    \"GradCAM\": GradCAM,\n    \"HiResCAM\": HiResCAM,\n    \"AblationCAM\": AblationCAM,\n    \"XGradCAM\": XGradCAM,\n    \"GradCAMPlusPlus\": GradCAMPlusPlus,\n    \"ScoreCAM\": ScoreCAM,\n    \"LayerCAM\": LayerCAM,\n    \"EigenCAM\": EigenCAM,\n    \"EigenGradCAM\": EigenGradCAM,\n    \"KPCA_CAM\": KPCA_CAM,\n    \"RandomCAM\": RandomCAM,\n}\n\n\nclass HuggingfaceToTensorModelWrapper(torch.nn.Module):\n    \"\"\"Model wrapper to return a tensor\"\"\"\n\n    def __init__(self, model):\n        super(HuggingfaceToTensorModelWrapper, self).__init__()\n        self.model = model\n\n    def forward(self, x):\n        return self.model(x).logits\n\n\ndef get_model_to_tensor_wrapper_class():\n    \"\"\"Helper function to get the model wrapper class.\"\"\"\n    return HuggingfaceToTensorModelWrapper\n\n\ndef get_target_layers_for_grad_cam(model: torch.nn.Module):\n    \"\"\"Helper function to get the target layer for GradCAM.\"\"\"\n    return [model.segformer.encoder.layer_norm[-1]]\n\n\ndef get_classifier_output_target_class():\n    \"\"\"Helper function to get the classifier output target class.\"\"\"\n    return ClassifierOutputTarget\n\n\ndef segformer_reshape_transform_huggingface(tensor, width, height):\n    result = tensor.reshape(tensor.size(0), height, width, tensor.size(2))\n    # Bring the channels to the first dimension,\n    # like in CNNs.\n    result = result.transpose(2, 3).transpose(1, 2)\n    return result\n\n\ndef get_reshape_transform(img_tensor):\n    \"\"\"Helper function to get the reshape transform for GradCAM.\"\"\"\n    return partial(\n        segformer_reshape_transform_huggingface,\n        width=img_tensor.shape[2] // 32,\n        height=img_tensor.shape[1] // 32,\n    )\n\n\ndef run_grad_cam_on_image(\n    model: torch.nn.Module,\n    target_layers: List[torch.nn.Module],\n    targets_for_gradcam: Optional[List[Callable]],\n    reshape_transform: Optional[Callable],\n    input_tensor: torch.Tensor,\n    input_image: torch.Tensor,\n    gradcam_method: Callable,\n):\n    \"\"\"Helper function to run GradCAM on an image and create a visualization.\n    Since the classification target is None, the highest scoring category will be used for every image in the batch.\n    \"\"\"\n\n    with gradcam_method(\n        model=model,\n        target_layers=target_layers,\n        reshape_transform=reshape_transform,\n    ) as cam:\n\n        # Replicate the tensor for each of the categories we want to create Grad-CAM for:\n        repeated_tensor = input_tensor[None, :].repeat(\n            (\n                1\n                if targets_for_gradcam is None or len(targets_for_gradcam) == 0\n                else len(targets_for_gradcam)\n            ),\n            1,\n            1,\n            1,\n        )\n\n        batch_results = cam(\n            input_tensor=repeated_tensor,\n            targets=(\n                None\n                if targets_for_gradcam is None or len(targets_for_gradcam) == 0\n                else targets_for_gradcam\n            ),\n        )\n        results = []\n        for grayscale_cam in batch_results:\n            # adjust the shape of the input_image from (3, 244, 244) to (244, 244, 3)\n            visualization = show_cam_on_image(\n                np.float32(input_image.permute(1, 2, 0).numpy()),\n                grayscale_cam,\n                use_rgb=True,\n            )\n            results.append(visualization)\n        output_image = Image.fromarray(np.hstack(results))\n\n        return output_image, cam.outputs\n\n\n# Initialize the FastAPI router\nrouter = APIRouter()\n\n\n@router.post(\"/run\")\nasync def run_model(\n    file: UploadFile = File(...),\n    ue_id: str = Form(...),\n    gradcam_method_name: str = Form(...),\n    target_category_indexes: Optional[List[int]] = Form(None),\n):\n    \"\"\"\n    Endpoint to run the XAI model.\"\"\"\n\n    try:\n        # Prepare the model input\n        print(\"Preparing the model input...\")\n        image = Image.open(file.file).convert(\"RGB\")\n        normalized_image_tensor = (\n            resize_and_normalize_processor(images=image, return_tensors=\"pt\")[\n                \"pixel_values\"\n            ]\n            .squeeze(0)\n            .to(device)\n        )\n        original_image_tensor = resize_only_processor(image)\n\n        if target_category_indexes is None or len(target_category_indexes) == 0:\n            targets_for_gradcam = None\n        else:\n            # Convert to output target from category indexes\n            targets_for_gradcam = [\n                ClassifierOutputTarget(index) for index in target_category_indexes\n            ]\n        assert (\n            gradcam_method_name in GRADCAM_METHODS\n        ), f\"GradCAM method '{gradcam_method_name}' is not supported. \"\n        gradcam_method = GRADCAM_METHODS[gradcam_method_name]\n\n        model_wrapper_class = get_model_to_tensor_wrapper_class()\n        target_layers = get_target_layers_for_grad_cam(model)\n        reshape_transform = get_reshape_transform(normalized_image_tensor)\n\n        # Perform inference\n        print(\"Running GradCAM...\")\n        xai_image, model_output_logits = run_grad_cam_on_image(\n            model=model_wrapper_class(model),\n            target_layers=target_layers,\n            targets_for_gradcam=targets_for_gradcam,\n            reshape_transform=reshape_transform,\n            input_tensor=normalized_image_tensor,\n            input_image=original_image_tensor,\n            gradcam_method=gradcam_method,\n        )\n\n        predictions = get_image_classification_results_from_model_output_logits(model, model_output_logits)\n\n        return JSONResponse(\n            {\n                \"ue_id\": ue_id,\n                \"xai_results\": {\n                    \"image\": encode_image(xai_image),\n                    \"xai_method\": gradcam_method_name,\n                },\n                \"model_results\": predictions,\n            }\n        )\n\n    except Exception as e:\n        print(f\"Error processing file: {e}\")\n        return JSONResponse(\n            content={\"error\": \"Failed to process the image. {e}\".format(e=str(e))},\n            status_code=500,\n        )\n\n\n@router.post(\"/profile_run\")\nasync def profile_run(\n    file: UploadFile = File(...),\n    ue_id: str = Form(...),\n    gradcam_method_name: str = Form(...),\n    target_category_indexes: Optional[List[int]] = Form(None),\n):\n    \"\"\"\n    Endpoint to profile the XAI run.\n    \"\"\"\n    try:\n        # Prepare the model input\n        image = Image.open(file.file).convert(\"RGB\")\n        normalized_image_tensor = (\n            resize_and_normalize_processor(images=image, return_tensors=\"pt\")[\n                \"pixel_values\"\n            ]\n            .squeeze(0)\n            .to(device)\n        )\n        original_image_tensor = resize_only_processor(image)\n        if target_category_indexes is None or len(target_category_indexes) == 0:\n            targets_for_gradcam = None\n        else:\n            # Convert to output target from category indexes\n            targets_for_gradcam = [\n                ClassifierOutputTarget(index) for index in target_category_indexes\n            ]\n\n        assert (\n            gradcam_method_name in GRADCAM_METHODS\n        ), f\"GradCAM method '{gradcam_method_name}' is not supported. \"\n        gradcam_method = GRADCAM_METHODS[gradcam_method_name]\n\n        model_wrapper_class = get_model_to_tensor_wrapper_class()\n        target_layers = get_target_layers_for_grad_cam(model)\n        reshape_transform = get_reshape_transform(normalized_image_tensor)\n\n        # perform profiling\n        with profile(\n            activities=profile_activities,\n            profile_memory=True,\n        ) as prof:\n            with record_function(\"xai_model_run\"):\n\n                # Perform inference\n                xai_image, model_output_logits = run_grad_cam_on_image(\n                    model=model_wrapper_class(model),\n                    target_layers=target_layers,\n                    targets_for_gradcam=targets_for_gradcam,\n                    reshape_transform=reshape_transform,\n                    input_tensor=normalized_image_tensor,\n                    input_image=original_image_tensor,\n                    gradcam_method=gradcam_method,\n                )\n\n        return JSONResponse(\n            {\n                \"ue_id\": ue_id,\n                \"xai_results\": {\n                    \"image\": encode_image(xai_image),\n                    \"xai_method\": gradcam_method_name,\n                },\n                \"model_results\": get_image_classification_results_from_model_output_logits(\n                    model, model_output_logits\n                ),\n                \"profile_result\": prepare_profile_results(prof),\n            }\n        )\n\n    except Exception as e:\n        print(f\"Error processing request: {e}\")\n        return JSONResponse(\n            content={\"error\": f\"Failed to process the request. {e}\"},\n            status_code=500,\n        )\n\n\nXAI_OUTPUT_JSON_SPEC = {\n    \"xai_results\": {\n        \"image\": \"XAI image result\",\n        \"xai_method\": \"XAI method used\",\n    }\n}\n"
    }
}