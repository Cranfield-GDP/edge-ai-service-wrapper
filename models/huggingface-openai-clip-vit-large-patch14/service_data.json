{
    "model_name": "openai/clip-vit-large-patch14",
    "model_url": "https://huggingface.co/openai/clip-vit-large-patch14",
    "task": "zero-shot-image-classification",
    "task_detail": "The pre-trained AI model, openai/clip-vit-large-patch14, is developed primarily for robust computer vision tasks, specializing in zero-shot image classification by leveraging image-text similarity. The model, released in January 2021, follows a Vision Transformer (ViT) architecture with a large patch size of 14 and employs a masked self-attention transformer as a text encoder. Both the image and text encoders are trained using a contrastive loss to align image and text representation in a shared embedding space, maximizing their similarity.\n\n### Functionality Overview\n\nThe core functionality of the openai/clip-vit-large-patch14 model is to compute the similarity between images and textual descriptions, making it highly effective for zero-shot learning tasks where it generalizes across various image classification scenarios without additional fine-tuning. This involves the comparison of image features to textual prompts, allowing users to determine the most likely textual descriptors for a given image or vice versa.\n\n### Expected Inputs\n\n- **Image Input:** The model accepts images as input, which should be pre-processed and resized according to the required dimensions to fit into the Vision Transformer architecture. The input image format is flexible as long as it can be processed into a tensor compatible with the model.\n- **Text Input:** Textual descriptions or labels should be provided in natural language sentences or phrases. These are processed using the text encoder to match semantic representation with the visual input.\n\n### Expected Outputs\n\n- **Image-Text Similarity Scores:** The model outputs a similarity score for each given (image, text) pair, indicating how likely the corresponding text describes the image. A higher score represents a stronger match between the image and its textual descriptor.\n- **Label Probabilities:** From the similarity scores, probability distributions over the provided text labels can be obtained, identifying the most probable labels for describing the image.\n\n### Potential Use Case Scenarios\n\n1. **Zero-Shot Image Classification:** Use the model to classify images into categories defined by textual descriptions without the need for task-specific training data.\n2. **Image Retrieval:** Implement image search systems where queries are expressed in natural language, leveraging the model\u2019s ability to align textual queries with visual content.\n3. **Cross-Modal Retrieval:** Facilitate multi-modal applications by enabling the retrieval of text data using visual queries or vice versa, promoting seamless integration of image and text modalities.\n4. **Research in Generalization and Robustness:** Employ the model in academic settings to explore model generalization across diverse image datasets and its robustness against classification challenges.\n\n### Limitations and Considerations\n\n- The model should not be used in unsupervised commercial deployments, as extensive domain-specific evaluation is necessary to ensure reliable performance.\n- CLIP displays varied performance based on classification tasks and may be subject to biases related to race, gender, and age. Thus, careful handling and consideration of class categories are required in sensitive applications.\n",
    "accuracy_info": "The README provides information about the model's accuracy in certain evaluations. Here is the extracted description:\n\n\"We found accuracy >96% across all races for gender classification with \u2018Middle Eastern\u2019 having the highest accuracy (98.4%) and \u2018White\u2019 having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification.\"",
    "image_repository_url": "docker.io/cranfield6g/cranfield-edge-openai-clip-vit-large-patch14",
    "service_disk_size_bytes": 3663934755,
    "profiles": [
        {
            "node_id": "LAP004262",
            "device_type": "DeviceType.CPU",
            "device_name": "None",
            "initialization_time_ms": 49941.17712974548,
            "eviction_time_ms": 0,
            "initialization_cost": 0,
            "keep_alive_cost": 0,
            "energy_consumption_idle": 0,
            "inference": {
                "cpu_time_ms": 794.3362480000001,
                "device_time_ms": 0.0,
                "cpu_memory_usage_MB": 1.0576248168945312,
                "self_cpu_memory_usage_MB": -641.5301094055176,
                "device_memory_usage_MB": 0.0,
                "self_device_memory_usage_MB": 0.0,
                "energy_consumption_execution": 0,
                "disk_IO_MB": 0,
                "input_data_MB": 0,
                "output_data_MB": 0,
                "execution_time_ms": 1150.3388086954753,
                "execution_cost": 0
            },
            "idle_container_cpu_memory_usage": "1GB",
            "idle_container_device_memory_usage": "0GB"
        }
    ],
    "feedback": {
        "likes": [],
        "dislikes": [],
        "comments": []
    },
    "code": {
        "readme_content": "---\ntags:\n- vision\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/cat-dog-music.png\n  candidate_labels: playing music, playing sports\n  example_title: Cat & Dog\n---\n\n# Model Card: CLIP\n\nDisclaimer: The model card is taken and modified from the official CLIP repository, it can be found [here](https://github.com/openai/CLIP/blob/main/model-card.md).\n\n## Model Details\n\nThe CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they\u2019re being deployed within.\n\n### Model Date\n\nJanuary 2021\n\n### Model Type\n\nThe base model uses a ViT-L/14 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss.\n\nThe original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.\n\n\n### Documents\n\n- [Blog Post](https://openai.com/blog/clip/)\n- [CLIP Paper](https://arxiv.org/abs/2103.00020)\n\n\n### Use with Transformers\n\n```python\nfrom PIL import Image\nimport requests\n\nfrom transformers import CLIPProcessor, CLIPModel\n\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n```\n\n\n## Model Use\n\n### Intended Use\n\nThe model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.\n\n#### Primary intended uses\n\nThe primary intended users of these models are AI researchers.\n\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\n\n### Out-of-Scope Use Cases\n\n**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP\u2019s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. \n\nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\n\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.\n\n\n\n## Data\n\nThe model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users.\n\n### Data Mission Statement\n\nOur goal with building this dataset was to test out robustness and generalizability in computer vision tasks. As a result, the focus was on gathering large quantities of data from different publicly-available internet data sources. The data was gathered in a mostly non-interventionist manner. However, we only crawled websites that had policies against excessively violent and adult images and allowed us to filter out such content. We do not intend for this dataset to be used as the basis for any commercial or deployed model and will not be releasing the dataset.\n\n\n\n## Performance and Limitations\n\n### Performance\n\nWe have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:\n\n- Food101\n- CIFAR10   \n- CIFAR100   \n- Birdsnap\n- SUN397\n- Stanford Cars\n- FGVC Aircraft\n- VOC2007\n- DTD\n- Oxford-IIIT Pet dataset\n- Caltech101\n- Flowers102\n- MNIST   \n- SVHN \n- IIIT5K   \n- Hateful Memes   \n- SST-2\n- UCF101\n- Kinetics700\n- Country211\n- CLEVR Counting\n- KITTI Distance\n- STL-10\n- RareAct\n- Flickr30\n- MSCOCO\n- ImageNet\n- ImageNet-A\n- ImageNet-R\n- ImageNet Sketch\n- ObjectNet (ImageNet Overlap)\n- Youtube-BB\n- ImageNet-Vid\n\n## Limitations\n\nCLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance.\n\n### Bias and Fairness\n\nWe find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface](https://arxiv.org/abs/1908.04913) into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).\n\nWe also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy >96% across all races for gender classification with \u2018Middle Eastern\u2019 having the highest accuracy (98.4%) and \u2018White\u2019 having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement/enthusiasm for such tasks.\n\n\n\n## Feedback\n\n### Where to send questions or comments about the model\n\nPlease use [this Google Form](https://forms.gle/Uv7afRH5dvY34ZEs9)",
        "dockerfile_content": "FROM python3.12_ai_service_base:latest\n\n# Set working directory\nWORKDIR /app\n\n# Copy application code\nCOPY . .\n\n# Install additional dependencies\nRUN pip install datasets transformers Pillow requests\n\n# Expose port 8000\nEXPOSE 8000\n\n# Start the FastAPI server\nCMD [\"uvicorn\", \"ai_server:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--timeout-keep-alive\", \"600\"]",
        "ai_server_script_content": "import json\nimport os\nimport time\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import JSONResponse\nfrom contextlib import asynccontextmanager\nfrom ai_server_utils import (\n    PROFILE_OUTPUT_JSON_SPEC,\n    NODE_ID,\n    K8S_POD_NAME,\n)\n\n\n# -------------------------------------------\n# App Lifespan setup\n# -------------------------------------------\n# Record the script start time (when uvicorn starts the process)\nSCRIPT_START_TIME = time.time()\nINITIALIZATION_DURATION = 0.0\nservice_endpoint_specs = {\n    \"model_input_form_spec\": None,\n    \"model_output_json_spec\": None,\n    \"profile_output_json_spec\": None,\n    \"xai_model_input_form_spec\": None,\n    \"xai_model_output_json_spec\": None,\n    \"xai_profile_output_json_spec\": None,\n}\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n\n    global INITIALIZATION_DURATION\n    global SCRIPT_START_TIME\n    global service_endpoint_specs\n\n    # Load the AI model\n    print(\"Loading AI model...\")\n    from model import (\n        MODEL_INPUT_FORM_SPEC,\n        MODEL_OUTPUT_JSON_SPEC,\n        router as model_router,\n    )\n\n    service_endpoint_specs[\"model_input_form_spec\"] = MODEL_INPUT_FORM_SPEC\n    service_endpoint_specs[\"model_output_json_spec\"] = MODEL_OUTPUT_JSON_SPEC\n    service_endpoint_specs[\"profile_output_json_spec\"] = PROFILE_OUTPUT_JSON_SPEC\n\n    app.include_router(model_router, prefix=\"/model\", tags=[\"AI Model\"])\n\n    # Load the XAI model\n    if os.path.exists(os.path.dirname(__file__) + \"/xai_model.py\"):\n        print(\"Loading XAI model...\")\n        from xai_model import (\n            XAI_OUTPUT_JSON_SPEC,\n            router as xai_model_router,\n        )\n\n        # by default, the xai_model input form spec is the same as the model input form spec\n        service_endpoint_specs[\"xai_model_input_form_spec\"] = MODEL_INPUT_FORM_SPEC\n        service_endpoint_specs[\"xai_model_output_json_spec\"] = MODEL_OUTPUT_JSON_SPEC\n        service_endpoint_specs[\"xai_model_output_json_spec\"].update(\n            XAI_OUTPUT_JSON_SPEC\n        )\n        service_endpoint_specs[\"xai_profile_output_json_spec\"] = (\n            PROFILE_OUTPUT_JSON_SPEC\n        )\n        service_endpoint_specs[\"xai_profile_output_json_spec\"].update(\n            XAI_OUTPUT_JSON_SPEC\n        )\n\n        app.include_router(xai_model_router, prefix=\"/xai_model\", tags=[\"XAI Model\"])\n\n    # Record the initialization duration\n    INITIALIZATION_DURATION = time.time() - SCRIPT_START_TIME\n\n    print(f\"AI service loaded in {INITIALIZATION_DURATION:.2f} seconds.\")\n\n    yield\n\n    # Clean up the models and release the resources\n    service_endpoint_specs.clear()\n\n\n# -------------------------------------------\n# FastAPI application setup\n# -------------------------------------------\napp = FastAPI(lifespan=lifespan)\n\n\n# -------------------------------------------\n# Middlewares\n# -------------------------------------------\n@app.middleware(\"http\")\nasync def prepare_header_middleware(request: Request, call_next):\n    start_time = time.perf_counter()\n    response = await call_next(request)\n    process_time = time.perf_counter() - start_time\n    response.headers[\"X-Process-Time\"] = str(process_time)\n    response.headers[\"X-NODE-ID\"] = NODE_ID\n    response.headers[\"X-K8S-POD-NAME\"] = K8S_POD_NAME\n    return response\n\n\n# -------------------------------------------\n# General Endpoints\n# -------------------------------------------\n@app.get(\"/initialization_duration\")\ndef get_initialization_duration():\n    \"\"\"\n    Endpoint to retrieve the initialization duration of the AI model.\n    \"\"\"\n    global INITIALIZATION_DURATION\n\n    if INITIALIZATION_DURATION == 0.0:\n        return JSONResponse(\n            content={\"error\": \"Model not initialized.\"},\n            status_code=500,\n        )\n    return JSONResponse(\n        content={\n            \"initialization_duration\": INITIALIZATION_DURATION,\n            \"script_start_time\": SCRIPT_START_TIME,\n        }\n    )\n\n\n@app.get(\"/help\")\ndef get_help():\n    global service_endpoint_specs\n    help_info = {\n        \"endpoints\": {\n            \"/model/run\": {\n                \"method\": \"POST\",\n                \"description\": \"Executes the AI model with the provided input data.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_output_json_spec\"],\n                },\n            },\n            \"/model/profile_run\": {\n                \"method\": \"POST\",\n                \"description\": \"Profiles the AI model execution.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **service_endpoint_specs[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    \"profile_result\": \"Profiling results of the AI model execution.\",\n                    **service_endpoint_specs[\"profile_output_json_spec\"],\n                },\n            },\n            \"/initialization_duration\": {\n                \"method\": \"GET\",\n                \"description\": \"Retrieves the initialization duration of the AI model.\",\n                \"response\": {\n                    \"initialization_duration\": \"Time taken to initialize the model (in seconds).\",\n                    \"script_start_time\": \"Timestamp when the script started (in seconds since epoch).\",\n                },\n            },\n        }\n    }\n\n    if service_endpoint_specs[\"xai_model_input_form_spec\"] is not None:\n        help_info[\"endpoints\"][\"/xai_model/run\"] = {\n            \"method\": \"POST\",\n            \"description\": \"Executes the XAI model with the provided input data.\",\n            \"parameters\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_input_form_spec\"],\n            },\n            \"response\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_output_json_spec\"],\n            },\n        }\n\n        help_info[\"endpoints\"][\"/xai_model/profile_run\"] = {\n            \"method\": \"POST\",\n            \"description\": \"Profiles the XAI model execution.\",\n            \"parameters\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                **service_endpoint_specs[\"xai_model_input_form_spec\"],\n            },\n            \"response\": {\n                \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                \"profile_result\": \"Profiling results of the XAI model execution.\",\n                **service_endpoint_specs[\"xai_profile_output_json_spec\"],\n            },\n        }\n    \n    return JSONResponse(content=help_info)\n",
        "ai_server_utils_script_content": "import os\nimport socket\nimport torch\nfrom io import BytesIO\nimport base64\n\nfrom torch.profiler import profile, ProfilerActivity, record_function\n\n\n# -------------------------------------------\n# ENV Variables\n# -------------------------------------------\nNODE_ID = os.getenv(\"NODE_ID\", socket.gethostname())\nK8S_POD_NAME = os.getenv(\"K8S_POD_NAME\", \"UNKNOWN\")\n\n\n# -------------------------------------------\n# Profile Utils\n# -------------------------------------------\nprofile_activities = [\n    ProfilerActivity.CPU,\n    ProfilerActivity.CUDA,\n    ProfilerActivity.MTIA,\n    ProfilerActivity.XPU,\n]\n\ndef get_image_classification_results_from_model_output_logits(model, model_output_logits):\n    \"\"\"\n    Process the model outputs to prepare for the response.\n    \"\"\"\n    probabilities = torch.nn.functional.softmax(model_output_logits[0], dim=0)\n\n    # Return the top 5 predictions with labels\n    top5_prob, top5_catid = torch.topk(probabilities, 5)\n    predictions = []\n    for i in range(top5_prob.size(0)):\n        category_id = top5_catid[i].item()\n        predictions.append(\n            {\n                \"category_id\": category_id,\n                \"label\": model.config.id2label[category_id],\n                \"probability\": top5_prob[i].item(),\n            }\n        )\n    return predictions\n\ndef prepare_profile_results(prof):\n    \"\"\"\n    Prepare the profile results for the model inputs and outputs.\n    \"\"\"\n    print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n\n    profile_event = prof.key_averages()[0]\n\n    profile_result = {\n        \"name\": profile_event.key,\n        \"device_type\": str(profile_event.device_type),\n        \"device_name\": str(profile_event.use_device),\n        \"cpu_memory_usage\": profile_event.cpu_memory_usage,\n        \"self_cpu_memory_usage\": profile_event.self_cpu_memory_usage,\n        \"device_memory_usage\": profile_event.device_memory_usage,\n        \"self_device_memory_usage\": profile_event.self_device_memory_usage,\n        \"cpu_time_total\": profile_event.cpu_time_total,\n        \"self_cpu_time_total\": profile_event.self_cpu_time_total,\n        \"device_time_total\": profile_event.device_time_total,\n        \"self_device_time_total\": profile_event.self_device_time_total,\n    }\n    return profile_result\n\n\ndef encode_image(image):\n    \"\"\"\n    Encode the image to bytes\n    \"\"\"\n    buffered = BytesIO()\n    image.save(buffered, format=\"PNG\")\n    encoded_image = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n    return encoded_image\n\n\nPROFILE_OUTPUT_JSON_SPEC = {\n    \"ue_id\": \"unique execution ID\",\n    \"profile_result\": {\n        \"name\": \"name of the profile event\",\n        \"device_type\": \"type of device used (e.g., CPU, GPU, ...)\",\n        \"device_name\": \"name of the device used\",\n        \"cpu_memory_usage\": \"CPU memory usage in bytes\",\n        \"self_cpu_memory_usage\": \"self CPU memory usage in bytes\",\n        \"device_memory_usage\": \"device memory usage in bytes\",\n        \"self_device_memory_usage\": \"self device memory usage in bytes\",\n        \"cpu_time_total\": \"total CPU time in microseconds\",\n        \"self_cpu_time_total\": \"self total CPU time in microseconds\",\n        \"device_time_total\": \"total device time in microseconds\",\n        \"self_device_time_total\": \"self total device time in microseconds\",\n    },\n    \"model_results\": \"the AI service model results\",\n}\n",
        "ai_client_script_content": "import base64\nimport json\nimport time\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nfrom ai_client_utils import (\n    prepare_ai_service_request_files,\n    prepare_ai_service_request_data,\n)\n\nXAI_GRADCAM_METHODS = [\n    \"GradCAM\",\n    \"HiResCAM\",\n    \"AblationCAM\",\n    \"XGradCAM\",\n    \"GradCAMPlusPlus\",\n    \"ScoreCAM\",\n    \"LayerCAM\",\n    \"EigenCAM\",\n    \"EigenGradCAM\",\n    \"KPCA_CAM\",\n    \"RandomCAM\",\n]\n\n# -------------------------------------\n# prompt for necessary inputs\n# -------------------------------------\nSERVER_URL = input(\"Please input server URL (default to http://localhost:9000): \")\nif SERVER_URL.strip() == \"\":\n    SERVER_URL = \"http://localhost:9000\"\nUE_ID = input(\"Please input UE_ID (default to 123456): \")\nif UE_ID.strip() == \"\":\n    UE_ID = \"123456\"\n\n\ndef send_post_request(url, data, files):\n    \"\"\"Send request to run AI service and display AI service responses.\"\"\"\n    try:\n        response = requests.post(url, files=files, data=data)\n        # get the process time, node id and k8s pod name from the response headers\n        process_time = response.headers.get(\"X-Process-Time\")\n        node_id = response.headers.get(\"X-NODE-ID\")\n        k8s_pod_name = response.headers.get(\"X-K8S-POD-NAME\")\n        if response.status_code == 200:\n            return response.json(), process_time, node_id, k8s_pod_name\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n\ndef send_get_request(url, params=None):\n    \"\"\"Send GET request to the specified URL and return the response.\"\"\"\n    try:\n        response = requests.get(url, params=params)\n        process_time = response.headers.get(\"X-Process-Time\")\n        node_id = response.headers.get(\"X-NODE-ID\")\n        k8s_pod_name = response.headers.get(\"X-K8S-POD-NAME\")\n        if response.status_code == 200:\n            return response.json(), process_time, node_id, k8s_pod_name\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n            return None\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n\nclass ProfileResultProcessor:\n\n    def __init__(self, server_url):\n        self.server_url = server_url\n        self.start_time = time.time()\n        self.service_initialization_duration = 0\n        self.response_counter = 0\n        self.profile_name = None\n        self.device_type = None\n        self.device_name = None\n        self.node_id = None\n        self.k8s_pod_name = None\n        self.cpu_memory_usage_bytes = 0\n        self.self_cpu_memory_usage_bytes = 0\n        self.device_memory_usage_bytes = 0\n        self.self_device_memory_usage_bytes = 0\n        self.cpu_time_total_us = 0\n        self.self_cpu_time_total_us = 0\n        self.device_time_total_us = 0\n        self.self_device_time_total_us = 0\n\n        # xai related\n        self.gradcam_method_name = None\n\n        self.fetch_service_initialization_duration()\n\n    def fetch_service_initialization_duration(self):\n        \"\"\"Fetch the service initialization duration from the server.\"\"\"\n        response, process_time, node_id, k8s_pod_name = send_get_request(\n            f\"{self.server_url}/initialization_duration\"\n        )\n        if response:\n            self.service_initialization_duration = response.get(\n                \"initialization_duration\", 0\n            )\n        else:\n            print(\"Failed to fetch initialization duration.\")\n            self.service_initialization_duration = 0\n\n    def process_new_response(\n        self,\n        profile_response,\n        process_time=None,\n        node_id=None,\n        k8s_pod_name=None,\n        gradcam_method_name=None,\n    ):\n        if not profile_response:\n            return\n\n        profile_result = profile_response[\"profile_result\"]\n\n        if not profile_result:\n            return\n\n        if self.profile_name is None:\n            self.profile_name = profile_result[\"name\"]\n        if self.device_type is None:\n            self.device_type = profile_result[\"device_type\"]\n        if self.device_name is None:\n            self.device_name = profile_result[\"device_name\"]\n        if self.node_id is None:\n            self.node_id = node_id\n        if self.k8s_pod_name is None:\n            self.k8s_pod_name = k8s_pod_name\n\n        if self.gradcam_method_name is None:\n            self.gradcam_method_name = gradcam_method_name\n\n        # update the max profile result\n        self.cpu_memory_usage_bytes = max(\n            self.cpu_memory_usage_bytes, profile_result[\"cpu_memory_usage\"]\n        )\n        # self cpu memory usage could be negative. here we take the value that has the max absolute value\n        if abs(profile_result[\"self_cpu_memory_usage\"]) > abs( self.self_cpu_memory_usage_bytes):\n            self.self_cpu_memory_usage_bytes = profile_result[\"self_cpu_memory_usage\"]\n        self.device_memory_usage_bytes = max(\n            self.device_memory_usage_bytes, profile_result[\"device_memory_usage\"]\n        )\n        # same as self cpu memory usage\n        if abs(profile_result[\"self_device_memory_usage\"]) > abs(self.self_device_memory_usage_bytes):\n            self.self_device_memory_usage_bytes = profile_result[\"self_device_memory_usage\"]\n        self.cpu_time_total_us = max(\n            self.cpu_time_total_us, profile_result[\"cpu_time_total\"]\n        )\n        self.self_cpu_time_total_us = max(\n            self.self_cpu_time_total_us, profile_result[\"self_cpu_time_total\"]\n        )\n        self.device_time_total_us = max(\n            self.device_time_total_us, profile_result[\"device_time_total\"]\n        )\n        self.self_device_time_total_us = max(\n            self.self_device_time_total_us, profile_result[\"self_device_time_total\"]\n        )\n\n        self.response_counter += 1\n\n    def complete_profile(self):\n        print(\"\\n--------- PROFILE EVENT ---------\\n\")\n        print(f\"Name: {self.profile_name}\")\n        print(f\"Device Type: {self.device_type}\")\n        print(f\"Device Name: {self.device_name}\")\n        print(f\"Node ID: {self.node_id}\")\n        print(f\"K8S_POD_NAME: {self.k8s_pod_name}\")\n\n        if self.gradcam_method_name:\n            print(f\"GradCAM Method Name: {self.gradcam_method_name}\")\n\n        print(\"\\n--------- LATENCY RESULT ---------\\n\")\n        print(\"Service Initialization Duration: \", self.service_initialization_duration)\n        print(\"Total Requests: \", self.response_counter)\n        print(f\"Total Time Taken: {time.time() - self.start_time:.2f} seconds\")\n        print(\n            f\"Average Time Taken: {(time.time() - self.start_time) / self.response_counter:.2f} seconds\"\n        )\n\n        print(\"\\n--------- RESOURCE USAGE ---------\\n\")\n        print(f\"CPU Memory Usage: {self.cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\")\n        print(\n            f\"Self CPU Memory Usage: {self.self_cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Device Memory Usage: {self.device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Self Device Memory Usage: {self.self_device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(f\"CPU Time Total: {self.cpu_time_total_us / 1000:.2f} ms\")\n        print(f\"Self CPU Time Total: {self.self_cpu_time_total_us / 1000:.2f} ms\")\n        print(f\"Device Time Total: {self.device_time_total_us / 1000:.2f} ms\")\n        print(f\"Self Device Time Total: {self.self_device_time_total_us / 1000:.2f} ms\")\n\n        # update the service_data.json automatically\n        with open(\"service_data.json\", \"r\") as f:\n            service_data = json.load(f)\n\n        if not self.gradcam_method_name:\n            complete_profile_data_to_save = {\n                \"node_id\": self.node_id,\n                \"device_type\": self.device_type,\n                \"device_name\": self.device_name,\n                \"initialization_time_ms\": self.service_initialization_duration * 1000,\n                \"eviction_time_ms\": 0,\n                \"initialization_cost\": 0,\n                \"keep_alive_cost\": 0,\n                \"energy_consumption_idle\": 0,\n                \"inference\": {\n                    \"cpu_time_ms\": self.cpu_time_total_us / 1000,\n                    \"device_time_ms\": self.device_time_total_us / 1000,\n                    \"cpu_memory_usage_MB\": self.cpu_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"self_cpu_memory_usage_MB\": self.self_cpu_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"device_memory_usage_MB\": self.device_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"self_device_memory_usage_MB\": self.self_device_memory_usage_bytes\n                    / (1024 * 1024),\n                    \"energy_consumption_execution\": 0,\n                    \"disk_IO_MB\": 0,\n                    \"input_data_MB\": 0,\n                    \"output_data_MB\": 0,\n                    \"execution_time_ms\": (time.time() - self.start_time)\n                    / self.response_counter\n                    * 1000,\n                    \"execution_cost\": 0,\n                },\n            }\n\n            # check if there is already a profile for this node id\n            profile_found = False\n            for profile in service_data[\"profiles\"]:\n                if profile[\"node_id\"] == self.node_id:\n                    profile[\"inference\"] = complete_profile_data_to_save[\"inference\"]\n                    profile_found = True\n                    break\n            if not profile_found:\n                service_data[\"profiles\"].append(complete_profile_data_to_save)\n\n            # save the updated service_data.json\n            with open(\"service_data.json\", \"w\") as f:\n                json.dump(service_data, f, indent=4)\n            print(\"\\n--------- SERVICE DATA UPDATED ---------\\n\")\n\n        else:\n            complete_xai_profile_data_to_save = {\n                \"node_id\": self.node_id,\n                \"device_type\": self.device_type,\n                \"device_name\": self.device_name,\n                \"initialization_time_ms\": self.service_initialization_duration * 1000,\n                \"eviction_time_ms\": 0,\n                \"initialization_cost\": 0,\n                \"keep_alive_cost\": 0,\n                \"energy_consumption_idle\": 0,\n                \"xai\": [\n                    {\n                        \"xai_method\": self.gradcam_method_name,\n                        \"cpu_time_ms\": self.cpu_time_total_us / 1000,\n                        \"device_time_ms\": self.device_time_total_us / 1000,\n                        \"cpu_memory_usage_MB\": self.cpu_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"self_cpu_memory_usage_MB\": self.self_cpu_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"device_memory_usage_MB\": self.device_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"self_device_memory_usage_MB\": self.self_device_memory_usage_bytes\n                        / (1024 * 1024),\n                        \"energy_consumption_execution\": 0,\n                        \"disk_IO_MB\": 0,\n                        \"input_data_MB\": 0,\n                        \"output_data_MB\": 0,\n                        \"execution_time_ms\": (time.time() - self.start_time)\n                        / self.response_counter\n                        * 1000,\n                        \"execution_cost\": 0,\n                    }\n                ],\n            }\n\n            # check if there is already a profile for this node id\n            profile_found = False\n            for profile in service_data[\"profiles\"]:\n                if profile[\"node_id\"] == self.node_id:\n                    profile_found = True\n\n                    # check if there is already a profile for this xai method\n                    xai_method_found = False\n                    if not profile.get(\"xai\"):\n                        profile[\"xai\"] = []\n                    for xai_profile in profile[\"xai\"]:\n                        if xai_profile[\"xai_method\"] == self.gradcam_method_name:\n                            xai_profile.update(complete_xai_profile_data_to_save[\"xai\"][0]) \n                            xai_method_found = True\n                            break\n                    if not xai_method_found:\n                        profile[\"xai\"].append(complete_xai_profile_data_to_save[\"xai\"][0])\n                    break\n\n            if not profile_found:\n                service_data[\"profiles\"].append(complete_xai_profile_data_to_save)\n\n            # save the updated service_data.json\n            with open(\"service_data.json\", \"w\") as f:\n                json.dump(service_data, f, indent=4)\n            print(\"\\n--------- SERVICE DATA UPDATED ---------\\n\")\n\n\ndef option_run():\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n    data = {**data, \"ue_id\": UE_ID}\n    response, process_time, node_id, pod_name = send_post_request(\n        f\"{SERVER_URL}/model/run\", data, files\n    )\n    print(\"Process Time: \", process_time)\n    print(\"Node ID: \", node_id)\n    print(\"K8S_POD_NAME: \", pod_name)\n    print(\"Response\")\n    print(json.dumps(response, indent=4))\n\n\ndef option_profile_run():\n    data = prepare_ai_service_request_data()\n    data = {**data, \"ue_id\": UE_ID}\n    files = prepare_ai_service_request_files()\n    num_requests = int(input(\"Enter the number of requests to send: \"))\n\n    try:\n        profile_result_processor = ProfileResultProcessor(SERVER_URL)\n\n        for _ in range(num_requests):\n            profile_response, process_time, node_id, k8s_node_name = send_post_request(\n                f\"{SERVER_URL}/model/profile_run\", data, files\n            )\n            print(\"Process Time: \", process_time)\n            print(\"Node ID: \", node_id)\n            print(\"K8S_POD_NAME: \", k8s_node_name)\n            print(\"Response\")\n            print(json.dumps(profile_response, indent=4))\n            if not profile_response:\n                print(\"No profile response received.\")\n                continue\n\n            profile_result_processor.process_new_response(\n                profile_response,\n                process_time=process_time,\n                node_id=node_id,\n                k8s_pod_name=k8s_node_name,\n            )\n\n        # Print the final profile result and update the service_data.json\n        profile_result_processor.complete_profile()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\n\ndef option_help():\n    \"\"\"Get help information from the server.\"\"\"\n    try:\n        response, process_time, node_id, pod_name = send_get_request(\n            f\"{SERVER_URL}/help\"\n        )\n        print(\"Process Time: \", process_time)\n        print(\"Node ID: \", node_id)\n        print(\"K8S_POD_NAME: \", pod_name)\n        print(\"Response\")\n        print(json.dumps(response, indent=4))\n    except Exception as e:\n        print(f\"Request failed: {e}\")\n\n\ndef option_run_with_xai():\n    \"\"\"Run AI service with XAI.\"\"\"\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n\n    print(\n        \"Note that currently only GradCAM methods on image-classification models are supported.\"\n    )\n    while True:\n        gradcam_method_name = input(\n            f\"Please select a GradCAM method (options: {XAI_GRADCAM_METHODS}): \"\n        )\n        if gradcam_method_name not in XAI_GRADCAM_METHODS:\n            print(f\"Invalid GradCAM method. Please select again.\")\n        else:\n            break\n\n    # ask for target class for explanation\n    target_category_indexes = input(\n        \"Please input target category indexes for explanation (comma-separated, e.g., 111, 32, 44, ...): \"\n    )\n    if not target_category_indexes or not target_category_indexes.strip():\n        print(\n            \"No target category indexes provided. Defaulting to explaining the top confident category.\"\n        )\n        target_category_indexes = []\n    else:\n        target_category_indexes = [\n            int(i.strip()) for i in target_category_indexes.split(\",\")\n        ]\n\n    data = {\n        **data,\n        \"ue_id\": UE_ID,\n        \"gradcam_method_name\": gradcam_method_name,\n        \"target_category_indexes\": target_category_indexes,\n    }\n    print(\"Data: \", data)\n    response, process_time, node_id, k8s_pod_name = send_post_request(\n        f\"{SERVER_URL}/xai_model/run\", data, files\n    )\n    print(\"Process Time: \", process_time)\n    print(\"Node ID: \", node_id)\n    print(\"K8S_POD_NAME: \", k8s_pod_name)\n\n    # Handle JSON response\n    model_results = response.get(\"model_results\")\n    if model_results:\n        print(\"Model Results:\", json.dumps(model_results, indent=4))\n\n    xai_results = response.get(\"xai_results\")\n    if xai_results:\n        print(\"XAI Results Method:\", xai_results.get(\"xai_method\"))\n\n    # Handle binary image response\n    encoded_image = xai_results.get(\"image\")\n    if encoded_image:\n        image_bytes = base64.b64decode(encoded_image)\n\n        # Load the image into Pillow\n        image = Image.open(BytesIO(image_bytes))\n\n        # Save image to disk\n        image.save(\"xai_output.png\")\n\n        # Display the image using matplotlib\n        plt.imshow(image)\n        plt.axis(\"off\")\n        plt.show()\n\n\ndef option_profile_run_with_xai():\n    \"\"\"Run AI service with XAI and profile the run.\"\"\"\n    data = prepare_ai_service_request_data()\n    files = prepare_ai_service_request_files()\n    print(\n        \"Note that currently only GradCAM methods on image-classification models are supported.\"\n    )\n\n    # ask for target class for explanation\n    target_category_indexes = input(\n        \"Please input target category indexes for explanation (comma-separated, e.g., 111, 32, 44, ...): \"\n    )\n    if not target_category_indexes or not target_category_indexes.strip():\n        print(\n            \"No target category indexes provided. Defaulting to explaining the top confident category.\"\n        )\n        target_category_indexes = []\n    else:\n        target_category_indexes = [\n            int(i.strip()) for i in target_category_indexes.split(\",\")\n        ]\n\n    num_requests = int(input(\"Enter the number of requests to send: \"))\n\n    try:\n        for gradcam_method_name in XAI_GRADCAM_METHODS:\n\n            data = {\n                **data,\n                \"ue_id\": UE_ID,\n                \"gradcam_method_name\": gradcam_method_name,\n                \"target_category_indexes\": target_category_indexes,\n            }\n            print(\"Data: \", data)\n\n            profile_result_processor = ProfileResultProcessor(SERVER_URL)\n\n            for _ in range(num_requests):\n                response, process_time, node_id, k8s_pod_name = send_post_request(\n                    f\"{SERVER_URL}/xai_model/profile_run\", data, files\n                )\n                print(\"Process Time: \", process_time)\n                print(\"Node ID: \", node_id)\n                print(\"K8S_POD_NAME: \", k8s_pod_name)\n                if not response:\n                    print(\"No profile response received.\")\n                    continue\n\n                # Handle JSON response\n                model_results = response.get(\"model_results\")\n                if model_results:\n                    print(\"Model Results:\", json.dumps(model_results, indent=4))\n\n                profile_result_processor.process_new_response(\n                    response,\n                    process_time=process_time,\n                    node_id=node_id,\n                    k8s_pod_name=k8s_pod_name,\n                    gradcam_method_name=gradcam_method_name,\n                )\n\n            # Print the final profile result and update the service_data.json\n            profile_result_processor.complete_profile()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\n\nOPTIONS = [\n    {\n        \"label\": \"Get help information\",\n        \"action\": option_help,\n    },\n    {\n        \"label\": \"Run AI service\",\n        \"action\": option_run,\n    },\n    {\n        \"label\": \"Profile AI service\",\n        \"action\": option_profile_run,\n    },\n    {\n        \"label\": \"Run AI service with XAI (only image-classification models)\",\n        \"action\": option_run_with_xai,\n    },\n    {\n        \"label\": \"Profile AI service with XAI (only image-classification models)\",\n        \"action\": option_profile_run_with_xai,\n    },\n]\n\n\nif __name__ == \"__main__\":\n    while True:\n        print(\"\\nOptions:\")\n        for i, option in enumerate(OPTIONS, start=1):\n            print(f\"{i}. {option['label']}\")\n        print(\"q. Quit\")\n        choice = input(\"Enter your choice: \")\n\n        if choice == \"q\":\n            print(\"Exiting the client. Goodbye!\")\n            break\n        else:\n            try:\n                choice = int(choice)\n                if 1 <= choice <= len(OPTIONS):\n                    OPTIONS[choice - 1][\"action\"]()\n                else:\n                    print(\"Invalid choice. Please try again.\")\n            except ValueError:\n                print(\"Invalid input. Please enter a number.\")\n",
        "ai_client_utils_script_content": "def prepare_ai_service_request_files():\n    \"\"\"Prepare the `files` part for the AI service request.\"\"\"\n    files = {}\n    image_file_path = input(\"Please input the image file path: \")\n    with open(image_file_path, \"rb\") as image_file:\n        files[\"file\"] = image_file.read()\n    return files\n\ndef prepare_ai_service_request_data():\n    \"\"\"Prepare the `data` part for the AI service request including text prompts and unique execution ID.\"\"\"\n    data = {}\n    data[\"ue_id\"] = input(\"Please input the unique execution ID (ue_id): \")\n\n    # Allow the user to input a non-empty list of text prompts\n    prompts = []\n    print(\"Please input text prompts (type 'done' when finished):\")\n    while True:\n        prompt = input(\"- \")\n        if prompt.lower() == 'done':\n            break\n        if prompt.strip():\n            prompts.append(prompt)\n\n    if not prompts:\n        raise ValueError(\"The list of text prompts cannot be empty.\")\n\n    data[\"text_prompts\"] = prompts\n    return data",
        "model_script_content": "# import server utils\nfrom ai_server_utils import (\n    get_image_classification_results_from_model_output_logits,\n    profile_activities,\n    prepare_profile_results,\n)\n# import profile utils\nfrom torch.profiler import profile, record_function\n\n# import necessary libs for AI model inference and request handling\nimport torch\nfrom fastapi import APIRouter, File, Form, UploadFile\nfrom fastapi.responses import JSONResponse\nfrom transformers import CLIPProcessor, CLIPModel\nfrom PIL import Image\n\n# --------------------------------\n# Device configuration\n# --------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# --------------------------------\n# Model-specific configuration\n# make sure the variables `MODEL_NAME` and `model` are defined here.\n# --------------------------------\nMODEL_NAME = \"openai/clip-vit-large-patch14\"\nprocessor = CLIPProcessor.from_pretrained(MODEL_NAME)\nmodel = CLIPModel.from_pretrained(MODEL_NAME).to(device)\nmodel.eval()\n\n# Initialize the FastAPI router\nrouter = APIRouter()\n\n@router.post(\"/run\")\nasync def run_model(file: UploadFile = File(...), text_prompts: list = Form(...), ue_id: str = Form(...)):\n    try:\n        # Prepare the model input\n        image = Image.open(file.file).convert(\"RGB\")\n        inputs = processor(text=text_prompts, images=image, return_tensors=\"pt\", padding=True).to(device)\n\n        # Perform inference\n        with torch.no_grad():\n            model_outputs = model(**inputs)\n\n        # Process the model_outputs\n        logits_per_image = model_outputs.logits_per_image\n        probs = logits_per_image.softmax(dim=1).cpu().numpy()\n\n        # Map probabilities to text prompts\n        results = [\n            {\"text_prompt\": text, \"probability\": float(prob)}\n            for text, prob in zip(text_prompts, probs[0])\n        ]\n\n        # Sort results by probability (optional)\n        results = sorted(results, key=lambda x: x[\"probability\"], reverse=True)\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"model_results\": results,\n            }\n        )\n    except Exception as e:\n        print(f\"Error processing file: {e}\")\n        return JSONResponse(\n            content={\"error\": \"Failed to process the image. {e}\".format(e=str(e))},\n            status_code=500,\n        )\n\n@router.post(\"/profile_run\")\nasync def profile_run(file: UploadFile = File(...), text_prompts: list = Form(...), ue_id: str = Form(...)):\n    \"\"\"\n    Endpoint to profile the AI model execution.\n    \"\"\"\n    try:\n        # Prepare the model input\n        image = Image.open(file.file).convert(\"RGB\")\n        inputs = processor(text=text_prompts, images=image, return_tensors=\"pt\", padding=True).to(device)\n\n        # perform profiling\n        with profile(\n            activities=profile_activities,\n            profile_memory=True,\n        ) as prof:\n            with record_function(\"model_run\"):\n                with torch.no_grad():\n                    model_outputs = model(**inputs)\n\n        profile_result = prepare_profile_results(prof)\n\n        # Process the model_outputs\n        logits_per_image = model_outputs.logits_per_image\n        probs = logits_per_image.softmax(dim=1).cpu().numpy()\n\n        # Map probabilities to text prompts\n        results = [\n            {\"text_prompt\": text, \"probability\": float(prob)}\n            for text, prob in zip(text_prompts, probs[0])\n        ]\n\n        # Sort results by probability (optional)\n        results = sorted(results, key=lambda x: x[\"probability\"], reverse=True)\n\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"profile_result\": profile_result,\n                \"model_results\": results,\n            }\n        )\n\n    except Exception as e:\n        print(f\"Error processing request: {e}\")\n        return JSONResponse(\n            content={\"error\": f\"Failed to process the request. {e}\"},\n            status_code=500,\n        )\n\n# Below are the model input and output specifications to be used by the `/help` endpoint\nMODEL_INPUT_FORM_SPEC = {\n    \"file\": {\n        \"type\": \"file upload\",\n        \"description\": \"The image file to be classified.\",\n        \"required\": True,\n        \"example\": \"puppy.png\",\n    },\n    \"text_prompts\": {\n        \"type\": \"list\",\n        \"description\": \"A list of text prompts for the model to classify the image against.\",\n        \"required\": True,\n        \"example\": [\"a photo of a cat\", \"a photo of a dog\"],\n    }\n}\n\nMODEL_OUTPUT_JSON_SPEC = {\n    \"ue_id\": \"unique execution ID\",\n    \"model_results\": [\n        {\n            \"category_id\": \"category id\",\n            \"label\": \"category label\",\n            \"probability\": \"probability value\",\n        }\n    ],\n}",
        "healthcheck_script_content": "import requests\n\nprint(requests.get(\"http://localhost:8000/help\"))"
    }
}