{
    "model_name": "microsoft/resnet-50",
    "model_url": "https://huggingface.co/microsoft/resnet-50",
    "task": "image-classification",
    "task_detail": "The Microsoft ResNet-50 v1.5 model is a sophisticated convolutional neural network specifically designed for image classification tasks. It employs a refined architecture known as Residual Networks, which prominently features the use of residual learning and skip connections. These components facilitate the training of deeper models, enhancing the model's ability to accurately classify images by capturing intricate patterns within visual data.\n\nThis pre-trained model is optimized for image classification using the widely respected ImageNet-1k dataset, allowing it to categorize images into one of 1,000 distinct classes. The dataset itself is vast and varied, covering a comprehensive spectrum of objects, animals, scenes, and other categories drawn from natural and man-made environments. Consequently, the ResNet-50 model is adept at handling a broad range of image types, making it suitable for various applications that require rapid and precise image recognition.\n\nThe expected input format for this model is an image with a resolution of 224x224 pixels, and the input should be processed into this specific format to achieve optimal performance. The output of the model is a set of logits, which are numerical scores representing the likelihood that the input image belongs to each of the 1,000 ImageNet classes. These logits can be post-processed to yield a predicted class label, indicating the most probable category for the input image.\n\nThe model has been fine-tuned to achieve slightly improved accuracy over the original ResNet version. This enhancement is due to an architectural adjustment in the bottleneck blocks, where the stride=2 configuration is applied to the 3x3 convolution instead of the 1x1 convolution seen in earlier versions. While this modification modestly increases accuracy by approximately 0.5%, it does so with a minor performance trade-off, reducing image processing speed by roughly 5%.\n\nIn practical usage, the model can be employed in scenarios that necessitate efficient and reliable image classification, such as automated tagging, content moderation, and visual search enhancement. The model's capacity to analyze and classify images from a wide-ranging dataset like ImageNet-1k makes it an invaluable tool for businesses, researchers, and developers seeking state-of-the-art solutions in computer vision and related fields.",
    "accuracy_info": "The README indicates that ResNet50 v1.5 is slightly more accurate than v1 by approximately 0.5% in top-1 accuracy.",
    "image_repository_url": "docker.io/cranfield6g/cranfield-edge-microsoft-resnet-50",
    "service_disk_size_bytes": 3315150206,
    "profiles": [
        {
            "node_id": "LAP004262",
            "resource": {
                "cpu_time_ms": 56.413723,
                "device_time_ms": 0.0,
                "cpu_memory_usage_MB": 122.220703125,
                "device_memory_usage_MB": 0.0,
                "energy_consumption_execution": 0,
                "energy_consumption_idle": 0,
                "disk_IO_MB": 0,
                "input_data_MB": 0,
                "output_data_MB": 0
            },
            "latency": {
                "initialization_time_ms": 14707.844734191895,
                "inference_time_ms": 178.7626028060913,
                "eviction_time_ms": 0
            },
            "billing": {
                "initialization_cost": 0,
                "keep_alive_cost": 0,
                "execution_cost": 0
            },
            "device_type": "DeviceType.CPU",
            "device_name": "None"
        }
    ],
    "feedback": {
        "likes": [],
        "dislikes": [],
        "comments": []
    },
    "code": {
        "readme_content": "---\nlicense: apache-2.0\ntags:\n- vision\n- image-classification\ndatasets:\n- imagenet-1k\n---\n\n# ResNet-50 v1.5\n\nResNet model pre-trained on ImageNet-1k at resolution 224x224. It was introduced in the paper [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) by He et al. \n\nDisclaimer: The team releasing ResNet did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nResNet (Residual Network) is a convolutional neural network that democratized the concepts of residual learning and skip connections. This enables to train much deeper models.\n\nThis is ResNet v1.5, which differs from the original model: in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate (\\~0.5% top1) than v1, but comes with a small performance drawback (~5% imgs/sec) according to [Nvidia](https://catalog.ngc.nvidia.com/orgs/nvidia/resources/resnet_50_v1_5_for_pytorch).\n\n![model image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/resnet_architecture.png)\n\n## Intended uses & limitations\n\nYou can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=resnet) to look for\nfine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\n\n```python\nfrom transformers import AutoImageProcessor, ResNetForImageClassification\nimport torch\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"huggingface/cats-image\")\nimage = dataset[\"test\"][\"image\"][0]\n\nprocessor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\nmodel = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\")\n\ninputs = processor(image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label])\n```\n\nFor more code examples, we refer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/resnet).\n\n### BibTeX entry and citation info\n\n```bibtex\n@inproceedings{he2016deep,\n  title={Deep residual learning for image recognition},\n  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},\n  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},\n  pages={770--778},\n  year={2016}\n}\n```\n",
        "dockerfile_content": "# Stage 1: Build stage\nFROM python:3.12-slim AS builder\n\n# Install build dependencies\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    build-essential \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Set working directory\nWORKDIR /app\n\n# Install Python dependencies\nRUN pip install --no-cache-dir --upgrade pip\nRUN pip install --no-cache-dir \\\n    fastapi \\\n    uvicorn[standard] \\\n    torch \\\n    torchvision \\\n    transformers \\\n    python-multipart \\\n    Pillow \\\n    psutil \\\n    requests \\\n    datasets\n\n# Stage 2: Final stage\nFROM python:3.12-slim\n\n# Set working directory\nWORKDIR /app\n\n# Copy installed packages from builder stage\nCOPY --from=builder /usr/local/lib/python3.12/site-packages /usr/local/lib/python3.12/site-packages\nCOPY --from=builder /usr/local/bin/uvicorn /usr/local/bin/uvicorn\n\n# Copy application code\nCOPY . .\n\n# Expose port 8000\nEXPOSE 8000\n\n# Start the FastAPI server\nCMD [\"uvicorn\", \"ai_server:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]",
        "ai_server_script_content": "import time\nfrom fastapi import FastAPI\nfrom fastapi.responses import JSONResponse\nfrom contextlib import asynccontextmanager\nfrom logger import Logger\n\n# -------------------------------------------\n# Model setup\n# -------------------------------------------\nai_model_endpoint_spec = {\n    \"model_input_form_spec\": None,\n    \"model_output_json_spec\": None,\n    \"profile_output_json_spec\": None,\n}\n\n\n# -------------------------------------------\n# App Lifespan setup\n# -------------------------------------------\n# Record the script start time (when uvicorn starts the process)\nSCRIPT_START_TIME = time.time()\nINITIALIZATION_DURATION = 0.0\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n\n    global INITIALIZATION_DURATION\n    global SCRIPT_START_TIME\n    global ai_model_endpoint_spec\n\n    # Load the AI model\n    print(\"Loading AI model...\")\n    from model import (\n        MODEL_INPUT_FORM_SPEC,\n        MODEL_OUTPUT_JSON_SPEC,\n        PROFILE_OUTPUT_JSON_SPEC,\n        router as model_router,\n    )\n\n    ai_model_endpoint_spec[\"model_input_form_spec\"] = MODEL_INPUT_FORM_SPEC\n    ai_model_endpoint_spec[\"model_output_json_spec\"] = MODEL_OUTPUT_JSON_SPEC\n    ai_model_endpoint_spec[\"profile_output_json_spec\"] = PROFILE_OUTPUT_JSON_SPEC\n\n    app.include_router(model_router, prefix=\"/model\", tags=[\"AI Model\"])\n\n    # Record the initialization duration\n    INITIALIZATION_DURATION = time.time() - SCRIPT_START_TIME\n\n    print(f\"AI model loaded in {INITIALIZATION_DURATION:.2f} seconds.\")\n\n    yield\n\n    # Clean up the models and release the resources\n    ai_model_endpoint_spec.clear()\n\n\n# -------------------------------------------\n# FastAPI application setup\n# -------------------------------------------\napp = FastAPI(lifespan=lifespan)\n\n\n@app.get(\"/get_ue_log\")\ndef get_log(ue_id: str):\n    # Retrieve logs for the specified UE_ID\n    logger = Logger.get_instance()\n    if logger is None:\n        return JSONResponse(\n            content={\"error\": \"Logger not initialized.\"},\n            status_code=500,\n        )\n\n    log_data = logger.get_ue_run_log(ue_id=ue_id)\n    if log_data is None:\n        return JSONResponse(\n            content={\"error\": f\"No logs found for UE_ID: {ue_id}\"},\n            status_code=404,\n        )\n    return JSONResponse(content=log_data)\n\n\n@app.get(\"/initialization_duration\")\ndef get_initialization_duration():\n    \"\"\"\n    Endpoint to retrieve the initialization duration of the AI model.\n    \"\"\"\n    global INITIALIZATION_DURATION\n\n    if INITIALIZATION_DURATION == 0.0:\n        return JSONResponse(\n            content={\"error\": \"Model not initialized.\"},\n            status_code=500,\n        )\n    return JSONResponse(\n        content={\n            \"initialization_duration\": INITIALIZATION_DURATION,\n            \"script_start_time\": SCRIPT_START_TIME,\n        }\n    )\n\n\n@app.get(\"/help\")\ndef get_help():\n    global ai_model_endpoint_spec\n    return {\n        \"endpoints\": {\n            \"/model/run\": {\n                \"method\": \"POST\",\n                \"description\": \"Executes the AI model with the provided input data.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **ai_model_endpoint_spec[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    \"execution_duration\": \"Time taken to execute the model (in seconds).\",\n                    **ai_model_endpoint_spec[\"model_output_json_spec\"],\n                },\n            },\n            \"/model/profile\": {\n                \"method\": \"POST\",\n                \"description\": \"Profiles the AI model execution.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    **ai_model_endpoint_spec[\"model_input_form_spec\"],\n                },\n                \"response\": {\n                    \"ue_id\": \"User Equipment ID (string) for tracking the request.\",\n                    \"profile_results\": \"Profiling results of the AI model execution.\",\n                    **ai_model_endpoint_spec[\"profile_output_json_spec\"],\n                },\n            },\n            \"/get_ue_log\": {\n                \"method\": \"GET\",\n                \"description\": \"Retrieves logs for a specific UE_ID.\",\n                \"parameters\": {\n                    \"ue_id\": \"User Equipment ID (string) to retrieve logs for.\"\n                },\n                \"response\": {\n                    \"node_id\": \"Name of the computation node running the model.\",\n                    \"k8s_pod_name\": \"Name of the Kubernetes pod running the model.\",\n                    \"model_name\": \"Name of the AI model.\",\n                    \"ue_id\": \"User Equipment ID (string) for which logs are retrieved.\",\n                    \"total_input_size\": \"Total size of input data processed for the UE_ID (in bytes).\",\n                    \"total_execution_duration\": \"Total time taken for all executions for the UE_ID (in seconds).\",\n                    \"total_executions\": \"Total number of executions for the UE_ID.\",\n                    \"average_execution_duration\": \"Average time taken for each execution for the UE_ID (in seconds).\",\n                    \"latest_run\": {\n                        \"input_size\": \"Size of the latest input data processed (in bytes).\",\n                        \"execution_duration\": \"Time taken for the latest execution (in seconds).\",\n                        \"timestamp\": \"Timestamp of the latest execution\"\n                        \"(in seconds since epoch).\",\n                    },\n                },\n            },\n            \"/initialization_duration\": {\n                \"method\": \"GET\",\n                \"description\": \"Retrieves the initialization duration of the AI model.\",\n                \"response\": {\n                    \"initialization_duration\": \"Time taken to initialize the model (in seconds).\",\n                    \"script_start_time\": \"Timestamp when the script started (in seconds since epoch).\",\n                },\n            }\n        }\n    }\n",
        "ai_client_script_content": "import json\nimport requests\nimport time\nfrom ai_client_utils import get_help, get_ue_log, ProfileResultProcessor, get_inference_url, get_profile_url\n\nSERVER_URL = input(\"Please input server URL (e.g., http://localhost:54660): \")\n\ndef send_post_request(url, files, data):\n    \"\"\"Send request to run AI service and display AI service responses.\"\"\"\n    try:\n        response = requests.post(url, files=files, data=data)\n        if response.status_code == 200:\n            return response.json()\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n            return None\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n        return None\n\ndef run_inferece_test(ue_id):\n    files = {}\n\n    image_file_path = input(\"Please input the image file path: \")\n    with open(image_file_path, \"rb\") as image_file:\n        files[\"file\"] = image_file\n        data = {\"ue_id\": ue_id}\n        print(send_post_request(get_inference_url(SERVER_URL), files, data))\n\ndef run_pressure_test(ue_id, num_requests):\n    files = {}\n\n    image_file_path = input(\"Please input the image file path: \")\n    with open(image_file_path, \"rb\") as image_file:\n        files[\"file\"] = image_file.read()\n\n    data = {\"ue_id\": ue_id}\n\n    try:\n        profile_result_processor = ProfileResultProcessor(SERVER_URL)\n\n        for _ in range(num_requests):\n            profile_response = send_post_request(get_profile_url(SERVER_URL), files, data)\n            print(json.dumps(profile_response, indent=4))\n            if not profile_response:\n                print(\"No profile response received.\")\n                continue\n\n            profile_result_processor.process_new_response(profile_response)\n\n        profile_result_processor.print_profile_result()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\nif __name__ == \"__main__\":\n    while True:\n        print(\"\\nOptions:\")\n        print(\"1. Run AI service once\")\n        print(\"2. Get help information\")\n        print(\"3. Get logs for a specific UE_ID\")\n        print(\"4. Perform a pressure test to profile the runtime statistics the AI service\")\n        print(\"5. Quit\")\n        choice = input(\"Enter your choice: \")\n\n        if choice == \"1\":\n            ue_id = input(\"Enter the UE_ID: \")\n            run_inferece_test(ue_id)\n        elif choice == \"2\":\n            url = f\"{SERVER_URL}/help\"\n            get_help(url)\n        elif choice == \"3\":\n            url = f\"{SERVER_URL}/get_ue_log\"\n            ue_id = input(\"Enter the UE_ID: \")\n            get_ue_log(url, ue_id)\n        elif choice == \"4\":\n            ue_id = input(\"Enter the UE_ID: \")\n            num_requests = int(input(\"Enter the number of requests to send: \"))\n            run_pressure_test(ue_id, num_requests)\n        elif choice == \"5\":\n            print(\"Exiting the client. Goodbye!\")\n            break\n        else:\n            print(\"Invalid choice. Please try again.\")",
        "ai_client_utils_script_content": "import json\nimport time\nimport requests\n\ndef get_help(url):\n    \"\"\"Get help information from the server.\"\"\"\n    try:\n        response = requests.get(url)\n        if response.status_code == 200:\n            print(\"Help Information:\")\n            print(response.json())\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\n\ndef get_ue_log(url, ue_id):\n    \"\"\"Retrieve logs for a specific UE_ID.\"\"\"\n    # url = f\"{SERVER_URL}/get_ue_log\"\n    try:\n        response = requests.get(url, params={\"ue_id\": ue_id})\n        if response.status_code == 200:\n            print(f\"Logs for UE_ID {ue_id}:\")\n            print(response.json())\n        else:\n            print(f\"Error: {response.status_code}, {response.text}\")\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n\ndef get_inference_url(server_url):\n    return f\"{server_url}/model/run\"\n\ndef get_profile_url(server_url):\n    return f\"{server_url}/model/profile\"\n\nclass ProfileResultProcessor:\n\n    def __init__(self, server_url):\n        self.server_url = server_url\n        self.start_time = time.time()\n        self.service_initialization_duration = 0\n        self.response_counter = 0\n        self.profile_name = None\n        self.device_type = None\n        self.device_name = None\n        self.node_id = None\n        self.k8s_pod_name = None\n        self.cpu_memory_usage_bytes = 0\n        self.self_cpu_memory_usage_bytes = 0\n        self.device_memory_usage_bytes = 0\n        self.self_device_memory_usage_bytes = 0\n        self.cpu_time_total_us = 0\n        self.self_cpu_time_total_us = 0\n        self.device_time_total_us = 0\n        self.self_device_time_total_us = 0\n\n        self.update_service_initialization_duration()\n    \n    def update_service_initialization_duration(self):\n        \"\"\"Fetch the service initialization duration from the server.\"\"\"\n        try:\n            response = requests.get(f\"{self.server_url}/initialization_duration\")\n            if response.status_code == 200:\n                self.service_initialization_duration = response.json().get(\"initialization_duration\", 0)\n            else:\n                print(f\"Error fetching initialization duration: {response.status_code}, {response.text}\")\n                return 0\n        except requests.exceptions.RequestException as e:\n            print(f\"Request failed: {e}\")\n            return 0\n\n    def process_new_response(self, profile_response):\n        if not profile_response:\n            return\n\n        profile_result = profile_response[\"profile_result\"]\n\n        if not profile_result:\n            return\n        \n        if self.profile_name is None:\n            self.profile_name = profile_result[\"name\"]\n        if self.device_type is None:\n            self.device_type = profile_result[\"device_type\"]\n        if self.device_name is None:    \n            self.device_name = profile_result[\"device_name\"]\n        if self.node_id is None:\n            self.node_id = profile_response[\"node_id\"]\n        if self.k8s_pod_name is None:\n            self.k8s_pod_name = profile_response[\"k8s_pod_name\"]\n\n        # update the max profile result\n        self.cpu_memory_usage_bytes = max(\n            self.cpu_memory_usage_bytes, profile_result[\"cpu_memory_usage\"]\n        )\n        self.self_cpu_memory_usage_bytes = max(\n            self.self_cpu_memory_usage_bytes, profile_result[\"self_cpu_memory_usage\"]\n        )\n        self.device_memory_usage_bytes = max(\n            self.device_memory_usage_bytes, profile_result[\"device_memory_usage\"]\n        )\n        self.self_device_memory_usage_bytes = max(      \n            self.self_device_memory_usage_bytes, profile_result[\"self_device_memory_usage\"]\n        )\n        self.cpu_time_total_us = max(\n            self.cpu_time_total_us, profile_result[\"cpu_time_total\"]\n        )\n        self.self_cpu_time_total_us = max(\n            self.self_cpu_time_total_us, profile_result[\"self_cpu_time_total\"]\n        )\n        self.device_time_total_us = max(\n            self.device_time_total_us, profile_result[\"device_time_total\"]\n        )\n        self.self_device_time_total_us = max(\n            self.self_device_time_total_us, profile_result[\"self_device_time_total\"]\n        )\n\n        self.response_counter += 1\n    \n    def print_profile_result(self):\n        print(\"\\n--------- PROFILE EVENT ---------\\n\")\n        print(f\"Name: {self.profile_name}\")\n        print(f\"Device Type: {self.device_type}\")\n        print(f\"Device Name: {self.device_name}\")\n        print(f\"Node ID: {self.node_id}\")\n        print(f\"K8S_POD_NAME: {self.k8s_pod_name}\")    \n        \n        print(\"\\n--------- LATENCY RESULT ---------\\n\")\n        print(\"Service Initialization Duration: \", self.service_initialization_duration)\n        print(\"Total Requests: \", self.response_counter)\n        print(\n            f\"Total Time Taken: {time.time() - self.start_time:.2f} seconds\"\n        )\n        print(\n            f\"Average Time Taken: {(time.time() - self.start_time) / self.response_counter:.2f} seconds\"\n        )\n\n        print(\"\\n--------- RESOURCE USAGE ---------\\n\")\n        print(\n            f\"CPU Memory Usage: {self.cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Self CPU Memory Usage: {self.self_cpu_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Device Memory Usage: {self.device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"Self Device Memory Usage: {self.self_device_memory_usage_bytes / (1024 * 1024):.2f} MB\"\n        )\n        print(\n            f\"CPU Time Total: {self.cpu_time_total_us / 1000:.2f} ms\"\n        )\n        print(\n            f\"Self CPU Time Total: {self.self_cpu_time_total_us / 1000:.2f} ms\"\n        )\n        print(\n            f\"Device Time Total: {self.device_time_total_us / 1000:.2f} ms\"\n        )\n        print(\n            f\"Self Device Time Total: {self.self_device_time_total_us / 1000:.2f} ms\"\n        )\n\n        # update the service_data.json automatically\n        with open(\"service_data.json\", \"r\") as f:\n            service_data = json.load(f)\n        \n        profile_data_to_save = {\n            \"node_id\": self.node_id,\n            \"resource\": {\n                \"cpu_time_ms\": self.cpu_time_total_us / 1000,\n                \"device_time_ms\": self.device_time_total_us / 1000,\n                \"cpu_memory_usage_MB\": max(self.cpu_memory_usage_bytes, self.self_cpu_memory_usage_bytes) / (1024 * 1024),\n                \"device_memory_usage_MB\": max(self.device_memory_usage_bytes, self.self_device_memory_usage_bytes) / (1024 * 1024),\n                \"energy_consumption_execution\": 0,\n                \"energy_consumption_idle\": 0,\n                \"disk_IO_MB\": 0,\n                \"input_data_MB\": 0,\n                \"output_data_MB\": 0\n            },\n            \"latency\": {\n                \"initialization_time_ms\": self.service_initialization_duration * 1000,\n                \"inference_time_ms\": (time.time() - self.start_time) / self.response_counter * 1000,\n                \"eviction_time_ms\": 0\n            },\n            \"billing\": {\n                \"initialization_cost\": 0,\n                \"keep_alive_cost\": 0,\n                \"execution_cost\": 0\n            }\n        }\n\n        # check if there is already a profile for this node id\n        for profile in service_data[\"profiles\"]:\n            if profile[\"node_id\"] == self.node_id:\n                # update the profile\n                profile.update(profile_data_to_save)\n                break\n        else:\n            # add a new profile\n            service_data[\"profiles\"].append(profile_data_to_save)\n        \n        # save the updated service_data.json\n        with open(\"service_data.json\", \"w\") as f:\n            json.dump(service_data, f, indent=4)\n        print(\"\\n--------- SERVICE DATA UPDATED ---------\\n\")   \n        \n        \n\n\n",
        "model_script_content": "import time\nfrom fastapi import APIRouter, File, Form, UploadFile\nfrom fastapi.responses import JSONResponse\nimport torch\nfrom PIL import Image\nfrom transformers import AutoImageProcessor, ResNetForImageClassification\nfrom logger import Logger\nfrom torch.profiler import profile, ProfilerActivity, record_function\n\n\n# -------------------------------------------\n# Model-specific configuration\n# -------------------------------------------\nMODEL_NAME = \"microsoft/resnet-50\"\nprocessor = AutoImageProcessor.from_pretrained(MODEL_NAME, use_fast=True)\nmodel = ResNetForImageClassification.from_pretrained(MODEL_NAME)\nmodel.eval()\nid2label = model.config.id2label\n\n# -------------------------------------------\n# Logger setup\n# -------------------------------------------\nlogger = Logger(model_name=MODEL_NAME)\n\n\nrouter = APIRouter()\n\n\n@router.post(\"/run\")\nasync def run_model(file: UploadFile = File(...), ue_id: str = Form(...)):\n    if ue_id is None:\n        return JSONResponse(\n            content={\"error\": \"UE_ID is required.\"},\n            status_code=400,\n        )\n\n    try:\n        start_time = time.time()\n\n        # Process the input image\n        image = Image.open(file.file).convert(\"RGB\")\n        inputs = processor(images=image, return_tensors=\"pt\")\n\n        # Perform inference\n        with torch.no_grad():\n            outputs = model(**inputs)\n        logits = outputs.logits\n        probabilities = torch.nn.functional.softmax(logits[0], dim=0)\n\n        # Return the top 5 predictions with labels\n        top5_prob, top5_catid = torch.topk(probabilities, 5)\n        predictions = []\n        for i in range(top5_prob.size(0)):\n            category_id = top5_catid[i].item()\n            predictions.append(\n                {\n                    \"category_id\": category_id,\n                    \"label\": id2label[category_id],\n                    \"probability\": top5_prob[i].item(),\n                }\n            )\n\n        execution_duration = time.time() - start_time\n\n        logger.add_ue_run_log(\n            ue_id=ue_id,\n            input_size=0,\n            execution_duration=execution_duration,\n        )\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"predictions\": predictions,\n                \"input_size_bytes\": 0,\n                \"execution_duration\": execution_duration,\n            }\n        )\n    except Exception as e:\n        print(f\"Error processing file: {e}\")\n        return JSONResponse(\n            content={\"error\": \"Failed to process the image. {e}\".format(e=str(e))},\n            status_code=500,\n        )\n\n\n@router.post(\"/profile\")\nasync def profile_service(file: UploadFile = File(...), ue_id: str = Form(...)):\n    \"\"\"\n    Endpoint to profile the AI model execution.\n    \"\"\"\n    if ue_id is None:\n        return JSONResponse(\n            content={\"error\": \"UE_ID is required.\"},\n            status_code=400,\n        )\n\n    profile_activities = [\n        ProfilerActivity.CPU,\n        ProfilerActivity.CUDA,\n        ProfilerActivity.MTIA,\n        ProfilerActivity.XPU,\n    ]\n\n    try:\n        profile_start_time = time.time()\n\n        # Process the input image\n        image = Image.open(file.file).convert(\"RGB\")\n        inputs = processor(images=image, return_tensors=\"pt\")\n\n        # Perform inference\n        with profile(\n            activities=profile_activities,\n            profile_memory=True,\n            # record_shapes=True,\n        ) as prof:\n            with record_function(\"model_inference\"):\n                # Run the model\n                with torch.no_grad():\n                    outputs = model(**inputs)\n\n        logits = outputs.logits\n        probabilities = torch.nn.functional.softmax(logits[0], dim=0)\n\n        # Return the top 5 predictions with labels\n        top5_prob, top5_catid = torch.topk(probabilities, 5)\n        predictions = []\n        for i in range(top5_prob.size(0)):\n            category_id = top5_catid[i].item()\n            predictions.append(\n                {\n                    \"category_id\": category_id,\n                    \"label\": id2label[category_id],\n                    \"probability\": top5_prob[i].item(),\n                }\n            )\n\n        execution_duration = time.time() - profile_start_time\n\n        # Format profiler data into JSON\n        print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n\n        # get only t\n        profile_event = prof.key_averages()[0]\n\n        profile_result = {\n            \"name\": profile_event.key,\n            \"device_type\": str(profile_event.device_type),\n            \"device_name\": str(profile_event.use_device),\n            \"cpu_memory_usage\": max([abs(e.cpu_memory_usage) for e in prof.key_averages()]),\n            \"self_cpu_memory_usage\": max([abs(e.self_cpu_memory_usage) for e in prof.key_averages()]),\n            \"device_memory_usage\": max([abs(e.device_memory_usage) for e in prof.key_averages()]),\n            \"self_device_memory_usage\": max([abs(e.self_device_memory_usage) for e in prof.key_averages()]),\n            \"cpu_time_total\": profile_event.cpu_time_total,\n            \"self_cpu_time_total\": profile_event.self_cpu_time_total,\n            \"device_time_total\": profile_event.device_time_total,\n            \"self_device_time_total\": profile_event.self_device_time_total,\n        }\n\n        return JSONResponse(\n            content={\n                \"ue_id\": ue_id,\n                \"node_id\": logger.node_id,\n                \"k8s_pod_name\": logger.k8s_pod_name,\n                \"profile_result\": profile_result,\n                \"ai_result\": predictions,\n                \"execution_duration\": execution_duration,\n            }\n        )\n\n    except Exception as e:\n        print(f\"Error processing request: {e}\")\n        return JSONResponse(\n            content={\"error\": f\"Failed to process the request. {e}\"},\n            status_code=500,\n        )\n\n\n# Below are the model input and output specifications to be used by the `/help` endpoint\nMODEL_INPUT_FORM_SPEC = {\n    \"file\": {\n        \"type\": \"file upload\",\n        \"description\": \"The image file to be classified.\",\n        \"required\": True,\n        \"example\": \"puppy.png\",\n    }\n}\n\nMODEL_OUTPUT_JSON_SPEC = {\n    \"ue_id\": \"unique execution ID\",\n    \"input_size_bytes\": \"size of the input image in bytes\",\n    \"execution_duration\": \"time taken for the model to process the image in seconds\",\n    \"predictions\": [\n        {\n            \"category_id\": \"category id\",\n            \"label\": \"category label\",\n            \"probability\": \"probability value\",\n        }\n    ],\n}\n\nPROFILE_OUTPUT_JSON_SPEC = {\n    \"ue_id\": \"unique execution ID\",\n    \"profile_result\": {\n        \"name\": \"name of the profile event\",\n        \"device_type\": \"type of device used (e.g., CPU, GPU, ...)\",\n        \"device_name\": \"name of the device used\",\n        \"cpu_memory_usage\": \"CPU memory usage in bytes\",\n        \"self_cpu_memory_usage\": \"self CPU memory usage in bytes\",\n        \"device_memory_usage\": \"device memory usage in bytes\",\n        \"self_device_memory_usage\": \"self device memory usage in bytes\",\n        \"cpu_time_total\": \"total CPU time in microseconds\",\n        \"self_cpu_time_total\": \"self total CPU time in microseconds\",\n        \"device_time_total\": \"total device time in microseconds\",\n        \"self_device_time_total\": \"self total device time in microseconds\",\n    },\n    \"ai_result\": [\n        {\n            \"category_id\": \"category id\",\n            \"label\": \"category label\",\n            \"probability\": \"probability value\",\n        }\n    ],\n    \"execution_duration\": \"time taken for the model to process the image in seconds\",\n}",
        "healthcheck_script_content": "import requests\n\nprint(requests.get(\"http://localhost:8000/help\"))",
        "logger_script_content": "import os\nimport socket\nfrom threading import Lock\nimport time\n\n# -------------------------------------------\n# Configuration\n# -------------------------------------------\nNODE_ID = os.getenv(\"NODE_ID\", socket.gethostname())\nK8S_POD_NAME = os.getenv(\"K8S_POD_NAME\", \"UNKNOWN\")\nprint(f\"Node ID: {NODE_ID}, K8S_POD_NAME: {K8S_POD_NAME}\")\n\n\nclass Logger:\n    \"\"\"Singleton Logger class to handle logging for the AI model.\"\"\"\n\n    _instance = None  # Class-level attribute to hold the singleton instance\n    _lock = Lock()  # Lock for thread-safe singleton initialization\n\n    @classmethod\n    def get_instance(cls):\n        \"\"\"Get the singleton instance of Logger.\"\"\"\n        if not cls._instance:\n            cls._instance = Logger()\n        return cls._instance\n\n    def __new__(cls, *args, **kwargs):\n        \"\"\"Ensure only one instance of Logger is created.\"\"\"\n        if not cls._instance:\n            with cls._lock:\n                if not cls._instance:  # Double-checked locking\n                    cls._instance = super(Logger, cls).__new__(cls)\n        return cls._instance\n\n    def __init__(self, model_name, node_id=NODE_ID, k8s_pod_name=K8S_POD_NAME):\n        \"\"\"Initialize the Logger instance.\"\"\"\n        # Avoid reinitializing if the instance already exists\n        if not hasattr(self, \"initialized\"):\n            self.model_name = model_name\n            self.node_id = node_id\n            self.k8s_pod_name = k8s_pod_name\n            self.ue_logs = {}\n            self.lock = Lock()\n            self.initialized = True  # Mark the instance as initialized\n\n    def add_ue_run_log(self, ue_id, input_size, execution_duration):\n        \"\"\"Add a run log entry for a specific UE_ID.\"\"\"\n        with self.lock:\n            if ue_id not in self.ue_logs:\n                self.ue_logs[ue_id] = {\n                    \"total_input_size\": 0,\n                    \"total_execution_duration\": 0.0,\n                    \"total_executions\": 0,\n                    \"average_execution_duration\": 0.0,\n                    \"latest_run\": {},\n                }\n            self.ue_logs[ue_id][\"total_input_size\"] += input_size\n            self.ue_logs[ue_id][\"total_execution_duration\"] += execution_duration\n            self.ue_logs[ue_id][\"total_executions\"] += 1\n            self.ue_logs[ue_id][\"average_execution_duration\"] = (\n                self.ue_logs[ue_id][\"total_execution_duration\"]\n                / self.ue_logs[ue_id][\"total_executions\"]\n            )\n            self.ue_logs[ue_id][\"latest_run\"] = {\n                \"input_size\": input_size,\n                \"execution_duration\": execution_duration,\n                \"timestamp\": time.time(),\n            }\n\n    def get_ue_run_log(self, ue_id):\n        \"\"\"Retrieve the run log for a specific UE_ID.\"\"\"\n        with self.lock:\n            if ue_id not in self.ue_logs:\n                return None\n            return {\n                \"node_id\": self.node_id,\n                \"k8s_pod_name\": self.k8s_pod_name,\n                \"model_name\": self.model_name,\n                \"ue_id\": ue_id,\n                \"total_input_size\": self.ue_logs[ue_id][\"total_input_size\"],\n                \"total_execution_duration\": self.ue_logs[ue_id][\n                    \"total_execution_duration\"\n                ],\n                \"total_executions\": self.ue_logs[ue_id][\"total_executions\"],\n                \"average_execution_duration\": self.ue_logs[ue_id][\n                    \"average_execution_duration\"\n                ],\n                \"latest_run\": self.ue_logs[ue_id][\"latest_run\"],\n            }"
    }
}